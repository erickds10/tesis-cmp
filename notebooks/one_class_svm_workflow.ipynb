{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b8c7f97",
   "metadata": {},
   "source": [
    "\n",
    "# One‑Class SVM (OC‑SVM) — AIS Anomaly Detection (Galápagos)\n",
    "\n",
    "**Objetivo:** Entrenar y evaluar un detector no supervisado (One‑Class SVM, kernel RBF) para identificar anomalías en series de tiempo de trayectorias AIS de pesqueros alrededor de la RMG, usando **solo comportamiento normal** para el entrenamiento y validando contra puntos/ventanas marcadas como `is_suspicious = 1`.\n",
    "\n",
    "**Pautas (resumen de la planificación):**\n",
    "- Entrenar **solo con `is_suspicious = 0`** (comportamiento normal).\n",
    "- Ingeniería de variables por MMSI y orden temporal: lags, diferencias, SMA/EMA, distancia Haversine por segmento, delta de tiempo, aceleración, velocidad angular, y ratios como `distancia_a_costa/distancia_a_puerto`.\n",
    "- Serialización por **ventanas deslizantes** (tamaño `T`) y representación vectorizada `(N, T×F)` para modelos tabulares.\n",
    "- **Split por MMSI** (GroupKFold) para evitar fuga por identidad.\n",
    "- Métricas: ROC‑AUC, PR‑AUC, Precision@k, Recall@k, F1@k; análisis por ventana y por trayectoria.\n",
    "- Guardado de **artefactos**: scaler, modelo, configuración y scores para replicabilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21cd9a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {\n",
      "  \"input_parquet\": \"data/ais_enriched.parquet\",\n",
      "  \"train_filter_col\": \"is_suspicious\",\n",
      "  \"mmsi_col\": \"mmsi\",\n",
      "  \"timestamp_col\": \"timestamp\",\n",
      "  \"base_features\": [\n",
      "    \"lat\",\n",
      "    \"lon\",\n",
      "    \"speed\",\n",
      "    \"course\",\n",
      "    \"depth\",\n",
      "    \"dist_coast_km\",\n",
      "    \"dist_port_km\"\n",
      "  ],\n",
      "  \"lags\": [\n",
      "    1,\n",
      "    2\n",
      "  ],\n",
      "  \"ema_spans\": [\n",
      "    3,\n",
      "    5\n",
      "  ],\n",
      "  \"window_size\": 10,\n",
      "  \"step_size\": 5,\n",
      "  \"svm_nu_grid\": [\n",
      "    0.01,\n",
      "    0.05,\n",
      "    0.1\n",
      "  ],\n",
      "  \"svm_gamma_grid\": [\n",
      "    \"scale\",\n",
      "    0.1,\n",
      "    0.01\n",
      "  ],\n",
      "  \"kernel\": \"rbf\",\n",
      "  \"kfold_splits\": 5,\n",
      "  \"out_dir\": \"runs/ocsvm\",\n",
      "  \"artifact_prefix\": \"ocsvm_rbf\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Setup ---\n",
    "import os, math, json, gc, pickle, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import GroupKFold, ParameterGrid\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibilidad\n",
    "import random\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# --- Configuración global ---\n",
    "CFG = {\n",
    "    # Entradas (ajusta paths a tu estructura real)\n",
    "    \"input_parquet\": \"data/ais_enriched.parquet\",        # Dataset enriquecido (AIS + contexto)\n",
    "    \"train_filter_col\": \"is_suspicious\",                  # Col de validación (no usada para entrenar)\n",
    "    \"mmsi_col\": \"mmsi\",\n",
    "    \"timestamp_col\": \"timestamp\",\n",
    "    # Variables base esperadas en el parquet (ajusta a tus nombres)\n",
    "    \"base_features\": [\"lat\", \"lon\", \"speed\", \"course\", \"depth\", \"dist_coast_km\", \"dist_port_km\"],\n",
    "    # Parámetros de ingeniería\n",
    "    \"lags\": [1, 2],\n",
    "    \"ema_spans\": [3, 5],\n",
    "    \"window_size\": 10,    # T\n",
    "    \"step_size\": 5,\n",
    "    # Modelado\n",
    "    \"svm_nu_grid\": [0.01, 0.05, 0.1],\n",
    "    \"svm_gamma_grid\": [\"scale\", 0.1, 0.01],\n",
    "    \"kernel\": \"rbf\",\n",
    "    # Evaluación\n",
    "    \"kfold_splits\": 5,\n",
    "    # Salidas\n",
    "    \"out_dir\": \"runs/ocsvm\",\n",
    "    \"artifact_prefix\": \"ocsvm_rbf\"\n",
    "}\n",
    "\n",
    "os.makedirs(CFG[\"out_dir\"], exist_ok=True)\n",
    "print(\"Config:\", json.dumps(CFG, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01d0d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    # Coordinates in decimal degrees\n",
    "    R = 6371.0\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "def angular_diff_deg(a, b):\n",
    "    # minimal signed angle difference in degrees\n",
    "    diff = (a - b + 180) % 360 - 180\n",
    "    return diff\n",
    "\n",
    "def ensure_datetime(df, col):\n",
    "    if not np.issubdtype(df[col].dtype, np.datetime64):\n",
    "        df[col] = pd.to_datetime(df[col], utc=True, errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def per_mmsi_sort(df, mmsi_col, ts_col):\n",
    "    return df.sort_values([mmsi_col, ts_col]).reset_index(drop=True)\n",
    "\n",
    "def add_temporal_dynamics(df, mmsi_col, ts_col):\n",
    "    # Requires sorted df\n",
    "    df = df.copy()\n",
    "    # segment distance (km) between consecutive points\n",
    "    df[\"lat_prev\"] = df.groupby(mmsi_col)[\"lat\"].shift(1)\n",
    "    df[\"lon_prev\"] = df.groupby(mmsi_col)[\"lon\"].shift(1)\n",
    "    df[\"segment_km\"] = haversine_km(df[\"lat_prev\"], df[\"lon_prev\"], df[\"lat\"], df[\"lon\"])\n",
    "    df[\"segment_km\"] = df[\"segment_km\"].fillna(0.0)\n",
    "\n",
    "    # delta time (seconds)\n",
    "    df[\"t_prev\"] = df.groupby(mmsi_col)[ts_col].shift(1)\n",
    "    dt = (df[ts_col] - df[\"t_prev\"]).dt.total_seconds()\n",
    "    df[\"delta_t_s\"] = dt.fillna(0.0)\n",
    "\n",
    "    # speed (knots) -> m/s conversion if needed; assume 'speed' es nudo; 1 knot = 0.514444 m/s\n",
    "    # Si tu columna ya está en m/s, ajusta esta sección.\n",
    "    spd_ms = df[\"speed\"].astype(float) * 0.514444\n",
    "    df[\"acc_ms2\"] = (spd_ms - spd_ms.groupby(df[mmsi_col]).shift(1)) / df[\"delta_t_s\"].replace(0, np.nan)\n",
    "    df[\"acc_ms2\"] = df[\"acc_ms2\"].fillna(0.0)\n",
    "\n",
    "    # angular velocity (deg/s) con diferencia mínima de ángulo\n",
    "    df[\"course_prev\"] = df.groupby(mmsi_col)[\"course\"].shift(1)\n",
    "    dtheta = angular_diff_deg(df[\"course\"], df[\"course_prev\"])\n",
    "    df[\"ang_vel_deg_s\"] = (dtheta / df[\"delta_t_s\"].replace(0, np.nan)).fillna(0.0)\n",
    "\n",
    "    # ratios robustos\n",
    "    df[\"ratio_coast_port\"] = df[\"dist_coast_km\"] / (df[\"dist_port_km\"].replace(0, np.nan))\n",
    "    df[\"ratio_coast_port\"] = df[\"ratio_coast_port\"].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    df.drop(columns=[\"lat_prev\", \"lon_prev\", \"t_prev\", \"course_prev\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "def add_lags_ema(df, mmsi_col, cols, lags=[1,2], ema_spans=[3,5]):\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        for L in lags:\n",
    "            df[f\"{c}_lag{L}\"] = df.groupby(mmsi_col)[c].shift(L)\n",
    "        for span in ema_spans:\n",
    "            df[f\"{c}_ema{span}\"] = df.groupby(mmsi_col)[c].transform(lambda s: s.ewm(span=span, adjust=False).mean())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac25629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_windows(df, mmsi_col, ts_col, feature_cols, T=10, step=5):\n",
    "    # df sorted per mmsi, ts; we assume it\n",
    "    X = []\n",
    "    groups = []\n",
    "    idx_rows = []  # track terminal row index per window if useful\n",
    "    by_mmsi = df.groupby(mmsi_col, sort=False)\n",
    "    for mmsi, g in by_mmsi:\n",
    "        g = g.reset_index(drop=True)\n",
    "        n = len(g)\n",
    "        for start in range(0, max(0, n - T + 1), step):\n",
    "            end = start + T\n",
    "            if end > n: break\n",
    "            win = g.iloc[start:end]\n",
    "            X.append(win[feature_cols].to_numpy().reshape(-1))  # flatten (T*F)\n",
    "            groups.append(mmsi)\n",
    "            idx_rows.append(win.index[-1])\n",
    "    X = np.array(X, dtype=np.float32) if len(X) else np.empty((0, len(feature_cols)*T), dtype=np.float32)\n",
    "    return X, np.array(groups), np.array(idx_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b543aca2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/ais_enriched.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- Carga de datos ---\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Asegurar tipos y orden\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m ensure_datetime(df, CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp_col\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pandas/io/parquet.py:670\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    668\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pandas/io/parquet.py:265\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    263\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    273\u001b[0m         path_or_handle,\n\u001b[1;32m    274\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    278\u001b[0m     )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pandas/io/parquet.py:139\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    129\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pandas/io/common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/ais_enriched.parquet'"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Carga de datos ---\n",
    "df = pd.read_parquet(CFG[\"input_parquet\"])\n",
    "\n",
    "# Asegurar tipos y orden\n",
    "df = ensure_datetime(df, CFG[\"timestamp_col\"])\n",
    "df = per_mmsi_sort(df, CFG[\"mmsi_col\"], CFG[\"timestamp_col\"])\n",
    "\n",
    "# Ingeniería de variables dinámicas\n",
    "df = add_temporal_dynamics(df, CFG[\"mmsi_col\"], CFG[\"timestamp_col\"])\n",
    "\n",
    "# Lags y EMAs sobre subconjunto de columnas continuas relevantes\n",
    "cont_cols = [\"speed\", \"course\", \"depth\", \"segment_km\", \"delta_t_s\", \"acc_ms2\", \"ang_vel_deg_s\", \"dist_coast_km\", \"dist_port_km\", \"ratio_coast_port\"]\n",
    "df = add_lags_ema(df, CFG[\"mmsi_col\"], cont_cols, lags=CFG[\"lags\"], ema_spans=CFG[\"ema_spans\"])\n",
    "\n",
    "# Selección de features finales (excluye etiquetas/ids)\n",
    "feature_cols = [c for c in df.columns if c not in [CFG[\"train_filter_col\"], CFG[\"mmsi_col\"], CFG[\"timestamp_col\"], \"lat\", \"lon\"]]\n",
    "print(\"N features:\", len(feature_cols))\n",
    "print(\"Ejemplo de features:\", feature_cols[:20])\n",
    "\n",
    "# Split por etiqueta (solo para EVALUACIÓN; entrenamiento NO usa la etiqueta)\n",
    "mask_norm = (df[CFG[\"train_filter_col\"]] == 0)\n",
    "mask_susp = (df[CFG[\"train_filter_col\"]] == 1)\n",
    "\n",
    "print(\"Puntos normales:\", int(mask_norm.sum()))\n",
    "print(\"Puntos sospechosos:\", int(mask_susp.sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31059658",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T = CFG[\"window_size\"]; step = CFG[\"step_size\"]\n",
    "\n",
    "# Construimos ventanas en TODO el dataset — etiquetamos por la proporción de puntos sospechosos dentro de la ventana\n",
    "# Para entrenamiento, luego filtramos a ventanas completamente normales (o con umbral bajo de sospechosos)\n",
    "X_all, groups_all, idx_rows_all = build_windows(df, CFG[\"mmsi_col\"], CFG[\"timestamp_col\"], feature_cols, T, step)\n",
    "\n",
    "# Derivamos una etiqueta de ventana basada en 'is_suspicious' de los puntos contenidos\n",
    "# Truco: usamos el índice de fila terminal de cada ventana para recuperar MMSI y tiempo aproximado para análisis adicional\n",
    "y_point = df[CFG[\"train_filter_col\"]].fillna(0).astype(int).to_numpy()\n",
    "\n",
    "# Para cada ventana, estimamos si contiene algún punto sospechoso dentro del rango [end-T+1, end]\n",
    "# Como no guardamos los índices de todos los puntos, aproximamos con una rolling sobre la serie original\n",
    "# Alternativa: reconstruir mejor el mapping índice->ventana si es necesario.\n",
    "# Aquí, simplificamos: una ventana es \"sospechosa\" si el punto terminal (end) es sospechoso (proxy razonable).\n",
    "y_win = y_point[idx_rows_all] if len(idx_rows_all) else np.array([], dtype=int)\n",
    "\n",
    "print(\"Ventanas totales:\", X_all.shape, \"Sospechosas (proxy):\", int(y_win.sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced03ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filtramos ventanas de entrenamiento: solo NORMAL (proxy)\n",
    "train_mask = (y_win == 0)\n",
    "X_train = X_all[train_mask]\n",
    "groups_train = groups_all[train_mask]\n",
    "\n",
    "# Para evaluación mantendremos todas las ventanas\n",
    "X_eval = X_all\n",
    "y_eval = y_win\n",
    "groups_eval = groups_all\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"X_eval:\", X_eval.shape)\n",
    "\n",
    "# Escalado: fit en TRAIN NORMAL, transform en ambos\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_eval_sc = scaler.transform(X_eval)\n",
    "\n",
    "# Guardamos scaler\n",
    "with open(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_scaler.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28979fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Buscamos hiperparámetros por pseudo-validación \"contaminada\" mínima:\n",
    "# Al no tener negativos en train, usamos la estabilidad de scores y ratio de outliers producidos como heurística.\n",
    "# Alternativa: usar un hold-out de ventanas con baja proporción (o 0) de sospechosos para early stopping.\n",
    "param_grid = list(ParameterGrid({\n",
    "    \"nu\": CFG[\"svm_nu_grid\"],\n",
    "    \"gamma\": CFG[\"svm_gamma_grid\"]\n",
    "}))\n",
    "\n",
    "def outlier_rate(scores):\n",
    "    # decision_function: mayores -> más normal. score_samples similar. Aquí usamos predicciones del modelo.\n",
    "    # Usaremos 'predict' que devuelve +1 normal, -1 outlier.\n",
    "    neg = (scores == -1).mean()\n",
    "    return float(neg)\n",
    "\n",
    "best_cfg = None\n",
    "best_obj = None  # minimizamos desviación del target_outlier_rate (heurística) y varianza entre folds\n",
    "target_outlier_rate = 0.05\n",
    "\n",
    "gkf = GroupKFold(n_splits=min(CFG[\"kfold_splits\"], len(np.unique(groups_train))))\n",
    "results = []\n",
    "\n",
    "for p in param_grid:\n",
    "    fold_rates = []\n",
    "    for tr_idx, va_idx in gkf.split(X_train_sc, groups=groups_train):\n",
    "        Xtr, Xva = X_train_sc[tr_idx], X_train_sc[va_idx]\n",
    "        model = OneClassSVM(kernel=CFG[\"kernel\"], nu=p[\"nu\"], gamma=p[\"gamma\"])\n",
    "        model.fit(Xtr)\n",
    "        pred = model.predict(Xva)  # +1 normal, -1 outlier\n",
    "        rate = outlier_rate(pred)\n",
    "        fold_rates.append(rate)\n",
    "    rate_mean = float(np.mean(fold_rates))\n",
    "    rate_std = float(np.std(fold_rates))\n",
    "    obj = abs(rate_mean - target_outlier_rate) + rate_std\n",
    "    results.append({\"params\": p, \"rate_mean\": rate_mean, \"rate_std\": rate_std, \"obj\": obj})\n",
    "    if best_obj is None or obj < best_obj:\n",
    "        best_obj = obj\n",
    "        best_cfg = p\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values(\"obj\")\n",
    "print(\"Top 5 configs by heuristic objective:\")\n",
    "display(res_df.head(5))\n",
    "print(\"Best params:\", best_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a0b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_model = OneClassSVM(kernel=CFG[\"kernel\"], nu=best_cfg[\"nu\"], gamma=best_cfg[\"gamma\"])\n",
    "final_model.fit(X_train_sc)\n",
    "\n",
    "# Scores en eval\n",
    "# decision_function: valores altos = más normal. Convertimos a \"anomaly_score\" = -decision\n",
    "decision = final_model.decision_function(X_eval_sc)\n",
    "anomaly_score = -decision\n",
    "\n",
    "# Curvas y métricas (necesita y_eval binaria; 1=sospechoso)\n",
    "roc = roc_auc_score(y_eval, anomaly_score) if len(np.unique(y_eval)) > 1 else np.nan\n",
    "ap = average_precision_score(y_eval, anomaly_score) if len(np.unique(y_eval)) > 1 else np.nan\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_eval, anomaly_score)\n",
    "pr_auc = auc(rec, prec) if len(rec) > 1 else np.nan\n",
    "\n",
    "print(f\"ROC-AUC: {roc:.4f} | PR-AUC: {pr_auc:.4f} | AP: {ap:.4f}\")\n",
    "\n",
    "# Guardamos artefactos\n",
    "with open(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "with open(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_config.json\"), \"w\") as f:\n",
    "    json.dump(CFG | {\"best_params\": best_cfg, \"metrics\": {\"roc_auc\": float(roc), \"pr_auc\": float(pr_auc), \"ap\": float(ap)}}, f, indent=2)\n",
    "\n",
    "# Guardamos scores por ventana\n",
    "out_df = pd.DataFrame({\n",
    "    \"anomaly_score\": anomaly_score,\n",
    "    \"y_eval\": y_eval.astype(int),\n",
    "    \"mmsi\": groups_eval,\n",
    "    \"idx_end\": idx_rows_all\n",
    "})\n",
    "out_path = os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_eval_scores.parquet\")\n",
    "out_df.to_parquet(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560672e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Plots ---\n",
    "# 1) Precision-Recall curve\n",
    "plt.figure()\n",
    "plt.plot(rec, prec, linewidth=2)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall curve (Eval)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 2) Score distributions por etiqueta\n",
    "plt.figure()\n",
    "plt.hist(anomaly_score[y_eval==0], bins=50, alpha=0.6, label=\"Normal (win)\")\n",
    "plt.hist(anomaly_score[y_eval==1], bins=50, alpha=0.6, label=\"Sospechosa (win)\")\n",
    "plt.xlabel(\"Anomaly score (−decision)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribución de scores por etiqueta (Eval)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec85bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Selección de umbral por top‑k (p.ej., 1% de ventanas más anómalas)\n",
    "k_rate = 0.01\n",
    "k = max(1, int(len(anomaly_score) * k_rate))\n",
    "thr_k = np.partition(anomaly_score, -k)[-k]\n",
    "pred_k = (anomaly_score >= thr_k).astype(int)  # 1 = anómala\n",
    "\n",
    "# Métricas @k\n",
    "tp = int(((pred_k==1) & (y_eval==1)).sum())\n",
    "fp = int(((pred_k==1) & (y_eval==0)).sum())\n",
    "fn = int(((pred_k==0) & (y_eval==1)).sum())\n",
    "prec_k = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "rec_k  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "f1_k   = 2*prec_k*rec_k/(prec_k+rec_k) if (prec_k+rec_k)>0 else 0.0\n",
    "\n",
    "print(f\"@k={k_rate*100:.1f}% -> P: {prec_k:.3f} | R: {rec_k:.3f} | F1: {f1_k:.3f}  (k={k})\")\n",
    "\n",
    "# Guardamos predicciones @k\n",
    "out_topk = out_df.copy()\n",
    "out_topk[\"pred_topk\"] = pred_k\n",
    "out_topk_path = os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_preds_topk.parquet\")\n",
    "out_topk.to_parquet(out_topk_path, index=False)\n",
    "print(\"Saved:\", out_topk_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea68340",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Agregamos por mmsi para ver tasa de detección por trayectoria (porcentaje de ventanas anómalas)\n",
    "agg = out_df.assign(anom=(anomaly_score>=thr_k).astype(int)).groupby(\"mmsi\").agg(\n",
    "    n_win=(\"anomaly_score\", \"size\"),\n",
    "    anom_win=(\"anom\", \"sum\"),\n",
    "    mean_score=(\"anomaly_score\", \"mean\")\n",
    ").reset_index()\n",
    "agg[\"anom_rate\"] = agg[\"anom_win\"] / agg[\"n_win\"]\n",
    "display(agg.sort_values(\"anom_rate\", ascending=False).head(10))\n",
    "\n",
    "agg_path = os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_mmsi_agg.parquet\")\n",
    "agg.to_parquet(agg_path, index=False)\n",
    "print(\"Saved:\", agg_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50506455",
   "metadata": {},
   "source": [
    "\n",
    "## Cómo ejecutar con tus datos\n",
    "\n",
    "1. Asegúrate de colocar tu **Parquet enriquecido** en `data/ais_enriched.parquet` (o ajusta `CFG[\"input_parquet\"]`). Debe contener:\n",
    "   - Identificador: `mmsi`\n",
    "   - Tiempo: `timestamp` (timezone aware)\n",
    "   - Variables base: `lat`, `lon`, `speed`, `course`, `depth`, `dist_coast_km`, `dist_port_km`\n",
    "   - Etiqueta de validación: `is_suspicious` (0/1), solo usada para evaluación.\n",
    "\n",
    "2. Ajusta tamaños de **ventana `T`** y `step` en `CFG`.\n",
    "\n",
    "3. Ejecuta todas las celdas. Los artefactos se guardarán en `runs/ocsvm/`:\n",
    "   - `*_scaler.pkl`, `*_model.pkl`, `*_config.json`\n",
    "   - `*_eval_scores.parquet`, `*_preds_topk.parquet`, `*_mmsi_agg.parquet`\n",
    "\n",
    "4. Opcional: Integra mapas (GeoPandas + shapely) para estudiar casos dentro/de fuera de la RMG y generar figuras para el informe.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
