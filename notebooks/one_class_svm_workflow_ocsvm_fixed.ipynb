{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b8c7f97",
   "metadata": {},
   "source": [
    "\n",
    "# One‑Class SVM (OC‑SVM) — AIS Anomaly Detection (Galápagos)\n",
    "\n",
    "**Objetivo:** Entrenar y evaluar un detector no supervisado (One‑Class SVM, kernel RBF) para identificar anomalías en series de tiempo de trayectorias AIS de pesqueros alrededor de la RMG, usando **solo comportamiento normal** para el entrenamiento y validando contra puntos/ventanas marcadas como `is_suspicious = 1`.\n",
    "\n",
    "**Pautas (resumen de la planificación):**\n",
    "- Entrenar **solo con `is_suspicious = 0`** (comportamiento normal).\n",
    "- Ingeniería de variables por MMSI y orden temporal: lags, diferencias, SMA/EMA, distancia Haversine por segmento, delta de tiempo, aceleración, velocidad angular, y ratios como `distancia_a_costa/distancia_a_puerto`.\n",
    "- Serialización por **ventanas deslizantes** (tamaño `T`) y representación vectorizada `(N, T×F)` para modelos tabulares.\n",
    "- **Split por MMSI** (GroupKFold) para evitar fuga por identidad.\n",
    "- Métricas: ROC‑AUC, PR‑AUC, Precision@k, Recall@k, F1@k; análisis por ventana y por trayectoria.\n",
    "- Guardado de **artefactos**: scaler, modelo, configuración y scores para replicabilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21cd9a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {\n",
      "  \"input_parquet\": \"/teamspace/studios/profound-silver-kn8tf/ais_anomaly/data/ais_enriched.parquet\",\n",
      "  \"train_filter_col\": \"is_suspicious\",\n",
      "  \"mmsi_col\": \"mmsi\",\n",
      "  \"timestamp_col\": \"timestamp\",\n",
      "  \"base_features\": [\n",
      "    \"lat\",\n",
      "    \"lon\",\n",
      "    \"speed\",\n",
      "    \"course\",\n",
      "    \"depth\",\n",
      "    \"dist_coast_km\",\n",
      "    \"dist_port_km\"\n",
      "  ],\n",
      "  \"lags\": [\n",
      "    1,\n",
      "    2\n",
      "  ],\n",
      "  \"ema_spans\": [\n",
      "    3,\n",
      "    5\n",
      "  ],\n",
      "  \"window_size\": 10,\n",
      "  \"step_size\": 5,\n",
      "  \"svm_nu_grid\": [\n",
      "    0.01,\n",
      "    0.05,\n",
      "    0.1\n",
      "  ],\n",
      "  \"svm_gamma_grid\": [\n",
      "    \"scale\",\n",
      "    0.1,\n",
      "    0.01\n",
      "  ],\n",
      "  \"kernel\": \"rbf\",\n",
      "  \"kfold_splits\": 5,\n",
      "  \"out_dir\": \"runs/ocsvm\",\n",
      "  \"artifact_prefix\": \"ocsvm_rbf\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Setup ---\n",
    "import os, math, json, gc, pickle, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import GroupKFold, ParameterGrid\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibilidad\n",
    "import random\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# --- Configuración global ---\n",
    "CFG = {\n",
    "    # Entradas (ajusta paths a tu estructura real)\n",
    "    \"input_parquet\": \"/teamspace/studios/profound-silver-kn8tf/ais_anomaly/data/ais_enriched.parquet\",        # Dataset enriquecido (AIS + contexto)\n",
    "    \"train_filter_col\": \"is_suspicious\",                  # Col de validación (no usada para entrenar)\n",
    "    \"mmsi_col\": \"mmsi\",\n",
    "    \"timestamp_col\": \"timestamp\",\n",
    "    # Variables base esperadas en el parquet (ajusta a tus nombres)\n",
    "    \"base_features\": [\"lat\", \"lon\", \"speed\", \"course\", \"depth\", \"dist_coast_km\", \"dist_port_km\"],\n",
    "    # Parámetros de ingeniería\n",
    "    \"lags\": [1, 2],\n",
    "    \"ema_spans\": [3, 5],\n",
    "    \"window_size\": 10,    # T\n",
    "    \"step_size\": 5,\n",
    "    # Modelado\n",
    "    \"svm_nu_grid\": [0.01, 0.05, 0.1],\n",
    "    \"svm_gamma_grid\": [\"scale\", 0.1, 0.01],\n",
    "    \"kernel\": \"rbf\",\n",
    "    # Evaluación\n",
    "    \"kfold_splits\": 5,\n",
    "    # Salidas\n",
    "    \"out_dir\": \"runs/ocsvm\",\n",
    "    \"artifact_prefix\": \"ocsvm_rbf\"\n",
    "}\n",
    "\n",
    "os.makedirs(CFG[\"out_dir\"], exist_ok=True)\n",
    "print(\"Config:\", json.dumps(CFG, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01d0d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    # Coordinates in decimal degrees\n",
    "    R = 6371.0\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "def angular_diff_deg(a, b):\n",
    "    # minimal signed angle difference in degrees\n",
    "    diff = (a - b + 180) % 360 - 180\n",
    "    return diff\n",
    "\n",
    "def ensure_datetime(df, col):\n",
    "    if not np.issubdtype(df[col].dtype, np.datetime64):\n",
    "        df[col] = pd.to_datetime(df[col], utc=True, errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def per_mmsi_sort(df, mmsi_col, ts_col):\n",
    "    return df.sort_values([mmsi_col, ts_col]).reset_index(drop=True)\n",
    "\n",
    "def add_temporal_dynamics(df, mmsi_col, ts_col):\n",
    "    # Requires sorted df\n",
    "    df = df.copy()\n",
    "    # segment distance (km) between consecutive points\n",
    "    df[\"lat_prev\"] = df.groupby(mmsi_col)[\"lat\"].shift(1)\n",
    "    df[\"lon_prev\"] = df.groupby(mmsi_col)[\"lon\"].shift(1)\n",
    "    df[\"segment_km\"] = haversine_km(df[\"lat_prev\"], df[\"lon_prev\"], df[\"lat\"], df[\"lon\"])\n",
    "    df[\"segment_km\"] = df[\"segment_km\"].fillna(0.0)\n",
    "\n",
    "    # delta time (seconds)\n",
    "    df[\"t_prev\"] = df.groupby(mmsi_col)[ts_col].shift(1)\n",
    "    dt = (df[ts_col] - df[\"t_prev\"]).dt.total_seconds()\n",
    "    df[\"delta_t_s\"] = dt.fillna(0.0)\n",
    "\n",
    "    # speed (knots) -> m/s conversion if needed; assume 'speed' es nudo; 1 knot = 0.514444 m/s\n",
    "    # Si tu columna ya está en m/s, ajusta esta sección.\n",
    "    spd_ms = df[\"speed\"].astype(float) * 0.514444\n",
    "    df[\"acc_ms2\"] = (spd_ms - spd_ms.groupby(df[mmsi_col]).shift(1)) / df[\"delta_t_s\"].replace(0, np.nan)\n",
    "    df[\"acc_ms2\"] = df[\"acc_ms2\"].fillna(0.0)\n",
    "\n",
    "    # angular velocity (deg/s) con diferencia mínima de ángulo\n",
    "    df[\"course_prev\"] = df.groupby(mmsi_col)[\"course\"].shift(1)\n",
    "    dtheta = angular_diff_deg(df[\"course\"], df[\"course_prev\"])\n",
    "    df[\"ang_vel_deg_s\"] = (dtheta / df[\"delta_t_s\"].replace(0, np.nan)).fillna(0.0)\n",
    "\n",
    "    # ratios robustos\n",
    "    df[\"ratio_coast_port\"] = df[\"dist_coast_km\"] / (df[\"dist_port_km\"].replace(0, np.nan))\n",
    "    df[\"ratio_coast_port\"] = df[\"ratio_coast_port\"].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    df.drop(columns=[\"lat_prev\", \"lon_prev\", \"t_prev\", \"course_prev\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "def add_lags_ema(df, mmsi_col, cols, lags=[1,2], ema_spans=[3,5]):\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        for L in lags:\n",
    "            df[f\"{c}_lag{L}\"] = df.groupby(mmsi_col)[c].shift(L)\n",
    "        for span in ema_spans:\n",
    "            df[f\"{c}_ema{span}\"] = df.groupby(mmsi_col)[c].transform(lambda s: s.ewm(span=span, adjust=False).mean())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac25629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_windows(df, mmsi_col, ts_col, feature_cols, T=10, step=5):\n",
    "    # df sorted per mmsi, ts; we assume it\n",
    "    X = []\n",
    "    groups = []\n",
    "    idx_rows = []  # track terminal row index per window if useful\n",
    "    by_mmsi = df.groupby(mmsi_col, sort=False)\n",
    "    for mmsi, g in by_mmsi:\n",
    "        g = g.reset_index(drop=True)\n",
    "        n = len(g)\n",
    "        for start in range(0, max(0, n - T + 1), step):\n",
    "            end = start + T\n",
    "            if end > n: break\n",
    "            win = g.iloc[start:end]\n",
    "            X.append(win[feature_cols].to_numpy().reshape(-1))  # flatten (T*F)\n",
    "            groups.append(mmsi)\n",
    "            idx_rows.append(win.index[-1])\n",
    "    X = np.array(X, dtype=np.float32) if len(X) else np.empty((0, len(feature_cols)*T), dtype=np.float32)\n",
    "    return X, np.array(groups), np.array(idx_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543aca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input parquet: /teamspace/studios/profound-silver-kn8tf/ais_anomaly/data\n"
     ]
    }
   ],
   "source": [
    "# --- Carga de datos ---\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "def resolve_input_parquet(cfg):\n",
    "    \"\"\"Resuelve la ruta real del parquet de entrada.\n",
    "    - Primero intenta cfg[\"input_parquet\"].\n",
    "    - Luego intenta candidatos comunes.\n",
    "    - Finalmente hace un glob recursivo buscando archivos .parquet con nombres típicos.\n",
    "    \"\"\"\n",
    "    p = Path(cfg[\"input_parquet\"])\n",
    "    if p.exists():\n",
    "        return str(p)\n",
    "\n",
    "    candidates = [\n",
    "        Path(\"/teamspace/studios/profound-silver-kn8tf/ais_anomaly/data\"),\n",
    "        Path(\"data/ais_enriched.parquet\"),\n",
    "        Path(\"/mnt/data/ais_enriched.parquet\"),\n",
    "        Path(\"/workspace/data/ais_enriched.parquet\"),\n",
    "        Path(\"/workspace/ais_enriched.parquet\"),\n",
    "        Path(os.environ.get(\"AIS_PARQUET\", \"\")) if os.environ.get(\"AIS_PARQUET\") else None,\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c and c.exists():\n",
    "            return str(c)\n",
    "\n",
    "    # Búsqueda amplia (recursiva). Filtra por nombres comunes para evitar abrir algo enorme por error.\n",
    "    hits = [Path(h) for h in glob.glob(\"**/*.parquet\", recursive=True)]\n",
    "    name_whitelist = {\"ais_enriched.parquet\", \"ais_clean.parquet\", \"ais_windows.parquet\"}\n",
    "    prio = [h for h in hits if h.name in name_whitelist] or hits\n",
    "\n",
    "    if prio:\n",
    "        print(\"[resolve_input_parquet] Usando:\", prio[0])\n",
    "        return str(prio[0])\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"No se encontró el parquet de entrada. \"\n",
    "        \"Actualiza CFG['input_parquet'] con la ruta correcta o define AIS_PARQUET en el entorno.\"\n",
    "    )\n",
    "\n",
    "CFG[\"input_parquet\"] = resolve_input_parquet(CFG)\n",
    "print(\"Input parquet:\", CFG[\"input_parquet\"])\n",
    "\n",
    "df = pd.read_parquet(CFG[\"input_parquet\"])\n",
    "\n",
    "# Asegurar tipos y orden\n",
    "df = ensure_datetime(df, CFG[\"timestamp_col\"])\n",
    "df = per_mmsi_sort(df, CFG[\"mmsi_col\"], CFG[\"timestamp_col\"])\n",
    "\n",
    "# Ingeniería de variables dinámicas\n",
    "df = add_temporal_dynamics(df, CFG[\"mmsi_col\"], CFG[\"timestamp_col\"])\n",
    "\n",
    "# Lags y EMAs sobre subconjunto de columnas continuas relevantes\n",
    "cont_cols = [\"speed\", \"course\", \"depth\", \"segment_km\", \"delta_t_s\", \"acc_ms2\", \"ang_vel_deg_s\", \"dist_coast_km\", \"dist_port_km\", \"ratio_coast_port\"]\n",
    "df = add_lags_ema(df, CFG[\"mmsi_col\"], cont_cols, lags=CFG[\"lags\"], ema_spans=CFG[\"ema_spans\"])\n",
    "\n",
    "# Selección de features finales (excluye etiquetas/ids)\n",
    "feature_cols = [c for c in df.columns if c not in [CFG[\"train_filter_col\"], CFG[\"mmsi_col\"], CFG[\"timestamp_col\"], \"lat\", \"lon\"]]\n",
    "print(\"N features:\", len(feature_cols))\n",
    "print(\"Ejemplo de features:\", feature_cols[:20])\n",
    "\n",
    "# Split por etiqueta (solo para EVALUACIÓN; entrenamiento NO usa la etiqueta)\n",
    "mask_norm = (df[CFG[\"train_filter_col\"]] == 0)\n",
    "mask_susp = (df[CFG[\"train_filter_col\"]] == 1)\n",
    "\n",
    "print(\"Puntos normales:\", int(mask_norm.sum()))\n",
    "print(\"Puntos sospechosos:\", int(mask_susp.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31059658",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T = CFG[\"window_size\"]; step = CFG[\"step_size\"]\n",
    "\n",
    "# Construimos ventanas en TODO el dataset — etiquetamos por la proporción de puntos sospechosos dentro de la ventana\n",
    "# Para entrenamiento, luego filtramos a ventanas completamente normales (o con umbral bajo de sospechosos)\n",
    "X_all, groups_all, idx_rows_all = build_windows(df, CFG[\"mmsi_col\"], CFG[\"timestamp_col\"], feature_cols, T, step)\n",
    "\n",
    "# Derivamos una etiqueta de ventana basada en 'is_suspicious' de los puntos contenidos\n",
    "# Truco: usamos el índice de fila terminal de cada ventana para recuperar MMSI y tiempo aproximado para análisis adicional\n",
    "y_point = df[CFG[\"train_filter_col\"]].fillna(0).astype(int).to_numpy()\n",
    "\n",
    "# Para cada ventana, estimamos si contiene algún punto sospechoso dentro del rango [end-T+1, end]\n",
    "# Como no guardamos los índices de todos los puntos, aproximamos con una rolling sobre la serie original\n",
    "# Alternativa: reconstruir mejor el mapping índice->ventana si es necesario.\n",
    "# Aquí, simplificamos: una ventana es \"sospechosa\" si el punto terminal (end) es sospechoso (proxy razonable).\n",
    "y_win = y_point[idx_rows_all] if len(idx_rows_all) else np.array([], dtype=int)\n",
    "\n",
    "print(\"Ventanas totales:\", X_all.shape, \"Sospechosas (proxy):\", int(y_win.sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced03ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filtramos ventanas de entrenamiento: solo NORMAL (proxy)\n",
    "train_mask = (y_win == 0)\n",
    "X_train = X_all[train_mask]\n",
    "groups_train = groups_all[train_mask]\n",
    "\n",
    "# Para evaluación mantendremos todas las ventanas\n",
    "X_eval = X_all\n",
    "y_eval = y_win\n",
    "groups_eval = groups_all\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"X_eval:\", X_eval.shape)\n",
    "\n",
    "# Escalado: fit en TRAIN NORMAL, transform en ambos\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_eval_sc = scaler.transform(X_eval)\n",
    "\n",
    "# Guardamos scaler\n",
    "with open(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_scaler.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28979fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Buscamos hiperparámetros por pseudo-validación \"contaminada\" mínima:\n",
    "# Al no tener negativos en train, usamos la estabilidad de scores y ratio de outliers producidos como heurística.\n",
    "# Alternativa: usar un hold-out de ventanas con baja proporción (o 0) de sospechosos para early stopping.\n",
    "param_grid = list(ParameterGrid({\n",
    "    \"nu\": CFG[\"svm_nu_grid\"],\n",
    "    \"gamma\": CFG[\"svm_gamma_grid\"]\n",
    "}))\n",
    "\n",
    "def outlier_rate(scores):\n",
    "    # decision_function: mayores -> más normal. score_samples similar. Aquí usamos predicciones del modelo.\n",
    "    # Usaremos 'predict' que devuelve +1 normal, -1 outlier.\n",
    "    neg = (scores == -1).mean()\n",
    "    return float(neg)\n",
    "\n",
    "best_cfg = None\n",
    "best_obj = None  # minimizamos desviación del target_outlier_rate (heurística) y varianza entre folds\n",
    "target_outlier_rate = 0.05\n",
    "\n",
    "gkf = GroupKFold(n_splits=min(CFG[\"kfold_splits\"], len(np.unique(groups_train))))\n",
    "results = []\n",
    "\n",
    "for p in param_grid:\n",
    "    fold_rates = []\n",
    "    for tr_idx, va_idx in gkf.split(X_train_sc, groups=groups_train):\n",
    "        Xtr, Xva = X_train_sc[tr_idx], X_train_sc[va_idx]\n",
    "        model = OneClassSVM(kernel=CFG[\"kernel\"], nu=p[\"nu\"], gamma=p[\"gamma\"])\n",
    "        model.fit(Xtr)\n",
    "        pred = model.predict(Xva)  # +1 normal, -1 outlier\n",
    "        rate = outlier_rate(pred)\n",
    "        fold_rates.append(rate)\n",
    "    rate_mean = float(np.mean(fold_rates))\n",
    "    rate_std = float(np.std(fold_rates))\n",
    "    obj = abs(rate_mean - target_outlier_rate) + rate_std\n",
    "    results.append({\"params\": p, \"rate_mean\": rate_mean, \"rate_std\": rate_std, \"obj\": obj})\n",
    "    if best_obj is None or obj < best_obj:\n",
    "        best_obj = obj\n",
    "        best_cfg = p\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values(\"obj\")\n",
    "print(\"Top 5 configs by heuristic objective:\")\n",
    "display(res_df.head(5))\n",
    "print(\"Best params:\", best_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a0b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_model = OneClassSVM(kernel=CFG[\"kernel\"], nu=best_cfg[\"nu\"], gamma=best_cfg[\"gamma\"])\n",
    "final_model.fit(X_train_sc)\n",
    "\n",
    "# Scores en eval\n",
    "# decision_function: valores altos = más normal. Convertimos a \"anomaly_score\" = -decision\n",
    "decision = final_model.decision_function(X_eval_sc)\n",
    "anomaly_score = -decision\n",
    "\n",
    "# Curvas y métricas (necesita y_eval binaria; 1=sospechoso)\n",
    "roc = roc_auc_score(y_eval, anomaly_score) if len(np.unique(y_eval)) > 1 else np.nan\n",
    "ap = average_precision_score(y_eval, anomaly_score) if len(np.unique(y_eval)) > 1 else np.nan\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_eval, anomaly_score)\n",
    "pr_auc = auc(rec, prec) if len(rec) > 1 else np.nan\n",
    "\n",
    "print(f\"ROC-AUC: {roc:.4f} | PR-AUC: {pr_auc:.4f} | AP: {ap:.4f}\")\n",
    "\n",
    "# Guardamos artefactos\n",
    "with open(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "with open(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_config.json\"), \"w\") as f:\n",
    "    json.dump(CFG | {\"best_params\": best_cfg, \"metrics\": {\"roc_auc\": float(roc), \"pr_auc\": float(pr_auc), \"ap\": float(ap)}}, f, indent=2)\n",
    "\n",
    "# Guardamos scores por ventana\n",
    "out_df = pd.DataFrame({\n",
    "    \"anomaly_score\": anomaly_score,\n",
    "    \"y_eval\": y_eval.astype(int),\n",
    "    \"mmsi\": groups_eval,\n",
    "    \"idx_end\": idx_rows_all\n",
    "})\n",
    "out_path = os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_eval_scores.parquet\")\n",
    "out_df.to_parquet(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560672e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Plots ---\n",
    "# 1) Precision-Recall curve\n",
    "plt.figure()\n",
    "plt.plot(rec, prec, linewidth=2)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall curve (Eval)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 2) Score distributions por etiqueta\n",
    "plt.figure()\n",
    "plt.hist(anomaly_score[y_eval==0], bins=50, alpha=0.6, label=\"Normal (win)\")\n",
    "plt.hist(anomaly_score[y_eval==1], bins=50, alpha=0.6, label=\"Sospechosa (win)\")\n",
    "plt.xlabel(\"Anomaly score (−decision)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribución de scores por etiqueta (Eval)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec85bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Selección de umbral por top‑k (p.ej., 1% de ventanas más anómalas)\n",
    "k_rate = 0.01\n",
    "k = max(1, int(len(anomaly_score) * k_rate))\n",
    "thr_k = np.partition(anomaly_score, -k)[-k]\n",
    "pred_k = (anomaly_score >= thr_k).astype(int)  # 1 = anómala\n",
    "\n",
    "# Métricas @k\n",
    "tp = int(((pred_k==1) & (y_eval==1)).sum())\n",
    "fp = int(((pred_k==1) & (y_eval==0)).sum())\n",
    "fn = int(((pred_k==0) & (y_eval==1)).sum())\n",
    "prec_k = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "rec_k  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "f1_k   = 2*prec_k*rec_k/(prec_k+rec_k) if (prec_k+rec_k)>0 else 0.0\n",
    "\n",
    "print(f\"@k={k_rate*100:.1f}% -> P: {prec_k:.3f} | R: {rec_k:.3f} | F1: {f1_k:.3f}  (k={k})\")\n",
    "\n",
    "# Guardamos predicciones @k\n",
    "out_topk = out_df.copy()\n",
    "out_topk[\"pred_topk\"] = pred_k\n",
    "out_topk_path = os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_preds_topk.parquet\")\n",
    "out_topk.to_parquet(out_topk_path, index=False)\n",
    "print(\"Saved:\", out_topk_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea68340",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Agregamos por mmsi para ver tasa de detección por trayectoria (porcentaje de ventanas anómalas)\n",
    "agg = out_df.assign(anom=(anomaly_score>=thr_k).astype(int)).groupby(\"mmsi\").agg(\n",
    "    n_win=(\"anomaly_score\", \"size\"),\n",
    "    anom_win=(\"anom\", \"sum\"),\n",
    "    mean_score=(\"anomaly_score\", \"mean\")\n",
    ").reset_index()\n",
    "agg[\"anom_rate\"] = agg[\"anom_win\"] / agg[\"n_win\"]\n",
    "display(agg.sort_values(\"anom_rate\", ascending=False).head(10))\n",
    "\n",
    "agg_path = os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_mmsi_agg.parquet\")\n",
    "agg.to_parquet(agg_path, index=False)\n",
    "print(\"Saved:\", agg_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50506455",
   "metadata": {},
   "source": [
    "\n",
    "## Cómo ejecutar con tus datos\n",
    "\n",
    "1. Asegúrate de colocar tu **Parquet enriquecido** en `data/ais_enriched.parquet` (o ajusta `CFG[\"input_parquet\"]`). Debe contener:\n",
    "   - Identificador: `mmsi`\n",
    "   - Tiempo: `timestamp` (timezone aware)\n",
    "   - Variables base: `lat`, `lon`, `speed`, `course`, `depth`, `dist_coast_km`, `dist_port_km`\n",
    "   - Etiqueta de validación: `is_suspicious` (0/1), solo usada para evaluación.\n",
    "\n",
    "2. Ajusta tamaños de **ventana `T`** y `step` en `CFG`.\n",
    "\n",
    "3. Ejecuta todas las celdas. Los artefactos se guardarán en `runs/ocsvm/`:\n",
    "   - `*_scaler.pkl`, `*_model.pkl`, `*_config.json`\n",
    "   - `*_eval_scores.parquet`, `*_preds_topk.parquet`, `*_mmsi_agg.parquet`\n",
    "\n",
    "4. Opcional: Integra mapas (GeoPandas + shapely) para estudiar casos dentro/de fuera de la RMG y generar figuras para el informe.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
