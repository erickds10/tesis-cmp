{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OC-SVM · AIS Anomaly Detection (Lightning, 110 GB RAM)\n",
    "- Lectura **robusta** de parquets (auto-resuelve ruta; valida formato Parquet).\n",
    "- Preprocesamiento **memory-safe** (imputación + estándar in-place).\n",
    "- HP search **paralela** (joblib) y entrenamiento OC-SVM con **cache grande**.\n",
    "- Evaluación **reanudable** con memmap + progreso incremental.\n",
    "- Artefactos y resultados en `./data/ocsvm_runs` (seguro en este proyecto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:    erickdsuarez10\n",
      "Host:    computeinstance-e00exnkvr257g0k5f5\n",
      "Python:  /home/zeus/miniconda3/envs/cloudspace/bin/python\n",
      "CWD:     /teamspace/studios/this_studio\n",
      "/data exists?: False\n"
     ]
    }
   ],
   "source": [
    "# --- Diagnóstico del entorno ---\n",
    "import os, sys, getpass, socket\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"User:   \", getpass.getuser())\n",
    "print(\"Host:   \", socket.gethostname())\n",
    "print(\"Python: \", sys.executable)\n",
    "print(\"CWD:    \", os.getcwd())\n",
    "print(\"/data exists?:\", Path(\"/data\").exists())\n",
    "if Path(\"/data\").exists():\n",
    "    print(\"#parquets en /data:\", len(list(Path(\"/data\").glob(\"*.parquet\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LECTURA  : /teamspace/studios/this_studio/data\n",
      "SALIDAS  : /teamspace/studios/this_studio/data/ocsvm_runs\n",
      "#parquets: 17\n",
      "CFG: {\n",
      "  \"external_data_dir\": \"/teamspace/studios/this_studio/data\",\n",
      "  \"out_dir\": \"/teamspace/studios/this_studio/data/ocsvm_runs\",\n",
      "  \"artifact_prefix\": \"ocsvm_rbf\",\n",
      "  \"svm_nu_grid\": [\n",
      "    0.01,\n",
      "    0.05,\n",
      "    0.1\n",
      "  ],\n",
      "  \"svm_gamma_grid\": [\n",
      "    \"scale\",\n",
      "    0.01\n",
      "  ],\n",
      "  \"kernel\": \"rbf\",\n",
      "  \"kfold_splits\": 5,\n",
      "  \"max_train_samples\": 800000,\n",
      "  \"max_search_samples\": 400000,\n",
      "  \"eval_batch_size\": 2000000\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Config + auto-resolución de fuente de lectura (solo-lectura) y salidas locales\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "\n",
    "def count_parquets(p: Path) -> int:\n",
    "    try:\n",
    "        return len(list(p.glob(\"*.parquet\"))) if p.exists() else 0\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "CANDIDATES = [\n",
    "    Path(\"/data\").resolve(),\n",
    "    Path(\"/teamspace/studios/this_studio/data\").resolve(),\n",
    "    Path(\"./data\").resolve(),\n",
    "]\n",
    "\n",
    "EXTERNAL_DATA_DIR = None\n",
    "for cand in CANDIDATES:\n",
    "    if cand.exists() and count_parquets(cand) > 0:\n",
    "        EXTERNAL_DATA_DIR = cand\n",
    "        break\n",
    "if EXTERNAL_DATA_DIR is None:\n",
    "    # fallback: usa data local aunque esté vacía (para que no falle la carga y puedas copiar ahí)\n",
    "    EXTERNAL_DATA_DIR = Path(\"/teamspace/studios/this_studio/data\").resolve()\n",
    "    print(\"⚠️ No hallé parquets; usando ruta local del Studio por defecto:\", EXTERNAL_DATA_DIR)\n",
    "\n",
    "OUT_DIR = Path(\"data/ocsvm_runs\").resolve()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CFG = {\n",
    "    \"external_data_dir\": str(EXTERNAL_DATA_DIR),\n",
    "    \"out_dir\": str(OUT_DIR),\n",
    "    \"artifact_prefix\": \"ocsvm_rbf\",\n",
    "    # HP + límites\n",
    "    \"svm_nu_grid\": [0.01, 0.05, 0.1],\n",
    "    \"svm_gamma_grid\": [\"scale\", 0.01],\n",
    "    \"kernel\": \"rbf\",\n",
    "    \"kfold_splits\": 5,\n",
    "    \"max_train_samples\": 800_000,     # ajustados para 110 GB RAM\n",
    "    \"max_search_samples\": 400_000,\n",
    "    \"eval_batch_size\": 2_000_000,     # lotes grandes para acelerar evaluación\n",
    "}\n",
    "\n",
    "print(\"LECTURA  :\", CFG[\"external_data_dir\"])\n",
    "print(\"SALIDAS  :\", CFG[\"out_dir\"])\n",
    "print(\"#parquets:\", count_parquets(Path(CFG[\"external_data_dir\"])))\n",
    "print(\"CFG:\", json.dumps({k: CFG[k] for k in CFG}, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores: 32 -> N_JOBS=31\n",
      "RAM disponible: 129.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Performance pack (usa todos los cores menos 1)\n",
    "import os, psutil\n",
    "\n",
    "N_JOBS = max(1, os.cpu_count() - 1)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(N_JOBS)\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(N_JOBS)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(N_JOBS)\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(N_JOBS)\n",
    "\n",
    "print(f\"Cores: {os.cpu_count()} -> N_JOBS={N_JOBS}\")\n",
    "print(f\"RAM disponible: {psutil.virtual_memory().available/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores visibles: 32 | N_JOBS=31\n",
      "RAM disponible aprox: 129.0 GB\n",
      "CFG actualizado:  {'max_train_samples': 800000, 'max_search_samples': 400000, 'eval_batch_size': 2000000}\n"
     ]
    }
   ],
   "source": [
    "# --- Performance pack (RAM 110 GB + todos los cores) ---\n",
    "import os, multiprocessing, psutil\n",
    "\n",
    "# Usa (n_cores - 1) para no saturar el sistema\n",
    "N_JOBS = max(1, os.cpu_count() - 1)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(N_JOBS)\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(N_JOBS)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(N_JOBS)\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(N_JOBS)\n",
    "\n",
    "print(f\"Cores visibles: {os.cpu_count()} | N_JOBS={N_JOBS}\")\n",
    "print(f\"RAM disponible aprox: {psutil.virtual_memory().available/1e9:.1f} GB\")\n",
    "\n",
    "# Recomendaciones de tamaño con 110 GB RAM (float32 => ~76 bytes/fila si F=19)\n",
    "# Ajusta tu CFG después de ejecutar esta celda:\n",
    "CFG[\"max_train_samples\"]  = 800_000     # si tu entrenamiento tarda demasiado, bájalo a 500k\n",
    "CFG[\"max_search_samples\"] = 400_000\n",
    "CFG[\"eval_batch_size\"]    = 2_000_000   # sube el lote de evaluación (menos overhead)\n",
    "\n",
    "# Guardar\n",
    "print(\"CFG actualizado: \",\n",
    "      {k: CFG[k] for k in [\"max_train_samples\",\"max_search_samples\",\"eval_batch_size\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: /teamspace/studios/this_studio/data\n",
      "TRAIN -> windows_aligned_normal.parquet | X_train: (27789660, 19)\n",
      "[pick_valid] → windows_with_labels_aligned_norm.parquet (pattern *windows_with_labels_aligned_norm*.parquet)\n",
      "[pick_valid] → eval_windows_aligned_norm.parquet (pattern *eval_windows_aligned_norm*.parquet)\n",
      "EVAL -> windows_with_labels_aligned.parquet | X_eval: (27789660, 19) | y_eval: (27789660,)\n",
      "Train -> X: (27789660, 19) | groups: 27789660\n",
      "Eval  -> X: (27789660, 19) | y: (27789660,) | groups: 27789660\n",
      "N feats: train 19 | eval 19\n"
     ]
    }
   ],
   "source": [
    "# Carga robusta desde CFG[\"external_data_dir\"] (con validación Parquet)\n",
    "import numpy as np, pandas as pd, gc\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "DATA_DIR = Path(CFG[\"external_data_dir\"])\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "\n",
    "def is_valid_parquet(path: Path) -> bool:\n",
    "    try:\n",
    "        pq.ParquetFile(path); return True\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] No Parquet válido -> {path.name} :: {type(e).__name__}\")\n",
    "        return False\n",
    "\n",
    "def read_parquet_min(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_parquet(path, engine=\"pyarrow\")\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_float_dtype(df[c]): df[c] = df[c].astype(np.float32)\n",
    "        elif pd.api.types.is_integer_dtype(df[c]) and df[c].max() <= np.iinfo(np.int32).max:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "    return df\n",
    "\n",
    "def detect_label_col(df):\n",
    "    for k in [\"y\",\"label\",\"is_suspicious\",\"target\"]:\n",
    "        if k in df.columns: return k\n",
    "    return None\n",
    "\n",
    "def detect_group_col(df):\n",
    "    for k in [\"mmsi\",\"group\",\"ship_id\"]:\n",
    "        if k in df.columns: return k\n",
    "    return None\n",
    "\n",
    "def pick_valid(base: Path, *candidates):\n",
    "    # exactos primero\n",
    "    for n in candidates:\n",
    "        if \"*\" not in n and \"?\" not in n and \"[\" not in n:\n",
    "            p = base / n\n",
    "            if p.exists() and is_valid_parquet(p): return p\n",
    "    # luego patrones (elige el mayor por tamaño que sea válido)\n",
    "    for patt in candidates:\n",
    "        if any(ch in patt for ch in \"*?[]\"):\n",
    "            matches = list(base.glob(patt))\n",
    "            matches.sort(key=lambda x: x.stat().st_size if x.exists() else 0, reverse=True)\n",
    "            for m in matches:\n",
    "                if is_valid_parquet(m): \n",
    "                    print(f\"[pick_valid] → {m.name} (pattern {patt})\"); \n",
    "                    return m\n",
    "    return None\n",
    "\n",
    "# --- TRAIN (ventanas normales) ---\n",
    "train_path = pick_valid(\n",
    "    DATA_DIR,\n",
    "    \"windows_aligned_normal.parquet\",\n",
    "    \"norm_windows_flat.parquet\",\n",
    "    \"ais_norm_windows.parquet\",\n",
    "    \"*windows_aligned_norm*.parquet\",\n",
    "    \"*norm*windows*.parquet\",\n",
    "    \"*ais_norm_windows*.parquet\",\n",
    ")\n",
    "if train_path is None:\n",
    "    raise FileNotFoundError(\"No encontré TRAIN normal válido en la ruta de datos.\")\n",
    "df_tr = read_parquet_min(train_path)\n",
    "\n",
    "ycol_tr = detect_label_col(df_tr)\n",
    "gcol_tr = detect_group_col(df_tr)\n",
    "drop_common = {\"lat\",\"lon\",\"idx\",\"idx_end\",\"window_id\"}\n",
    "drop_train = set([c for c in [ycol_tr, gcol_tr] if c]) | drop_common\n",
    "feat_tr = [c for c in df_tr.columns if c not in drop_train]\n",
    "X_train = df_tr[feat_tr].to_numpy(dtype=np.float32)\n",
    "groups_train = df_tr[gcol_tr].to_numpy() if gcol_tr else None\n",
    "print(\"TRAIN ->\", train_path.name, \"| X_train:\", X_train.shape)\n",
    "\n",
    "# --- EVAL (único o split) ---\n",
    "eval_single = pick_valid(DATA_DIR,\n",
    "    \"windows_with_labels_aligned.parquet\", \"*windows_with_labels_aligned*.parquet\",\n",
    "    \"eval_windows_aligned.parquet\", \"*eval_windows_aligned*.parquet\",\n",
    "    \"windows_with_labels.parquet\", \"*windows_with_labels*.parquet\",\n",
    ")\n",
    "eval_wl_norm = pick_valid(DATA_DIR, \"windows_with_labels_aligned_normal.parquet\",\n",
    "                          \"*windows_with_labels_aligned_norm*.parquet\", \"*windows_with_labels_aligned_normal*.parquet\")\n",
    "eval_wl_anom = pick_valid(DATA_DIR, \"windows_with_labels_aligned_anom.parquet\",\n",
    "                          \"*windows_with_labels_aligned_anom*.parquet\")\n",
    "eval_norm    = pick_valid(DATA_DIR, \"eval_windows_aligned_normal.parquet\",\n",
    "                          \"*eval_windows_aligned_norm*.parquet\", \"*eval_windows_aligned_normal*.parquet\")\n",
    "eval_anom    = pick_valid(DATA_DIR, \"eval_windows_aligned_anom.parquet\",\n",
    "                          \"*eval_windows_aligned_anom*.parquet\")\n",
    "labels_any   = pick_valid(DATA_DIR, \"eval_labels_aligned.parquet\", \"*eval_labels_aligned*.parquet\",\n",
    "                          \"labels.parquet\", \"labels_anom.parquet\", \"*labels*.parquet\")\n",
    "\n",
    "if eval_single is not None:\n",
    "    df_ev = read_parquet_min(eval_single)\n",
    "    ycol_ev = detect_label_col(df_ev)\n",
    "    gcol_ev = detect_group_col(df_ev)\n",
    "    if ycol_ev is None:\n",
    "        if labels_any is None: raise FileNotFoundError(\"Eval único sin etiquetas y no hay archivo de labels.\")\n",
    "        df_y = read_parquet_min(labels_any)\n",
    "        ycol_y = detect_label_col(df_y) or df_y.select_dtypes(include=[\"int8\",\"int16\",\"int32\"]).columns[-1]\n",
    "        if len(df_y) != len(df_ev): raise ValueError(f\"Desalineación eval vs labels: {len(df_ev)} vs {len(df_y)}\")\n",
    "        drop_eval = set([gcol_ev]) | drop_common\n",
    "        feat_ev = [c for c in df_ev.columns if c not in drop_eval]\n",
    "        X_eval = df_ev[feat_ev].to_numpy(dtype=np.float32)\n",
    "        y_eval = df_y[ycol_y].astype(np.int8).to_numpy()\n",
    "    else:\n",
    "        drop_eval = set([ycol_ev, gcol_ev]) | drop_common\n",
    "        feat_ev = [c for c in df_ev.columns if c not in drop_eval]\n",
    "        X_eval = df_ev[feat_ev].to_numpy(dtype=np.float32)\n",
    "        y_eval = df_ev[ycol_ev].astype(np.int8).to_numpy()\n",
    "    groups_eval = df_ev[gcol_ev].to_numpy() if gcol_ev else None\n",
    "    print(\"EVAL ->\", eval_single.name, \"| X_eval:\", X_eval.shape, \"| y_eval:\", y_eval.shape)\n",
    "\n",
    "elif eval_wl_norm is not None and eval_wl_anom is not None:\n",
    "    dn, da = read_parquet_min(eval_wl_norm), read_parquet_min(eval_wl_anom)\n",
    "    common = [c for c in dn.columns if c in da.columns]\n",
    "    dn, da = dn[common], da[common]\n",
    "    ycol_ev, gcol_ev = detect_label_col(dn), detect_group_col(dn)\n",
    "    drop_eval = set([ycol_ev, gcol_ev]) | drop_common\n",
    "    feat_ev = [c for c in common if c not in drop_eval]\n",
    "    X_eval = pd.concat([dn[feat_ev], da[feat_ev]], ignore_index=True).to_numpy(np.float32)\n",
    "    y_eval = pd.concat([dn[ycol_ev], da[ycol_ev]], ignore_index=True).astype(np.int8).to_numpy()\n",
    "    groups_eval = (pd.concat([dn[gcol_ev], da[gcol_ev]], ignore_index=True).to_numpy() if gcol_ev else None)\n",
    "    print(\"EVAL ->\", eval_wl_norm.name, \"+\", eval_wl_anom.name, \"| X_eval:\", X_eval.shape, \"| y_eval:\", y_eval.shape)\n",
    "\n",
    "elif eval_norm is not None and eval_anom is not None:\n",
    "    if labels_any is None: raise FileNotFoundError(\"Eval split sin labels embebidas y no hay archivo de labels.\")\n",
    "    dn, da, dy = read_parquet_min(eval_norm), read_parquet_min(eval_anom), read_parquet_min(labels_any)\n",
    "    common = [c for c in dn.columns if c in da.columns]\n",
    "    dn, da = dn[common], da[common]\n",
    "    gcol_ev = detect_group_col(dn)\n",
    "    drop_eval = set([gcol_ev]) | drop_common\n",
    "    feat_ev = [c for c in common if c not in drop_eval]\n",
    "    df_concat = pd.concat([dn[feat_ev], da[feat_ev]], ignore_index=True)\n",
    "    X_eval = df_concat.to_numpy(np.float32)\n",
    "    ycol_y = detect_label_col(dy) or dy.select_dtypes(include=[\"int8\",\"int16\",\"int32\"]).columns[-1]\n",
    "    if len(dy) != len(df_concat): raise ValueError(f\"Desalineación eval concat vs labels: {len(df_concat)} vs {len(dy)}\")\n",
    "    y_eval = dy[ycol_y].astype(np.int8).to_numpy()\n",
    "    groups_eval = (pd.concat([dn[gcol_ev], da[gcol_ev]], ignore_index=True).to_numpy() if gcol_ev else None)\n",
    "    print(\"EVAL ->\", eval_norm.name, \"+\", eval_anom.name, \"| X_eval:\", X_eval.shape, \"| y_eval:\", y_eval.shape)\n",
    "\n",
    "else:\n",
    "    raise FileNotFoundError(\"No se pudo resolver un set de EVAL válido.\")\n",
    "\n",
    "del df_tr; gc.collect()\n",
    "print(\"Train -> X:\", X_train.shape, \"| groups:\", None if groups_train is None else len(groups_train))\n",
    "print(\"Eval  -> X:\", X_eval.shape,  \"| y:\", y_eval.shape, \"| groups:\", None if groups_eval is None else len(groups_eval))\n",
    "print(\"N feats: train\", X_train.shape[1], \"| eval\", X_eval.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sampled: (799920, 19)\n",
      "Scaled train: (799920, 19)\n"
     ]
    }
   ],
   "source": [
    "# Imputación + estándar + muestreo (in-place, memory-safe)\n",
    "import numpy as np, os, gc\n",
    "\n",
    "def sample_by_group(n_max, X, groups):\n",
    "    if (n_max is None) or (X.shape[0] <= n_max):\n",
    "        idx = np.arange(X.shape[0]); return X, (groups if groups is not None else None), idx\n",
    "    rng = np.random.default_rng(42)\n",
    "    if groups is None:\n",
    "        idx = rng.choice(X.shape[0], n_max, replace=False); return X[idx], None, idx\n",
    "    uniq = np.unique(groups); per_g = max(1, n_max // len(uniq)); take = []\n",
    "    for g in uniq:\n",
    "        g_idx = np.where(groups == g)[0]\n",
    "        take.extend(rng.choice(g_idx, min(per_g, g_idx.size), replace=False).tolist())\n",
    "    take = np.array(take)\n",
    "    if take.size > n_max: take = rng.choice(take, n_max, replace=False)\n",
    "    return X[take], groups[take], take\n",
    "\n",
    "def colwise_nanmedian(X):\n",
    "    Xc = X.copy(); Xc[~np.isfinite(Xc)] = np.nan\n",
    "    med = np.nanmedian(Xc, axis=0)\n",
    "    med = np.where(np.isfinite(med), med, 0.0).astype(np.float32)\n",
    "    return med\n",
    "\n",
    "def impute_inplace(X, medians):\n",
    "    bad = ~np.isfinite(X)\n",
    "    if bad.any():\n",
    "        cols = np.where(bad)[1]\n",
    "        X[bad] = medians[cols]\n",
    "\n",
    "def fit_standardizer(X):\n",
    "    mean = X.mean(axis=0).astype(np.float32)\n",
    "    var  = X.var(axis=0).astype(np.float32)\n",
    "    std  = np.sqrt(var, dtype=np.float32); std[std == 0.0] = 1.0\n",
    "    return mean, std\n",
    "\n",
    "def apply_standardizer_inplace(X, mean, std):\n",
    "    X -= mean; X /= std\n",
    "\n",
    "X_train_s, groups_train_s, _ = sample_by_group(CFG[\"max_train_samples\"], X_train, groups_train)\n",
    "print(\"Train sampled:\", X_train_s.shape)\n",
    "\n",
    "X_train_s = X_train_s.astype(np.float32, copy=False)\n",
    "X_train_s[~np.isfinite(X_train_s)] = np.nan\n",
    "train_medians = colwise_nanmedian(X_train_s)\n",
    "impute_inplace(X_train_s, train_medians)\n",
    "train_mean, train_std = fit_standardizer(X_train_s)\n",
    "apply_standardizer_inplace(X_train_s, train_mean, train_std)\n",
    "\n",
    "X_train_sc = X_train_s\n",
    "groups_train = groups_train_s\n",
    "\n",
    "# Guardar preprocesamiento\n",
    "np.save(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_imputer_medians.npy\"), train_medians)\n",
    "np.savez(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_scaler_params.npz\"), mean=train_mean, std=train_std)\n",
    "\n",
    "# Transformador por lotes para eval\n",
    "def transform_eval_in_batches(X, batch_size=CFG[\"eval_batch_size\"]):\n",
    "    n = X.shape[0]\n",
    "    for s in range(0, n, batch_size):\n",
    "        e = min(s + batch_size, n)\n",
    "        Xe = X[s:e].astype(np.float32, copy=False)\n",
    "        Xe[~np.isfinite(Xe)] = np.nan\n",
    "        impute_inplace(Xe, train_medians)\n",
    "        apply_standardizer_inplace(Xe, train_mean, train_std)\n",
    "        yield s, e, Xe\n",
    "\n",
    "gc.collect()\n",
    "print(\"Scaled train:\", X_train_sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=31)]: Using backend LokyBackend with 31 concurrent workers.\n",
      "[Parallel(n_jobs=31)]: Done   3 out of   6 | elapsed: 72.9min remaining: 72.9min\n",
      "[Parallel(n_jobs=31)]: Done   6 out of   6 | elapsed: 124.8min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>rate_mean</th>\n",
       "      <th>rate_std</th>\n",
       "      <th>obj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'gamma': 0.01, 'nu': 0.05}</td>\n",
       "      <td>0.051925</td>\n",
       "      <td>0.007809</td>\n",
       "      <td>0.009734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'gamma': 'scale', 'nu': 0.05}</td>\n",
       "      <td>0.056986</td>\n",
       "      <td>0.015195</td>\n",
       "      <td>0.022181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'gamma': 0.01, 'nu': 0.01}</td>\n",
       "      <td>0.010304</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>0.041470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'gamma': 'scale', 'nu': 0.01}</td>\n",
       "      <td>0.015864</td>\n",
       "      <td>0.010365</td>\n",
       "      <td>0.044501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'gamma': 0.01, 'nu': 0.1}</td>\n",
       "      <td>0.103243</td>\n",
       "      <td>0.018722</td>\n",
       "      <td>0.071965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'gamma': 'scale', 'nu': 0.1}</td>\n",
       "      <td>0.106106</td>\n",
       "      <td>0.020934</td>\n",
       "      <td>0.077039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           params  rate_mean  rate_std       obj\n",
       "4     {'gamma': 0.01, 'nu': 0.05}   0.051925  0.007809  0.009734\n",
       "1  {'gamma': 'scale', 'nu': 0.05}   0.056986  0.015195  0.022181\n",
       "3     {'gamma': 0.01, 'nu': 0.01}   0.010304  0.001774  0.041470\n",
       "0  {'gamma': 'scale', 'nu': 0.01}   0.015864  0.010365  0.044501\n",
       "5      {'gamma': 0.01, 'nu': 0.1}   0.103243  0.018722  0.071965\n",
       "2   {'gamma': 'scale', 'nu': 0.1}   0.106106  0.020934  0.077039"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'gamma': 0.01, 'nu': 0.05} | splits: 5 | search_subset: (399960, 19)\n"
     ]
    }
   ],
   "source": [
    "# HP search paralela (nu, gamma) minimizando |outlier_rate - 5%|\n",
    "import numpy as np, pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import GroupKFold, KFold, ParameterGrid\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "param_grid = list(ParameterGrid({\"nu\": CFG[\"svm_nu_grid\"], \"gamma\": CFG[\"svm_gamma_grid\"]}))\n",
    "target_outlier_rate = 0.05\n",
    "\n",
    "def build_search_subset(X, groups, n_max):\n",
    "    if (n_max is None) or (X.shape[0] <= n_max): return X, groups\n",
    "    rng = np.random.default_rng(123)\n",
    "    if groups is None:\n",
    "        idx = rng.choice(X.shape[0], n_max, replace=False); return X[idx], None\n",
    "    uniq = np.unique(groups); per_g = max(1, n_max // len(uniq)); take = []\n",
    "    for g in uniq:\n",
    "        g_idx = np.where(groups == g)[0]\n",
    "        take.extend(rng.choice(g_idx, min(per_g, g_idx.size), replace=False).tolist())\n",
    "    take = np.array(take)\n",
    "    if take.size > n_max: take = rng.choice(take, n_max, replace=False)\n",
    "    return X[take], groups[take]\n",
    "\n",
    "X_search, groups_search = build_search_subset(X_train_sc, groups_train, CFG[\"max_search_samples\"])\n",
    "\n",
    "if (groups_search is not None) and (len(np.unique(groups_search)) >= 2):\n",
    "    n_splits = min(CFG[\"kfold_splits\"], len(np.unique(groups_search)))\n",
    "    splitter = GroupKFold(n_splits=n_splits); split_args = dict(X=X_search, y=None, groups=groups_search)\n",
    "else:\n",
    "    n_splits = max(2, CFG[\"kfold_splits\"])\n",
    "    splitter = KFold(n_splits=n_splits, shuffle=True, random_state=42); split_args = dict(X=X_search, y=None)\n",
    "\n",
    "def outlier_rate(pred): return float((pred == -1).mean())\n",
    "\n",
    "def eval_param(p):\n",
    "    rates = []\n",
    "    for tr_idx, va_idx in splitter.split(**split_args):\n",
    "        Xtr, Xva = X_search[tr_idx], X_search[va_idx]\n",
    "        m = OneClassSVM(kernel=CFG[\"kernel\"], nu=p[\"nu\"], gamma=p[\"gamma\"],\n",
    "                        cache_size=2048, tol=1e-3, shrinking=True)\n",
    "        m.fit(Xtr)\n",
    "        rates.append(outlier_rate(m.predict(Xva)))\n",
    "    rate_mean, rate_std = float(np.mean(rates)), float(np.std(rates))\n",
    "    obj = abs(rate_mean - target_outlier_rate) + rate_std\n",
    "    return {\"params\": p, \"rate_mean\": rate_mean, \"rate_std\": rate_std, \"obj\": obj}\n",
    "\n",
    "rows = Parallel(n_jobs=N_JOBS, prefer=\"processes\", verbose=5)(\n",
    "    delayed(eval_param)(p) for p in param_grid\n",
    ")\n",
    "\n",
    "res_df = pd.DataFrame(rows).sort_values(\"obj\")\n",
    "best_cfg = res_df.iloc[0][\"params\"]\n",
    "display(res_df.head(10))\n",
    "print(\"Best params:\", best_cfg, \"| splits:\", n_splits, \"| search_subset:\", X_search.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model trained. Params: {'gamma': 0.01, 'nu': 0.05}\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento final OC-SVM (cache grande)\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "if \"best_cfg\" not in globals() or not best_cfg:  # fallback\n",
    "    best_cfg = {\"nu\": 0.05, \"gamma\": \"scale\"}\n",
    "\n",
    "final_model = OneClassSVM(\n",
    "    kernel=CFG.get(\"kernel\",\"rbf\"),\n",
    "    nu=best_cfg[\"nu\"],\n",
    "    gamma=best_cfg[\"gamma\"],\n",
    "    cache_size=8192,   # 8 GB cache para libSVM\n",
    "    tol=1e-3,\n",
    "    shrinking=True\n",
    ")\n",
    "final_model.fit(X_train_sc)\n",
    "print(\"Final model trained. Params:\", best_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESUME] next_start = 27,789,660/27,789,660\n",
      "ROC-AUC: nan | PR-AUC: nan | AP: nan\n",
      "Scores memmap: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_eval_scores_mm.dat\n"
     ]
    }
   ],
   "source": [
    "# Evaluación por lotes con REANUDACIÓN + memmap (rápida)\n",
    "import os, json, time, pickle, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"]); OUT.mkdir(parents=True, exist_ok=True)\n",
    "n_eval = X_eval.shape[0]\n",
    "scores_path = OUT / f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"\n",
    "progress_path = OUT / f\"{CFG['artifact_prefix']}_progress.json\"\n",
    "expected_bytes = n_eval * 4\n",
    "\n",
    "# memmap correcto o recrea\n",
    "if scores_path.exists() and scores_path.stat().st_size != expected_bytes:\n",
    "    print(f\"[WARN] Memmap {scores_path.stat().st_size} != {expected_bytes} -> recreando.\")\n",
    "    scores_path.unlink()\n",
    "\n",
    "mode = \"r+\" if scores_path.exists() else \"w+\"\n",
    "scores_mm = np.memmap(scores_path, dtype=np.float32, mode=mode, shape=(n_eval,))\n",
    "if mode == \"w+\":\n",
    "    scores_mm[:] = np.nan\n",
    "    scores_mm.flush()\n",
    "\n",
    "# cargar preprocesamiento por si el kernel cambió\n",
    "medians = np.load(OUT / f\"{CFG['artifact_prefix']}_imputer_medians.npy\")\n",
    "sp = np.load(OUT / f\"{CFG['artifact_prefix']}_scaler_params.npz\")\n",
    "train_mean, train_std = sp[\"mean\"], sp[\"std\"]\n",
    "train_std = train_std.copy(); train_std[train_std==0]=1.0\n",
    "\n",
    "def impute_inplace(X, med):\n",
    "    bad = ~np.isfinite(X)\n",
    "    if bad.any(): X[bad] = med[np.where(bad)[1]]\n",
    "def standardize_inplace(X, m, s):\n",
    "    X -= m; X /= s\n",
    "\n",
    "# reanudación\n",
    "start = 0\n",
    "if progress_path.exists():\n",
    "    try: start = int(json.loads(progress_path.read_text()).get(\"next_start\", 0))\n",
    "    except: start = 0\n",
    "if start <= 0:\n",
    "    nan_mask = np.isnan(scores_mm)\n",
    "    start = int(np.argmax(nan_mask)) if nan_mask.any() else n_eval\n",
    "print(f\"[RESUME] next_start = {start:,}/{n_eval:,}\")\n",
    "\n",
    "bs = int(CFG.get(\"eval_batch_size\", 2_000_000))\n",
    "t0 = time.time(); last = t0\n",
    "\n",
    "try:\n",
    "    for s in range(start, n_eval, bs):\n",
    "        e = min(s + bs, n_eval)\n",
    "        Xe = X_eval[s:e].astype(np.float32, copy=False)\n",
    "        Xe[~np.isfinite(Xe)] = np.nan\n",
    "        impute_inplace(Xe, medians)\n",
    "        standardize_inplace(Xe, train_mean, train_std)\n",
    "\n",
    "        scores_mm[s:e] = -final_model.decision_function(Xe)\n",
    "        scores_mm.flush()\n",
    "        progress_path.write_text(json.dumps({\"next_start\": e}))\n",
    "\n",
    "        now = time.time()\n",
    "        if now - last > 10:\n",
    "            rate = (e - start) / max(1e-6, (now - t0))\n",
    "            print(f\"Progress: {100*e/n_eval:5.2f}% | {e:,}/{n_eval:,} | ~{rate:,.0f} rows/s\")\n",
    "            last = now\n",
    "except KeyboardInterrupt:\n",
    "    scores_mm.flush(); progress_path.write_text(json.dumps({\"next_start\": e})); print(\"\\n[INTERRUPTED] Guardado.\")\n",
    "    raise\n",
    "\n",
    "scores = scores_mm\n",
    "yb = y_eval.astype(int)\n",
    "if len(np.unique(yb)) > 1:\n",
    "    roc = roc_auc_score(yb, scores)\n",
    "    prec, rec, _ = precision_recall_curve(yb, scores); pr_auc = auc(rec, prec)\n",
    "    ap = average_precision_score(yb, scores)\n",
    "else:\n",
    "    roc = pr_auc = ap = np.nan\n",
    "\n",
    "print(f\"ROC-AUC: {roc:.4f} | PR-AUC: {pr_auc:.4f} | AP: {ap:.4f}\")\n",
    "print(\"Scores memmap:\", str(scores_path))\n",
    "\n",
    "# Guardar modelo + config + métricas\n",
    "with open(OUT / f\"{CFG['artifact_prefix']}_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_model, f)\n",
    "with open(OUT / f\"{CFG['artifact_prefix']}_config.json\", \"w\") as f:\n",
    "    json.dump(CFG | {\"best_params\": best_cfg,\n",
    "                     \"metrics\": {\"roc_auc\": float(roc), \"pr_auc\": float(pr_auc), \"ap\": float(ap)}},\n",
    "              f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TOP-K: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_topk_1pct.parquet | rows: 277896\n",
      "@k=1.0% -> P:0.000 | R:0.000 | F1:0.000  (k=277896)\n",
      "Saved MMSI agg: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_mmsi_agg.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmsi</th>\n",
       "      <th>n_win</th>\n",
       "      <th>anom_win</th>\n",
       "      <th>anom_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>33266086194351</td>\n",
       "      <td>428760</td>\n",
       "      <td>34116</td>\n",
       "      <td>0.079569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>49534994750419</td>\n",
       "      <td>374280</td>\n",
       "      <td>18958</td>\n",
       "      <td>0.050652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>77832927010710</td>\n",
       "      <td>423960</td>\n",
       "      <td>20276</td>\n",
       "      <td>0.047825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>87919276942456</td>\n",
       "      <td>567520</td>\n",
       "      <td>25836</td>\n",
       "      <td>0.045524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>100771710683634</td>\n",
       "      <td>32440</td>\n",
       "      <td>1381</td>\n",
       "      <td>0.042571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12639560807591</td>\n",
       "      <td>23520</td>\n",
       "      <td>924</td>\n",
       "      <td>0.039286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>103576446797335</td>\n",
       "      <td>148920</td>\n",
       "      <td>5618</td>\n",
       "      <td>0.037725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>77182424306278</td>\n",
       "      <td>169660</td>\n",
       "      <td>5875</td>\n",
       "      <td>0.034628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23770783250938</td>\n",
       "      <td>473600</td>\n",
       "      <td>14403</td>\n",
       "      <td>0.030412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>95062718521348</td>\n",
       "      <td>169480</td>\n",
       "      <td>4468</td>\n",
       "      <td>0.026363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mmsi   n_win  anom_win  anom_rate\n",
       "12   33266086194351  428760     34116   0.079569\n",
       "19   49534994750419  374280     18958   0.050652\n",
       "28   77832927010710  423960     20276   0.047825\n",
       "33   87919276942456  567520     25836   0.045524\n",
       "40  100771710683634   32440      1381   0.042571\n",
       "1    12639560807591   23520       924   0.039286\n",
       "42  103576446797335  148920      5618   0.037725\n",
       "27   77182424306278  169660      5875   0.034628\n",
       "7    23770783250938  473600     14403   0.030412\n",
       "37   95062718521348  169480      4468   0.026363"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Top-K y agregados por MMSI\n",
    "import numpy as np, pandas as pd, os\n",
    "\n",
    "scores = np.memmap(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"),\n",
    "                   dtype=np.float32, mode=\"r\", shape=(X_eval.shape[0],))\n",
    "\n",
    "k_rate = 0.01\n",
    "k = max(1, int(len(scores) * k_rate))\n",
    "thr_k = np.partition(scores, -k)[-k]\n",
    "pred_topk = (scores >= thr_k).astype(np.int8)\n",
    "\n",
    "topk_idx = np.where(pred_topk == 1)[0]\n",
    "topk_df = pd.DataFrame({\n",
    "    \"idx\": topk_idx.astype(np.int64),\n",
    "    \"anomaly_score\": scores[topk_idx].astype(np.float32),\n",
    "    \"y_eval\": y_eval[topk_idx].astype(np.int8)\n",
    "})\n",
    "if 'groups_eval' in globals() and groups_eval is not None:\n",
    "    topk_df[\"mmsi\"] = groups_eval[topk_idx].astype(np.int64)\n",
    "\n",
    "topk_path = os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_topk_{int(k_rate*100)}pct.parquet\")\n",
    "topk_df.to_parquet(topk_path, index=False)\n",
    "print(\"Saved TOP-K:\", topk_path, \"| rows:\", len(topk_df))\n",
    "\n",
    "# Métricas @k\n",
    "yb = y_eval.astype(int)\n",
    "tp = int(((pred_topk==1) & (yb==1)).sum())\n",
    "fp = int(((pred_topk==1) & (yb==0)).sum())\n",
    "fn = int(((pred_topk==0) & (yb==1)).sum())\n",
    "prec_k = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "rec_k  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "f1_k   = 2*prec_k*rec_k/(prec_k+rec_k) if (prec_k+rec_k)>0 else 0.0\n",
    "print(f\"@k={k_rate*100:.1f}% -> P:{prec_k:.3f} | R:{rec_k:.3f} | F1:{f1_k:.3f}  (k={k})\")\n",
    "\n",
    "# Agregado por MMSI\n",
    "if 'groups_eval' in globals() and groups_eval is not None:\n",
    "    mmsi_all, n_by_mmsi = np.unique(groups_eval, return_counts=True)\n",
    "    mmsi_top, n_top_by_mmsi = np.unique(groups_eval[pred_topk==1], return_counts=True)\n",
    "    top_map = dict(zip(mmsi_top.tolist(), n_top_by_mmsi.tolist()))\n",
    "    anom_win = np.array([top_map.get(m, 0) for m in mmsi_all], dtype=np.int32)\n",
    "    anom_rate = anom_win / n_by_mmsi\n",
    "    agg_df = pd.DataFrame({\"mmsi\": mmsi_all, \"n_win\": n_by_mmsi, \"anom_win\": anom_win, \"anom_rate\": anom_rate})\n",
    "    agg_path = os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_mmsi_agg.parquet\")\n",
    "    agg_df.to_parquet(agg_path, index=False)\n",
    "    print(\"Saved MMSI agg:\", agg_path)\n",
    "    display(agg_df.sort_values(\"anom_rate\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 === Métricas finales OC-SVM ===\n",
      "ROC-AUC : nan\n",
      "PR-AUC  : nan\n",
      "AP Score: nan\n",
      "\n",
      "🔧 Mejor configuración encontrada:\n",
      "{\n",
      "  \"gamma\": 0.01,\n",
      "  \"nu\": 0.05\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_632cf\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_632cf_level0_col0\" class=\"col_heading level0 col0\" >Modelo</th>\n",
       "      <th id=\"T_632cf_level0_col1\" class=\"col_heading level0 col1\" >Kernel</th>\n",
       "      <th id=\"T_632cf_level0_col2\" class=\"col_heading level0 col2\" >nu</th>\n",
       "      <th id=\"T_632cf_level0_col3\" class=\"col_heading level0 col3\" >gamma</th>\n",
       "      <th id=\"T_632cf_level0_col4\" class=\"col_heading level0 col4\" >ROC-AUC</th>\n",
       "      <th id=\"T_632cf_level0_col5\" class=\"col_heading level0 col5\" >PR-AUC</th>\n",
       "      <th id=\"T_632cf_level0_col6\" class=\"col_heading level0 col6\" >AP</th>\n",
       "      <th id=\"T_632cf_level0_col7\" class=\"col_heading level0 col7\" >N ventanas (train)</th>\n",
       "      <th id=\"T_632cf_level0_col8\" class=\"col_heading level0 col8\" >N ventanas (eval)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_632cf_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_632cf_row0_col0\" class=\"data row0 col0\" >One-Class SVM (RBF)</td>\n",
       "      <td id=\"T_632cf_row0_col1\" class=\"data row0 col1\" >rbf</td>\n",
       "      <td id=\"T_632cf_row0_col2\" class=\"data row0 col2\" >0.050000</td>\n",
       "      <td id=\"T_632cf_row0_col3\" class=\"data row0 col3\" >0.010000</td>\n",
       "      <td id=\"T_632cf_row0_col4\" class=\"data row0 col4\" >nan</td>\n",
       "      <td id=\"T_632cf_row0_col5\" class=\"data row0 col5\" >nan</td>\n",
       "      <td id=\"T_632cf_row0_col6\" class=\"data row0 col6\" >nan</td>\n",
       "      <td id=\"T_632cf_row0_col7\" class=\"data row0 col7\" >800000</td>\n",
       "      <td id=\"T_632cf_row0_col8\" class=\"data row0 col8\" >400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f1f3ef876e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Métricas finales resumen (para informe) ---\n",
    "import json, numpy as np, os, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"])\n",
    "cfg_path = OUT / f\"{CFG['artifact_prefix']}_config.json\"\n",
    "scores_path = OUT / f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"\n",
    "\n",
    "if not cfg_path.exists():\n",
    "    raise FileNotFoundError(\"No se encontró el archivo de configuración con métricas guardadas.\")\n",
    "\n",
    "# Cargar configuración y métricas\n",
    "with open(cfg_path, \"r\") as f:\n",
    "    cfg_data = json.load(f)\n",
    "\n",
    "metrics = cfg_data.get(\"metrics\", {})\n",
    "roc_auc = metrics.get(\"roc_auc\", np.nan)\n",
    "pr_auc  = metrics.get(\"pr_auc\", np.nan)\n",
    "ap      = metrics.get(\"ap\", np.nan)\n",
    "best_params = cfg_data.get(\"best_params\", {})\n",
    "\n",
    "print(\"📊 === Métricas finales OC-SVM ===\")\n",
    "print(f\"ROC-AUC : {roc_auc:.4f}\")\n",
    "print(f\"PR-AUC  : {pr_auc:.4f}\")\n",
    "print(f\"AP Score: {ap:.4f}\")\n",
    "print()\n",
    "print(\"🔧 Mejor configuración encontrada:\")\n",
    "print(json.dumps(best_params, indent=2))\n",
    "\n",
    "# Resumen para tabla del informe\n",
    "summary_df = pd.DataFrame([{\n",
    "    \"Modelo\": \"One-Class SVM (RBF)\",\n",
    "    \"Kernel\": cfg_data.get(\"kernel\", \"rbf\"),\n",
    "    \"nu\": best_params.get(\"nu\"),\n",
    "    \"gamma\": best_params.get(\"gamma\"),\n",
    "    \"ROC-AUC\": roc_auc,\n",
    "    \"PR-AUC\": pr_auc,\n",
    "    \"AP\": ap,\n",
    "    \"N ventanas (train)\": int(cfg_data.get(\"max_train_samples\", 0)),\n",
    "    \"N ventanas (eval)\": int(cfg_data.get(\"max_search_samples\", 0))\n",
    "}])\n",
    "display(summary_df.style.format({\"ROC-AUC\": \"{:.4f}\", \"PR-AUC\": \"{:.4f}\", \"AP\": \"{:.4f}\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (y \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m), info\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 1) Mapear etiquetas a binario (1=anómalo)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m yb, map_info \u001b[38;5;241m=\u001b[39m map_labels_to_binary(\u001b[43my_eval\u001b[49m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistribución original:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39munique(y_eval, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistribución mapeada :\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39munique(yb, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_eval' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Diagnóstico de etiquetas y recálculo de métricas @k con mapeo robusto ---\n",
    "\n",
    "import numpy as np, pandas as pd, os, json\n",
    "from pathlib import Path\n",
    "\n",
    "def map_labels_to_binary(y):\n",
    "    \"\"\"Devuelve (yb, info) con yb en {0,1} donde 1=anómalo.\"\"\"\n",
    "    vals = np.unique(y)\n",
    "    s = set(vals.tolist())\n",
    "    info = {\"original_values\": vals.tolist(), \"mapping\": None}\n",
    "    # Caso estándar\n",
    "    if s == {0, 1}:\n",
    "        info[\"mapping\"] = \"0=normal, 1=anómalo (sin cambio)\"\n",
    "        return y.astype(int), info\n",
    "    # Muy común en detección: -1 anómalo, +1 normal\n",
    "    if s == {-1, 1}:\n",
    "        info[\"mapping\"] = \"-1=anómalo, +1=normal -> mapeado a {0,1}\"\n",
    "        return (y == -1).astype(int), info\n",
    "    # A veces hay {0,1,-1}; asumimos 1=anómalo, 0=normal, -1=desconocido -> lo tratamos como 0 (conservador)\n",
    "    if s == {0, 1, -1}:\n",
    "        info[\"mapping\"] = \"1=anómalo, 0=normal, -1=desconocido -> mapeado con -1->0\"\n",
    "        y2 = y.copy()\n",
    "        y2[y2 == -1] = 0\n",
    "        return y2.astype(int), info\n",
    "    # Fallback conservador: cualquier valor >0 lo consideramos anómalo\n",
    "    info[\"mapping\"] = f\"Fallback: valores {sorted(s)} -> (y>0) como anómalo\"\n",
    "    return (y > 0).astype(int), info\n",
    "\n",
    "# 1) Mapear etiquetas a binario (1=anómalo)\n",
    "yb, map_info = map_labels_to_binary(y_eval)\n",
    "print(\"Distribución original:\", np.unique(y_eval, return_counts=True))\n",
    "print(\"Distribución mapeada :\", np.unique(yb, return_counts=True))\n",
    "print(\"Mapping usado:\", map_info[\"mapping\"])\n",
    "\n",
    "# 2) Recalcular métricas @k con el mapeo correcto\n",
    "scores = np.memmap(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"),\n",
    "                   dtype=np.float32, mode=\"r\", shape=(X_eval.shape[0],))\n",
    "\n",
    "k_rate = 0.01\n",
    "k = max(1, int(len(scores) * k_rate))\n",
    "thr_k = np.partition(scores, -k)[-k]\n",
    "pred_topk = (scores >= thr_k).astype(np.int8)\n",
    "\n",
    "tp = int(((pred_topk==1) & (yb==1)).sum())\n",
    "fp = int(((pred_topk==1) & (yb==0)).sum())\n",
    "fn = int(((pred_topk==0) & (yb==1)).sum())\n",
    "prec_k = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "rec_k  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "f1_k   = 2*prec_k*rec_k/(prec_k+rec_k) if (prec_k+rec_k)>0 else 0.0\n",
    "\n",
    "print(f\"@k={k_rate*100:.1f}% -> P:{prec_k:.4f} | R:{rec_k:.4f} | F1:{f1_k:.4f}  (k={k})\")\n",
    "\n",
    "# 3) Guardar métricas @k junto a las globales para el informe\n",
    "cfg_path = Path(CFG[\"out_dir\"]) / f\"{CFG['artifact_prefix']}_config.json\"\n",
    "if cfg_path.exists():\n",
    "    cfg = json.loads(cfg_path.read_text())\n",
    "else:\n",
    "    cfg = {\"metrics\": {}}\n",
    "cfg.setdefault(\"metrics_at_k\", {})[str(k_rate)] = {\n",
    "    \"k\": int(k),\n",
    "    \"precision\": float(prec_k),\n",
    "    \"recall\": float(rec_k),\n",
    "    \"f1\": float(f1_k),\n",
    "    \"label_mapping\": map_info[\"mapping\"],\n",
    "    \"label_values_original\": map_info[\"original_values\"],\n",
    "}\n",
    "cfg_path.write_text(json.dumps(cfg, indent=2))\n",
    "print(\"📁 Métricas @k añadidas a:\", cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores cargados: (27789660,)\n",
      "Distribución etiquetas mapeadas: {0: 27789660}\n",
      "ROC-AUC: nan | PR-AUC: nan | AP: nan\n",
      "@k=1.0% -> P:0.0000 | R:0.0000 | F1:0.0000  (k=277896)\n",
      "Mapping etiquetas: fallback>0 anómalo (vals=[0])\n",
      "✅ Métricas actualizadas en: data/ocsvm_runs/ocsvm_rbf_config.json\n"
     ]
    }
   ],
   "source": [
    "# === RECUPERAR MÉTRICAS DESDE ARTEFACTOS (sin re-entrenar) ===\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "\n",
    "OUT = Path(\"data/ocsvm_runs\")\n",
    "cfg_path = OUT / \"ocsvm_rbf_config.json\"\n",
    "assert cfg_path.exists(), \"No encuentro data/ocsvm_runs/ocsvm_rbf_config.json\"\n",
    "\n",
    "# 1) Cargar CFG guardado y paths básicos\n",
    "CFG = json.loads(cfg_path.read_text())\n",
    "DATA_DIR = Path(CFG.get(\"external_data_dir\", \"/teamspace/studios/this_studio/data\")).resolve()\n",
    "scores_path = OUT / f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"\n",
    "assert scores_path.exists(), f\"No existe memmap de scores: {scores_path}\"\n",
    "\n",
    "# 2) Cargar scores desde memmap (inferimos N por tamaño)\n",
    "n_eval = scores_path.stat().st_size // 4  # float32\n",
    "scores = np.memmap(scores_path, dtype=np.float32, mode=\"r\", shape=(n_eval,))\n",
    "print(f\"Scores cargados: {scores.shape}\")\n",
    "\n",
    "# 3) Encontrar archivo(s) de etiquetas y leer SOLO la columna de label\n",
    "def first_valid_label_col(pqfile: pq.ParquetFile, prefer=(\"is_suspicious\",\"label\",\"y\",\"target\")):\n",
    "    cols = [name for name in pqfile.schema.names]\n",
    "    for c in prefer:\n",
    "        if c in cols: return c\n",
    "    # fallback: primera columna entera corta\n",
    "    for c in cols:\n",
    "        t = pqfile.schema.field(c).type\n",
    "        if str(t).startswith((\"int8\",\"int16\",\"int32\")): return c\n",
    "    # si no, última columna\n",
    "    return cols[-1]\n",
    "\n",
    "def read_label_series(path: Path) -> pd.Series:\n",
    "    pf = pq.ParquetFile(path)\n",
    "    col = first_valid_label_col(pf)\n",
    "    tbl = pf.read(columns=[col])\n",
    "    s = tbl.to_pandas()[col]\n",
    "    return s\n",
    "\n",
    "# candidatos de eval\n",
    "single = DATA_DIR / \"windows_with_labels_aligned.parquet\"\n",
    "wl_norm = DATA_DIR / \"windows_with_labels_aligned_normal.parquet\"\n",
    "wl_anom = DATA_DIR / \"windows_with_labels_aligned_anom.parquet\"\n",
    "eval_norm = DATA_DIR / \"eval_windows_aligned_normal.parquet\"\n",
    "eval_anom = DATA_DIR / \"eval_windows_aligned_anom.parquet\"\n",
    "labels_a = DATA_DIR / \"eval_labels_aligned.parquet\"\n",
    "labels_b = DATA_DIR / \"labels.parquet\"\n",
    "\n",
    "y = None\n",
    "if single.exists():\n",
    "    y = read_label_series(single)\n",
    "elif wl_norm.exists() and wl_anom.exists():\n",
    "    y = pd.concat([read_label_series(wl_norm), read_label_series(wl_anom)], ignore_index=True)\n",
    "elif eval_norm.exists() and eval_anom.exists():\n",
    "    # etiquetas por archivo aparte\n",
    "    lab_path = labels_a if labels_a.exists() else (labels_b if labels_b.exists() else None)\n",
    "    assert lab_path is not None, \"No encontré archivo de labels para eval split.\"\n",
    "    y = read_label_series(lab_path)\n",
    "else:\n",
    "    # buscar genérico por patrón\n",
    "    pats = [\"*with_labels*aligned*.parquet\", \"*eval*labels*aligned*.parquet\", \"*labels*.parquet\"]\n",
    "    for patt in pats:\n",
    "        cands = sorted(DATA_DIR.glob(patt))\n",
    "        if cands:\n",
    "            y = read_label_series(cands[0]); break\n",
    "\n",
    "assert y is not None, f\"No encontré etiquetas en {DATA_DIR}\"\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "# 4) Alinear longitudes y mapear etiquetas a {0,1} (1 = anómalo)\n",
    "assert len(y) == n_eval, f\"Desalineación: labels={len(y)} vs scores={n_eval}\"\n",
    "\n",
    "def map_labels_to_binary(y_arr):\n",
    "    vals = np.unique(y_arr)\n",
    "    s = set(vals.tolist())\n",
    "    if s == {0,1}:\n",
    "        return y_arr.astype(int), \"0=normal, 1=anómalo\"\n",
    "    if s == {-1,1}:\n",
    "        return (y_arr == -1).astype(int), \"-1=anómalo, +1=normal\"\n",
    "    if s == {0,1,-1}:\n",
    "        y2 = y_arr.copy(); y2[y2==-1]=0\n",
    "        return y2.astype(int), \"1=anómalo, 0=normal, -1→0\"\n",
    "    # fallback: todo >0 es anómalo\n",
    "    return (y_arr>0).astype(int), f\"fallback>0 anómalo (vals={sorted(s)})\"\n",
    "\n",
    "yb, mapping_info = map_labels_to_binary(y.to_numpy())\n",
    "print(\"Distribución etiquetas mapeadas:\", dict(zip(*np.unique(yb, return_counts=True))))\n",
    "\n",
    "# 5) Métricas globales\n",
    "if len(np.unique(yb)) > 1:\n",
    "    roc = roc_auc_score(yb, scores)\n",
    "    prec, rec, _ = precision_recall_curve(yb, scores); pr = auc(rec, prec)\n",
    "    ap = average_precision_score(yb, scores)\n",
    "else:\n",
    "    roc = pr = ap = float(\"nan\")\n",
    "\n",
    "# 6) Métricas @k (1%)\n",
    "k_rate = 0.01\n",
    "k = max(1, int(n_eval * k_rate))\n",
    "thr = np.partition(scores, -k)[-k]\n",
    "pred_topk = (scores >= thr).astype(np.int8)\n",
    "\n",
    "tp = int(((pred_topk==1) & (yb==1)).sum())\n",
    "fp = int(((pred_topk==1) & (yb==0)).sum())\n",
    "fn = int(((pred_topk==0) & (yb==1)).sum())\n",
    "prec_k = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "rec_k  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "f1_k   = 2*prec_k*rec_k/(prec_k+rec_k) if (prec_k+rec_k)>0 else 0.0\n",
    "\n",
    "print(f\"ROC-AUC: {roc:.4f} | PR-AUC: {pr:.4f} | AP: {ap:.4f}\")\n",
    "print(f\"@k={k_rate*100:.1f}% -> P:{prec_k:.4f} | R:{rec_k:.4f} | F1:{f1_k:.4f}  (k={k})\")\n",
    "print(\"Mapping etiquetas:\", mapping_info)\n",
    "\n",
    "# 7) Persistir métricas actualizadas al JSON (idempotente)\n",
    "cfg = CFG.copy()\n",
    "cfg[\"metrics\"] = {\"roc_auc\": float(roc), \"pr_auc\": float(pr), \"ap\": float(ap)}\n",
    "cfg.setdefault(\"metrics_at_k\", {})[str(k_rate)] = {\n",
    "    \"k\": int(k), \"precision\": float(prec_k), \"recall\": float(rec_k), \"f1\": float(f1_k),\n",
    "    \"label_mapping\": mapping_info\n",
    "}\n",
    "cfg_path.write_text(json.dumps(cfg, indent=2))\n",
    "print(\"✅ Métricas actualizadas en:\", cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>candidates</th>\n",
       "      <th>n_rows_scanned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>windows_with_labels_aligned.parquet</td>\n",
       "      <td>[]</td>\n",
       "      <td>500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>windows_with_labels.parquet</td>\n",
       "      <td>[]</td>\n",
       "      <td>500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eval_windows_aligned.parquet</td>\n",
       "      <td>[]</td>\n",
       "      <td>500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eval_labels_aligned.parquet</td>\n",
       "      <td>[]</td>\n",
       "      <td>500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>labels.parquet</td>\n",
       "      <td>[]</td>\n",
       "      <td>500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>labels_anom.parquet</td>\n",
       "      <td>[]</td>\n",
       "      <td>131420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  file candidates  n_rows_scanned\n",
       "0  windows_with_labels_aligned.parquet         []          500000\n",
       "1          windows_with_labels.parquet         []          500000\n",
       "2         eval_windows_aligned.parquet         []          500000\n",
       "3          eval_labels_aligned.parquet         []          500000\n",
       "4                       labels.parquet         []          500000\n",
       "5                  labels_anom.parquet         []          131420"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👉 Elige (archivo, columna) con 0/1 o -1/1 (aunque sea float). Si nada aparece, usamos labels_anom como índices.\n"
     ]
    }
   ],
   "source": [
    "# Auditoría de etiquetas en /data (incluye floats ~binarios)\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(CFG[\"external_data_dir\"])\n",
    "cands = [\n",
    "    \"windows_with_labels_aligned.parquet\",\n",
    "    \"windows_with_labels.parquet\",\n",
    "    \"eval_windows_aligned.parquet\",\n",
    "    \"eval_windows_aligned_normal.parquet\",\n",
    "    \"eval_windows_aligned_anom.parquet\",\n",
    "    \"eval_labels_aligned.parquet\",\n",
    "    \"labels.parquet\",\n",
    "    \"labels_anom.parquet\",\n",
    "]\n",
    "\n",
    "def try_counts(p: Path, max_rows=500_000, bin_tol=1e-6):\n",
    "    try:\n",
    "        pf = pq.ParquetFile(p)\n",
    "    except Exception as e:\n",
    "        return {\"file\": p.name, \"error\": f\"no parquet ({type(e).__name__})\"}\n",
    "    schema = pf.schema_arrow\n",
    "    cols = schema.names\n",
    "\n",
    "    labelish = []\n",
    "    for field in schema:\n",
    "        t = str(field.type).lower()\n",
    "        if any(x in t for x in [\"int8\",\"int16\",\"int32\",\"int64\",\"bool\",\"float16\",\"float32\",\"float64\"]):\n",
    "            labelish.append(field.name)\n",
    "\n",
    "    out = []\n",
    "    for c in labelish:\n",
    "        try:\n",
    "            tbl = pf.read(columns=[c], max_rows=max_rows)\n",
    "            s = pd.to_numeric(tbl.to_pandas()[c], errors=\"coerce\")\n",
    "            s = s.dropna()\n",
    "            if s.empty: \n",
    "                continue\n",
    "            # ¿binario exacto?\n",
    "            uniq = np.unique(s.values)\n",
    "            uniq_set = set(np.round(uniq, 6))\n",
    "            is_binary_exact = uniq_set <= {0,1} or uniq_set <= {-1,1} or uniq_set <= {0,1,-1}\n",
    "            # ¿casi binario? (tolerancia)\n",
    "            is_binary_close = np.all((np.abs(s - 0) < bin_tol) | (np.abs(s - 1) < bin_tol) | (np.abs(s + 1) < bin_tol))\n",
    "            if is_binary_exact or is_binary_close:\n",
    "                vc = s.value_counts().head(5).to_dict()\n",
    "                out.append((c, vc, str(schema.field(c).type)))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return {\n",
    "        \"file\": p.name,\n",
    "        \"candidates\": out,\n",
    "        \"n_rows_scanned\": int(min(max_rows, pf.metadata.num_rows))\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "for name in cands:\n",
    "    paths = []\n",
    "    if (DATA_DIR / name).exists():\n",
    "        paths.append(DATA_DIR / name)\n",
    "    else:\n",
    "        paths += sorted(DATA_DIR.glob(f\"*{name.replace('.parquet','')}*.parquet\"))\n",
    "    for p in paths:\n",
    "        rows.append(try_counts(p))\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "pd.set_option(\"display.max_colwidth\", 180)\n",
    "display(df.fillna(\"\"))\n",
    "print(\"👉 Elige (archivo, columna) con 0/1 o -1/1 (aunque sea float). Si nada aparece, usamos labels_anom como índices.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruido y desde labels_anom.parquet usando columna 'window_id' | positivos: 9120 / 27789660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC:0.1978 | PR-AUC:0.0004 | AP:0.0004\n",
      "@1% -> P:0.0032 | R:0.0967 | F1:0.0061 (k=277896)\n",
      "✅ Métricas guardadas en: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_config.json\n"
     ]
    }
   ],
   "source": [
    "# Recalcular métricas desde scores con (archivo,columna) o con labels_anom como índices\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"])\n",
    "DATA_DIR = Path(CFG[\"external_data_dir\"])\n",
    "\n",
    "scores_path = OUT / f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"\n",
    "assert scores_path.exists(), \"No existe memmap de scores.\"\n",
    "n_eval = scores_path.stat().st_size // 4\n",
    "scores = np.memmap(scores_path, dtype=np.float32, mode=\"r\", shape=(n_eval,))\n",
    "\n",
    "# === 1) EDITA si ya identificaste archivo/columna de etiqueta binaria ===\n",
    "LABEL_FILE = None          # ej \"eval_labels_aligned.parquet\" o \"windows_with_labels_aligned.parquet\"\n",
    "LABEL_COL  = None          # ej \"is_suspicious\" (None = autodetectar)\n",
    "\n",
    "def read_binary_labels_from_file(path: Path, col: str|None):\n",
    "    pf = pq.ParquetFile(path)\n",
    "    if col is None:\n",
    "        # auto: preferidas luego cualquiera ~binaria\n",
    "        prefer = [\"is_suspicious\",\"label\",\"y\",\"target\"]\n",
    "        cols = pf.schema_arrow.names\n",
    "        for c in prefer:\n",
    "            if c in cols:\n",
    "                col = c; break\n",
    "        if col is None:\n",
    "            for f in pf.schema_arrow:\n",
    "                if str(f.type).lower() in (\"int8\",\"int16\",\"int32\",\"int64\",\"bool\",\"float16\",\"float32\",\"float64\"):\n",
    "                    col = f.name; break\n",
    "    tbl = pf.read(columns=[col])\n",
    "    y = pd.to_numeric(tbl.to_pandas()[col], errors=\"coerce\").fillna(0).to_numpy()\n",
    "    # mapear a {0,1}\n",
    "    vals = set(np.round(np.unique(y), 6).tolist())\n",
    "    if vals <= {0,1}:\n",
    "        yb = (np.abs(y - 1.0) < 1e-6).astype(int)\n",
    "        mapping = \"float/int 0/1 -> 1=anomalo\"\n",
    "    elif vals <= {-1,1} or vals <= {0,1,-1}:\n",
    "        yb = (np.abs(y + 1.0) < 1e-6).astype(int)  # -1 -> 1\n",
    "        mapping = \"float/int -1/1 -> -1=anomalo\"\n",
    "    else:\n",
    "        # fallback: >0 anómalo\n",
    "        yb = (y > 0).astype(int)\n",
    "        mapping = f\"fallback (>0 anómalo), vals={sorted(vals)}\"\n",
    "    return yb, mapping, col\n",
    "\n",
    "def metrics_from_y(scores, yb, k_rate=0.01):\n",
    "    if len(np.unique(yb)) > 1:\n",
    "        roc = roc_auc_score(yb, scores)\n",
    "        prec, rec, _ = precision_recall_curve(yb, scores); pr = auc(rec, prec)\n",
    "        ap = average_precision_score(yb, scores)\n",
    "    else:\n",
    "        roc = pr = ap = float(\"nan\")\n",
    "    k = max(1, int(len(scores) * k_rate))\n",
    "    thr = np.partition(scores, -k)[-k]\n",
    "    pred_topk = (scores >= thr).astype(np.int8)\n",
    "    tp = int(((pred_topk==1) & (yb==1)).sum())\n",
    "    fp = int(((pred_topk==1) & (yb==0)).sum())\n",
    "    fn = int(((pred_topk==0) & (yb==1)).sum())\n",
    "    prec_k = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "    rec_k  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "    f1_k   = 2*prec_k*rec_k/(prec_k+rec_k) if (prec_k+rec_k)>0 else 0.0\n",
    "    return roc, pr, ap, k, prec_k, rec_k, f1_k\n",
    "\n",
    "def persist_metrics(cfg_extra: dict, metrics, k_rate=0.01):\n",
    "    roc, pr, ap, k, pk, rk, f1k = metrics\n",
    "    cfg_path = OUT / f\"{CFG['artifact_prefix']}_config.json\"\n",
    "    cfg = json.loads(cfg_path.read_text()) if cfg_path.exists() else {}\n",
    "    cfg.update(cfg_extra)\n",
    "    cfg[\"metrics\"] = {\"roc_auc\": float(roc), \"pr_auc\": float(pr), \"ap\": float(ap)}\n",
    "    cfg.setdefault(\"metrics_at_k\", {})[str(k_rate)] = {\n",
    "        \"k\": int(k), \"precision\": float(pk), \"recall\": float(rk), \"f1\": float(f1k)\n",
    "    }\n",
    "    cfg_path.write_text(json.dumps(cfg, indent=2))\n",
    "    print(\"✅ Métricas guardadas en:\", cfg_path)\n",
    "\n",
    "used = False\n",
    "if LABEL_FILE is not None:\n",
    "    lab_path = DATA_DIR / LABEL_FILE\n",
    "    assert lab_path.exists(), f\"No existe {lab_path}\"\n",
    "    yb, mapping, used_col = read_binary_labels_from_file(lab_path, LABEL_COL)\n",
    "    assert len(yb) == n_eval, f\"Desalineación: labels={len(yb)} vs scores={n_eval}\"\n",
    "    print(f\"Usando {LABEL_FILE} :: {used_col}  ({mapping})\")\n",
    "    M = metrics_from_y(scores, yb, k_rate=0.01)\n",
    "    print(f\"ROC-AUC:{M[0]:.4f} | PR-AUC:{M[1]:.4f} | AP:{M[2]:.4f}\")\n",
    "    print(f\"@1% -> P:{M[4]:.4f} | R:{M[5]:.4f} | F1:{M[6]:.4f} (k={M[3]})\")\n",
    "    persist_metrics({\"label_file\": LABEL_FILE, \"label_col\": used_col, \"label_mapping\": mapping}, M)\n",
    "    used = True\n",
    "\n",
    "# === 2) Si no se definió LABEL_FILE o no trae positivos, intentar construir y desde labels_anom como ÍNDICES\n",
    "if not used:\n",
    "    la = None\n",
    "    for name in [\"labels_anom.parquet\", *list(DATA_DIR.glob(\"*labels_anom*.parquet\"))]:\n",
    "        p = name if isinstance(name, Path) else (DATA_DIR / name)\n",
    "        if Path(p).exists():\n",
    "            la = Path(p); break\n",
    "    assert la is not None, \"No encontré labels_anom.parquet para reconstrucción por índices.\"\n",
    "\n",
    "    pf = pq.ParquetFile(la)\n",
    "    cols = pf.schema_arrow.names\n",
    "    # heurística: elegir columna índice\n",
    "    key_candidates = [c for c in [\"idx\",\"window_id\",\"idx_end\",\"row\",\"row_id\"] if c in cols]\n",
    "    if not key_candidates:\n",
    "        # si solo hay una columna numérica, usarla como índices\n",
    "        num_cols = [f.name for f in pf.schema_arrow if \"int\" in str(f.type).lower()]\n",
    "        assert num_cols, f\"No hallé columnas numéricas en {la}\"\n",
    "        key = num_cols[0]\n",
    "    else:\n",
    "        key = key_candidates[0]\n",
    "\n",
    "    s = pf.read(columns=[key]).to_pandas()[key]\n",
    "    idx = pd.to_numeric(s, errors=\"coerce\").dropna().astype(int).to_numpy()\n",
    "\n",
    "    # Ajuste 0/1-based: si el máximo es == n_eval y el mínimo es 1, usamos 1-based\n",
    "    if idx.min() >= 1 and idx.max() <= n_eval and (1 in idx):\n",
    "        idx0 = idx - 1\n",
    "    else:\n",
    "        idx0 = idx\n",
    "    idx0 = idx0[(idx0 >= 0) & (idx0 < n_eval)]\n",
    "\n",
    "    yb = np.zeros(n_eval, dtype=int)\n",
    "    yb[idx0] = 1\n",
    "    print(f\"Reconstruido y desde {la.name} usando columna '{key}' | positivos: {yb.sum()} / {n_eval}\")\n",
    "\n",
    "    M = metrics_from_y(scores, yb, k_rate=0.01)\n",
    "    print(f\"ROC-AUC:{M[0]:.4f} | PR-AUC:{M[1]:.4f} | AP:{M[2]:.4f}\")\n",
    "    print(f\"@1% -> P:{M[4]:.4f} | R:{M[5]:.4f} | F1:{M[6]:.4f} (k={M[3]})\")\n",
    "    persist_metrics({\"label_file\": la.name, \"label_col\": key, \"label_mapping\": \"indices (1-based auto-ajustado)\"}, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: (27789660,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas construidas: positivos=1003200 / 27789660  | fuente=JOIN on window_id from labels_anom.parquet\n",
      "Scores tal cual   -> ROC:0.5069 | PR-AUC:0.0389 | AP:0.0367\n",
      "Scores invertidos -> ROC:0.4931 | PR-AUC:0.0357 | AP:0.0369\n",
      "👉 Usa la versión con mejores métricas.\n"
     ]
    }
   ],
   "source": [
    "# --- Test de polaridad (auto, sin depender de X_eval) ---\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "\n",
    "# 0) Cargar CFG desde JSON si no está en memoria\n",
    "if 'CFG' not in globals():\n",
    "    cfg_path_guess = Path(\"data/ocsvm_runs/ocsvm_rbf_config.json\")\n",
    "    assert cfg_path_guess.exists(), \"No encontré data/ocsvm_runs/ocsvm_rbf_config.json\"\n",
    "    CFG = json.loads(cfg_path_guess.read_text())\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"])\n",
    "DATA_DIR = Path(CFG.get(\"external_data_dir\", \"/teamspace/studios/this_studio/data\"))\n",
    "scores_path = OUT / f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"\n",
    "assert scores_path.exists(), f\"No existe memmap de scores: {scores_path}\"\n",
    "\n",
    "# 1) Cargar scores y n_eval a partir del tamaño de archivo\n",
    "n_eval = scores_path.stat().st_size // 4  # float32\n",
    "scores = np.memmap(scores_path, dtype=np.float32, mode=\"r\", shape=(n_eval,))\n",
    "print(\"Scores:\", scores.shape)\n",
    "\n",
    "# -------- Helpers --------\n",
    "def metrics_from(scores, yb):\n",
    "    if len(np.unique(yb)) > 1:\n",
    "        roc = roc_auc_score(yb, scores)\n",
    "        prec, rec, _ = precision_recall_curve(yb, scores); pr_auc = auc(rec, prec)\n",
    "        ap = average_precision_score(yb, scores)\n",
    "    else:\n",
    "        roc = pr_auc = ap = float('nan')\n",
    "    return roc, pr_auc, ap\n",
    "\n",
    "def build_y_from_label_file(label_file:str, label_col:str|None):\n",
    "    p = DATA_DIR / label_file\n",
    "    assert p.exists(), f\"No existe {p}\"\n",
    "    pf = pq.ParquetFile(p)\n",
    "    col = label_col\n",
    "    if col is None:\n",
    "        # autodetectar columna \"binaria\" (int/bool/float 0/1/-1)\n",
    "        pref = [\"is_suspicious\",\"label\",\"y\",\"target\"]\n",
    "        cols = pf.schema_arrow.names\n",
    "        for c in pref:\n",
    "            if c in cols: col = c; break\n",
    "        if col is None:\n",
    "            # como fallback, primera numérica\n",
    "            for f in pf.schema_arrow:\n",
    "                t = str(f.type).lower()\n",
    "                if any(x in t for x in [\"int\",\"bool\",\"float\"]):\n",
    "                    col = f.name; break\n",
    "    tbl = pf.read(columns=[col])\n",
    "    y = pd.to_numeric(tbl.to_pandas()[col], errors=\"coerce\").fillna(0).to_numpy()\n",
    "    assert len(y) == n_eval, f\"Desalineación labels={len(y)} vs scores={n_eval}\"\n",
    "    vals = set(np.round(np.unique(y),6).tolist())\n",
    "    if vals <= {0,1}:\n",
    "        yb = (np.abs(y - 1.0) < 1e-6).astype(int); mapping = \"0/1 -> 1=anómalo\"\n",
    "    elif vals <= {-1,1} or vals <= {0,1,-1}:\n",
    "        yb = (np.abs(y + 1.0) < 1e-6).astype(int); mapping = \"-1/1 -> -1=anómalo\"\n",
    "    else:\n",
    "        yb = (y > 0).astype(int); mapping = f\"fallback (>0 anómalo), vals={sorted(vals)}\"\n",
    "    return yb, mapping\n",
    "\n",
    "def build_y_from_labels_anom_join():\n",
    "    # localizar parquet de eval para tomar el ORDEN real\n",
    "    eval_candidates = [\n",
    "        \"windows_with_labels_aligned.parquet\",\n",
    "        \"eval_windows_aligned.parquet\",\n",
    "        \"windows_with_labels.parquet\",\n",
    "        \"windows_with_labels_aligned_normal.parquet\",  # si es split, al menos nos da el orden de una mitad\n",
    "    ]\n",
    "    EVAL_PATH = None\n",
    "    for name in eval_candidates:\n",
    "        p = DATA_DIR / name\n",
    "        if p.exists(): EVAL_PATH = p; break\n",
    "    if EVAL_PATH is None:\n",
    "        cands = sorted(DATA_DIR.glob(\"*windows*aligned*.parquet\"),\n",
    "                       key=lambda x: x.stat().st_size if x.exists() else 0, reverse=True)\n",
    "        assert cands, \"No hallé parquet de EVAL para inferir orden\"\n",
    "        EVAL_PATH = cands[0]\n",
    "\n",
    "    pf_eval = pq.ParquetFile(EVAL_PATH)\n",
    "    key_eval = next((k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in pf_eval.schema_arrow.names), None)\n",
    "    assert key_eval is not None, f\"No encontré columna clave en {EVAL_PATH.name}\"\n",
    "    eval_key = pf_eval.read(columns=[key_eval]).to_pandas()[key_eval].astype(np.int64).reset_index(drop=True)\n",
    "    assert len(eval_key) == n_eval, f\"Desalineación eval_key={len(eval_key)} vs scores={n_eval}\"\n",
    "\n",
    "    # cargar labels_anom\n",
    "    lab_anom = None\n",
    "    for p in [DATA_DIR / \"labels_anom.parquet\", *DATA_DIR.glob(\"*labels_anom*.parquet\")]:\n",
    "        if p.exists(): lab_anom = p; break\n",
    "    assert lab_anom is not None, \"No encontré labels_anom.parquet\"\n",
    "    pf_lab = pq.ParquetFile(lab_anom)\n",
    "    key_lab = key_eval if key_eval in pf_lab.schema_arrow.names else (\n",
    "        next((k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in pf_lab.schema_arrow.names), None)\n",
    "    )\n",
    "    assert key_lab is not None, f\"labels_anom no tiene clave compatible (busqué window_id/idx/idx_end/row/row_id)\"\n",
    "    anom_keys = pf_lab.read(columns=[key_lab]).to_pandas()[key_lab].astype(np.int64).to_numpy()\n",
    "    anom_set = set(anom_keys.tolist())\n",
    "    yb = eval_key.isin(anom_set).astype(int).to_numpy()\n",
    "    return yb, f\"JOIN on {key_eval} from {lab_anom.name}\"\n",
    "\n",
    "# 2) Construir yb según lo que tengamos en config\n",
    "cfg_path = OUT / f\"{CFG['artifact_prefix']}_config.json\"\n",
    "cfg_json = json.loads(cfg_path.read_text()) if cfg_path.exists() else {}\n",
    "\n",
    "yb = None; mapping_src = None\n",
    "lab_file = cfg_json.get(\"label_file\")\n",
    "lab_col  = cfg_json.get(\"label_col\")\n",
    "lab_mapping_hint = cfg_json.get(\"label_mapping\", \"\")\n",
    "\n",
    "try:\n",
    "    if lab_file and lab_file != \"labels_anom.parquet\":\n",
    "        # usar archivo+columna si se registró en config\n",
    "        yb, mapping_src = build_y_from_label_file(lab_file, lab_col)\n",
    "    else:\n",
    "        # por defecto (o si era labels_anom), usar JOIN por clave\n",
    "        yb, mapping_src = build_y_from_labels_anom_join()\n",
    "except Exception as e:\n",
    "    # fallback: intentar archivo estándar de labels\n",
    "    for candidate in [\"windows_with_labels_aligned.parquet\", \"eval_labels_aligned.parquet\"]:\n",
    "        p = DATA_DIR / candidate\n",
    "        if p.exists():\n",
    "            yb, mapping_src = build_y_from_label_file(candidate, None)\n",
    "            break\n",
    "    if yb is None:\n",
    "        # último recurso: indices desde labels_anom como posicional (puede desalinear)\n",
    "        la = DATA_DIR / \"labels_anom.parquet\"\n",
    "        assert la.exists(), \"No encontré labels_anom.parquet para fallback.\"\n",
    "        pf = pq.ParquetFile(la)\n",
    "        key = next((k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in pf.schema_arrow.names), pf.schema_arrow.names[0])\n",
    "        idx = pf.read(columns=[key]).to_pandas()[key].astype(int).to_numpy()\n",
    "        if idx.min()>=1 and idx.max()<=n_eval: idx = idx - 1\n",
    "        idx = idx[(idx>=0)&(idx<n_eval)]\n",
    "        yb = np.zeros(n_eval, dtype=int); yb[idx]=1\n",
    "        mapping_src = f\"indices from {la.name} (positional fallback)\"\n",
    "\n",
    "print(f\"Etiquetas construidas: positivos={yb.sum()} / {len(yb)}  | fuente={mapping_src}\")\n",
    "\n",
    "# 3) Métricas con scores directos e invertidos\n",
    "roc1, pr1, ap1 = metrics_from(scores, yb)\n",
    "roc2, pr2, ap2 = metrics_from(-scores, yb)\n",
    "\n",
    "print(f\"Scores tal cual   -> ROC:{roc1:.4f} | PR-AUC:{pr1:.4f} | AP:{ap1:.4f}\")\n",
    "print(f\"Scores invertidos -> ROC:{roc2:.4f} | PR-AUC:{pr2:.4f} | AP:{ap2:.4f}\")\n",
    "print(\"👉 Usa la versión con mejores métricas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positivos (JOIN): 1003200 / 27789660\n",
      "ROC-AUC:0.5069 | PR-AUC:0.0389 | AP:0.0367\n",
      "@1% -> P:0.0423 | R:0.0117 | F1:0.0183 (k=277896)\n",
      "✅ Actualizado: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_config.json\n"
     ]
    }
   ],
   "source": [
    "# === Persistir métricas finales (polarity correcta + @k) ===\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"])\n",
    "DATA_DIR = Path(CFG[\"external_data_dir\"])\n",
    "scores_path = OUT / f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"\n",
    "cfg_path = OUT / f\"{CFG['artifact_prefix']}_config.json\"\n",
    "assert scores_path.exists(), \"No hay scores memmap.\"\n",
    "\n",
    "# 1) Cargar scores y construir y por JOIN con window_id (lo que ya validaste)\n",
    "n_eval = scores_path.stat().st_size // 4\n",
    "scores = np.memmap(scores_path, dtype=np.float32, mode=\"r\", shape=(n_eval,))\n",
    "\n",
    "# encontrar parquet de eval para tomar el ORDEN exacto\n",
    "eval_candidates = [\n",
    "    \"windows_with_labels_aligned.parquet\",\n",
    "    \"eval_windows_aligned.parquet\",\n",
    "    \"windows_with_labels.parquet\",\n",
    "    \"windows_with_labels_aligned_normal.parquet\",\n",
    "]\n",
    "EVAL_PATH = None\n",
    "for name in eval_candidates:\n",
    "    p = DATA_DIR / name\n",
    "    if p.exists():\n",
    "        EVAL_PATH = p; break\n",
    "if EVAL_PATH is None:\n",
    "    cands = sorted(DATA_DIR.glob(\"*windows*aligned*.parquet\"), key=lambda x: x.stat().st_size if x.exists() else 0, reverse=True)\n",
    "    assert cands, \"No encontré parquet de EVAL para inferir orden.\"\n",
    "    EVAL_PATH = cands[0]\n",
    "\n",
    "pf_eval = pq.ParquetFile(EVAL_PATH)\n",
    "key_eval = next((k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in pf_eval.schema_arrow.names), None)\n",
    "assert key_eval is not None, f\"{EVAL_PATH.name} no tiene columna clave esperada.\"\n",
    "eval_key = pf_eval.read(columns=[key_eval]).to_pandas()[key_eval].astype(np.int64).reset_index(drop=True)\n",
    "assert len(eval_key) == n_eval, f\"Desalineación: eval_key={len(eval_key)} vs scores={n_eval}\"\n",
    "\n",
    "# labels_anom como conjunto de claves\n",
    "lab_anom = None\n",
    "for p in [DATA_DIR / \"labels_anom.parquet\", *DATA_DIR.glob(\"*labels_anom*.parquet\")]:\n",
    "    if p.exists(): lab_anom = p; break\n",
    "assert lab_anom is not None, \"No hallé labels_anom.parquet.\"\n",
    "pf_la = pq.ParquetFile(lab_anom)\n",
    "key_lab = key_eval if key_eval in pf_la.schema_arrow.names else (\n",
    "    next((k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in pf_la.schema_arrow.names), None)\n",
    ")\n",
    "assert key_lab is not None, \"labels_anom no tiene una clave compatible.\"\n",
    "anom_keys = pf_la.read(columns=[key_lab]).to_pandas()[key_lab].astype(np.int64).to_numpy()\n",
    "anom_set = set(anom_keys.tolist())\n",
    "\n",
    "yb = eval_key.isin(anom_set).astype(int).to_numpy()\n",
    "print(\"Positivos (JOIN):\", yb.sum(), \"/\", len(yb))\n",
    "\n",
    "# 2) Métricas con scores tal cual (polarity validada)\n",
    "def metrics(scores, yb, k_rate=0.01):\n",
    "    if len(np.unique(yb)) > 1:\n",
    "        roc = roc_auc_score(yb, scores)\n",
    "        prec, rec, _ = precision_recall_curve(yb, scores); pr_auc = auc(rec, prec)\n",
    "        ap = average_precision_score(yb, scores)\n",
    "    else:\n",
    "        roc = pr_auc = ap = float('nan')\n",
    "    k = max(1, int(len(scores) * k_rate))\n",
    "    thr = np.partition(scores, -k)[-k]\n",
    "    pred_topk = (scores >= thr).astype(np.int8)\n",
    "    tp = int(((pred_topk==1) & (yb==1)).sum())\n",
    "    fp = int(((pred_topk==1) & (yb==0)).sum())\n",
    "    fn = int(((pred_topk==0) & (yb==1)).sum())\n",
    "    prec_k = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "    rec_k  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "    f1_k   = 2*prec_k*rec_k/(prec_k+rec_k) if (prec_k+rec_k)>0 else 0.0\n",
    "    return {\"roc_auc\": float(roc), \"pr_auc\": float(pr_auc), \"ap\": float(ap),\n",
    "            \"k\": int(k), \"precision_k\": float(prec_k), \"recall_k\": float(rec_k), \"f1_k\": float(f1_k)}\n",
    "\n",
    "M = metrics(scores, yb, k_rate=0.01)\n",
    "print(f\"ROC-AUC:{M['roc_auc']:.4f} | PR-AUC:{M['pr_auc']:.4f} | AP:{M['ap']:.4f}\")\n",
    "print(f\"@1% -> P:{M['precision_k']:.4f} | R:{M['recall_k']:.4f} | F1:{M['f1_k']:.4f} (k={M['k']})\")\n",
    "\n",
    "# 3) Guardar en config.json (idempotente)\n",
    "cfg = json.loads(cfg_path.read_text()) if cfg_path.exists() else {}\n",
    "cfg[\"external_data_dir\"] = CFG[\"external_data_dir\"]\n",
    "cfg[\"kernel\"] = CFG.get(\"kernel\",\"rbf\")\n",
    "cfg[\"best_params\"] = cfg.get(\"best_params\", {})\n",
    "cfg[\"metrics\"] = {\"roc_auc\": M[\"roc_auc\"], \"pr_auc\": M[\"pr_auc\"], \"ap\": M[\"ap\"]}\n",
    "cfg.setdefault(\"metrics_at_k\", {})[\"0.01\"] = {\n",
    "    \"k\": M[\"k\"], \"precision\": M[\"precision_k\"], \"recall\": M[\"recall_k\"], \"f1\": M[\"f1_k\"]\n",
    "}\n",
    "cfg.update({\"label_file\": lab_anom.name, \"label_col\": key_lab, \"label_mapping\": f\"JOIN on {key_eval}\"})\n",
    "cfg_path.write_text(json.dumps(cfg, indent=2))\n",
    "print(\"✅ Actualizado:\", cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f143c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f143c_level0_col0\" class=\"col_heading level0 col0\" >Modelo</th>\n",
       "      <th id=\"T_f143c_level0_col1\" class=\"col_heading level0 col1\" >nu</th>\n",
       "      <th id=\"T_f143c_level0_col2\" class=\"col_heading level0 col2\" >gamma</th>\n",
       "      <th id=\"T_f143c_level0_col3\" class=\"col_heading level0 col3\" >ROC-AUC</th>\n",
       "      <th id=\"T_f143c_level0_col4\" class=\"col_heading level0 col4\" >PR-AUC</th>\n",
       "      <th id=\"T_f143c_level0_col5\" class=\"col_heading level0 col5\" >AP</th>\n",
       "      <th id=\"T_f143c_level0_col6\" class=\"col_heading level0 col6\" >P@1%</th>\n",
       "      <th id=\"T_f143c_level0_col7\" class=\"col_heading level0 col7\" >R@1%</th>\n",
       "      <th id=\"T_f143c_level0_col8\" class=\"col_heading level0 col8\" >F1@1%</th>\n",
       "      <th id=\"T_f143c_level0_col9\" class=\"col_heading level0 col9\" >Labels fuente</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f143c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f143c_row0_col0\" class=\"data row0 col0\" >One-Class SVM (RBF)</td>\n",
       "      <td id=\"T_f143c_row0_col1\" class=\"data row0 col1\" >0.050000</td>\n",
       "      <td id=\"T_f143c_row0_col2\" class=\"data row0 col2\" >0.010000</td>\n",
       "      <td id=\"T_f143c_row0_col3\" class=\"data row0 col3\" >0.5069</td>\n",
       "      <td id=\"T_f143c_row0_col4\" class=\"data row0 col4\" >0.0389</td>\n",
       "      <td id=\"T_f143c_row0_col5\" class=\"data row0 col5\" >0.0367</td>\n",
       "      <td id=\"T_f143c_row0_col6\" class=\"data row0 col6\" >0.0423</td>\n",
       "      <td id=\"T_f143c_row0_col7\" class=\"data row0 col7\" >0.0117</td>\n",
       "      <td id=\"T_f143c_row0_col8\" class=\"data row0 col8\" >0.0183</td>\n",
       "      <td id=\"T_f143c_row0_col9\" class=\"data row0 col9\" >labels_anom.parquet · window_id (JOIN on window_id)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f1559798aa0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Resumen final (listo para el informe) ===\n",
    "import json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "cfg_path = Path(CFG[\"out_dir\"]) / f\"{CFG['artifact_prefix']}_config.json\"\n",
    "cfg = json.loads(cfg_path.read_text())\n",
    "\n",
    "metrics = cfg.get(\"metrics\", {})\n",
    "metrics_k = cfg.get(\"metrics_at_k\", {}).get(\"0.01\", {})\n",
    "best_params = cfg.get(\"best_params\", {})\n",
    "\n",
    "row = {\n",
    "    \"Modelo\": \"One-Class SVM (RBF)\",\n",
    "    \"nu\": best_params.get(\"nu\", CFG.get(\"svm_nu_grid\",[None])[0]),\n",
    "    \"gamma\": best_params.get(\"gamma\", CFG.get(\"svm_gamma_grid\",[None])[0]),\n",
    "    \"ROC-AUC\": metrics.get(\"roc_auc\"),\n",
    "    \"PR-AUC\": metrics.get(\"pr_auc\"),\n",
    "    \"AP\": metrics.get(\"ap\"),\n",
    "    \"P@1%\": metrics_k.get(\"precision\"),\n",
    "    \"R@1%\": metrics_k.get(\"recall\"),\n",
    "    \"F1@1%\": metrics_k.get(\"f1\"),\n",
    "    \"Labels fuente\": f\"{cfg.get('label_file','?')} · {cfg.get('label_col','?')} ({cfg.get('label_mapping','?')})\",\n",
    "}\n",
    "\n",
    "df = pd.DataFrame([row])\n",
    "display(df.style.format({\n",
    "    \"ROC-AUC\":\"{:.4f}\", \"PR-AUC\":\"{:.4f}\", \"AP\":\"{:.4f}\",\n",
    "    \"P@1%\":\"{:.4f}\", \"R@1%\":\"{:.4f}\", \"F1@1%\":\"{:.4f}\"\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m20000\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m X2d \u001b[38;5;241m=\u001b[39m PCA(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX_eval\u001b[49m[idx])\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(X2d[:,\u001b[38;5;241m0\u001b[39m], X2d[:,\u001b[38;5;241m1\u001b[39m], c\u001b[38;5;241m=\u001b[39myb[idx], cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoolwarm\u001b[39m\u001b[38;5;124m'\u001b[39m, s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistribución de anomalías (labels_anom)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_eval' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "idx = np.random.choice(len(scores), 20000, replace=False)\n",
    "X2d = PCA(2).fit_transform(X_eval[idx])\n",
    "plt.scatter(X2d[:,0], X2d[:,1], c=yb[idx], cmap='coolwarm', s=2)\n",
    "plt.title(\"Distribución de anomalías (labels_anom)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
