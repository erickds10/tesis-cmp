{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OC-SVM Â· AIS Anomaly Detection (Lightning, 110 GB RAM)\n",
    "- Lectura **robusta** de parquets (auto-resuelve ruta; valida formato Parquet).\n",
    "- Preprocesamiento **memory-safe** (imputaciÃ³n + estÃ¡ndar in-place).\n",
    "- HP search **paralela** (joblib) y entrenamiento OC-SVM con **cache grande**.\n",
    "- EvaluaciÃ³n **reanudable** con memmap + progreso incremental.\n",
    "- Artefactos y resultados en `./data/ocsvm_runs` (seguro en este proyecto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:    erickdsuarez10\n",
      "Host:    computeinstance-e00exnkvr257g0k5f5\n",
      "Python:  /home/zeus/miniconda3/envs/cloudspace/bin/python\n",
      "CWD:     /teamspace/studios/this_studio\n",
      "/data exists?: False\n"
     ]
    }
   ],
   "source": [
    "# --- DiagnÃ³stico del entorno ---\n",
    "import os, sys, getpass, socket\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"User:   \", getpass.getuser())\n",
    "print(\"Host:   \", socket.gethostname())\n",
    "print(\"Python: \", sys.executable)\n",
    "print(\"CWD:    \", os.getcwd())\n",
    "print(\"/data exists?:\", Path(\"/data\").exists())\n",
    "if Path(\"/data\").exists():\n",
    "    print(\"#parquets en /data:\", len(list(Path(\"/data\").glob(\"*.parquet\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LECTURA  : /teamspace/studios/this_studio/data\n",
      "SALIDAS  : /teamspace/studios/this_studio/data/ocsvm_runs\n",
      "#parquets: 17\n",
      "CFG: {\n",
      "  \"external_data_dir\": \"/teamspace/studios/this_studio/data\",\n",
      "  \"out_dir\": \"/teamspace/studios/this_studio/data/ocsvm_runs\",\n",
      "  \"artifact_prefix\": \"ocsvm_rbf\",\n",
      "  \"svm_nu_grid\": [\n",
      "    0.01,\n",
      "    0.05,\n",
      "    0.1\n",
      "  ],\n",
      "  \"svm_gamma_grid\": [\n",
      "    \"scale\",\n",
      "    0.01\n",
      "  ],\n",
      "  \"kernel\": \"rbf\",\n",
      "  \"kfold_splits\": 5,\n",
      "  \"max_train_samples\": 800000,\n",
      "  \"max_search_samples\": 400000,\n",
      "  \"eval_batch_size\": 2000000\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Config + auto-resoluciÃ³n de fuente de lectura (solo-lectura) y salidas locales\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "\n",
    "def count_parquets(p: Path) -> int:\n",
    "    try:\n",
    "        return len(list(p.glob(\"*.parquet\"))) if p.exists() else 0\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "CANDIDATES = [\n",
    "    Path(\"/data\").resolve(),\n",
    "    Path(\"/teamspace/studios/this_studio/data\").resolve(),\n",
    "    Path(\"./data\").resolve(),\n",
    "]\n",
    "\n",
    "EXTERNAL_DATA_DIR = None\n",
    "for cand in CANDIDATES:\n",
    "    if cand.exists() and count_parquets(cand) > 0:\n",
    "        EXTERNAL_DATA_DIR = cand\n",
    "        break\n",
    "if EXTERNAL_DATA_DIR is None:\n",
    "    # fallback: usa data local aunque estÃ© vacÃ­a (para que no falle la carga y puedas copiar ahÃ­)\n",
    "    EXTERNAL_DATA_DIR = Path(\"/teamspace/studios/this_studio/data\").resolve()\n",
    "    print(\"âš ï¸ No hallÃ© parquets; usando ruta local del Studio por defecto:\", EXTERNAL_DATA_DIR)\n",
    "\n",
    "OUT_DIR = Path(\"data/ocsvm_runs\").resolve()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CFG = {\n",
    "    \"external_data_dir\": str(EXTERNAL_DATA_DIR),\n",
    "    \"out_dir\": str(OUT_DIR),\n",
    "    \"artifact_prefix\": \"ocsvm_rbf\",\n",
    "    # HP + lÃ­mites\n",
    "    \"svm_nu_grid\": [0.01, 0.05, 0.1],\n",
    "    \"svm_gamma_grid\": [\"scale\", 0.01],\n",
    "    \"kernel\": \"rbf\",\n",
    "    \"kfold_splits\": 5,\n",
    "    \"max_train_samples\": 800_000,     # ajustados para 110 GB RAM\n",
    "    \"max_search_samples\": 400_000,\n",
    "    \"eval_batch_size\": 2_000_000,     # lotes grandes para acelerar evaluaciÃ³n\n",
    "}\n",
    "\n",
    "print(\"LECTURA  :\", CFG[\"external_data_dir\"])\n",
    "print(\"SALIDAS  :\", CFG[\"out_dir\"])\n",
    "print(\"#parquets:\", count_parquets(Path(CFG[\"external_data_dir\"])))\n",
    "print(\"CFG:\", json.dumps({k: CFG[k] for k in CFG}, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores: 32 -> N_JOBS=31\n",
      "RAM disponible: 129.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Performance pack (usa todos los cores menos 1)\n",
    "import os, psutil\n",
    "\n",
    "N_JOBS = max(1, os.cpu_count() - 1)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(N_JOBS)\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(N_JOBS)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(N_JOBS)\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(N_JOBS)\n",
    "\n",
    "print(f\"Cores: {os.cpu_count()} -> N_JOBS={N_JOBS}\")\n",
    "print(f\"RAM disponible: {psutil.virtual_memory().available/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores visibles: 32 | N_JOBS=31\n",
      "RAM disponible aprox: 129.0 GB\n",
      "CFG actualizado:  {'max_train_samples': 800000, 'max_search_samples': 400000, 'eval_batch_size': 2000000}\n"
     ]
    }
   ],
   "source": [
    "# --- Performance pack (RAM 110 GB + todos los cores) ---\n",
    "import os, multiprocessing, psutil\n",
    "\n",
    "# Usa (n_cores - 1) para no saturar el sistema\n",
    "N_JOBS = max(1, os.cpu_count() - 1)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(N_JOBS)\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(N_JOBS)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(N_JOBS)\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(N_JOBS)\n",
    "\n",
    "print(f\"Cores visibles: {os.cpu_count()} | N_JOBS={N_JOBS}\")\n",
    "print(f\"RAM disponible aprox: {psutil.virtual_memory().available/1e9:.1f} GB\")\n",
    "\n",
    "# Recomendaciones de tamaÃ±o con 110 GB RAM (float32 => ~76 bytes/fila si F=19)\n",
    "# Ajusta tu CFG despuÃ©s de ejecutar esta celda:\n",
    "CFG[\"max_train_samples\"]  = 800_000     # si tu entrenamiento tarda demasiado, bÃ¡jalo a 500k\n",
    "CFG[\"max_search_samples\"] = 400_000\n",
    "CFG[\"eval_batch_size\"]    = 2_000_000   # sube el lote de evaluaciÃ³n (menos overhead)\n",
    "\n",
    "# Guardar\n",
    "print(\"CFG actualizado: \",\n",
    "      {k: CFG[k] for k in [\"max_train_samples\",\"max_search_samples\",\"eval_batch_size\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: /teamspace/studios/this_studio/data\n",
      "TRAIN -> windows_aligned_normal.parquet | X_train: (27789660, 19)\n",
      "[pick_valid] â†’ windows_with_labels_aligned_norm.parquet (pattern *windows_with_labels_aligned_norm*.parquet)\n",
      "[pick_valid] â†’ eval_windows_aligned_norm.parquet (pattern *eval_windows_aligned_norm*.parquet)\n",
      "EVAL -> windows_with_labels_aligned.parquet | X_eval: (27789660, 19) | y_eval: (27789660,)\n",
      "Train -> X: (27789660, 19) | groups: 27789660\n",
      "Eval  -> X: (27789660, 19) | y: (27789660,) | groups: 27789660\n",
      "N feats: train 19 | eval 19\n"
     ]
    }
   ],
   "source": [
    "# Carga robusta desde CFG[\"external_data_dir\"] (con validaciÃ³n Parquet)\n",
    "import numpy as np, pandas as pd, gc\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "DATA_DIR = Path(CFG[\"external_data_dir\"])\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "\n",
    "def is_valid_parquet(path: Path) -> bool:\n",
    "    try:\n",
    "        pq.ParquetFile(path); return True\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] No Parquet vÃ¡lido -> {path.name} :: {type(e).__name__}\")\n",
    "        return False\n",
    "\n",
    "def read_parquet_min(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_parquet(path, engine=\"pyarrow\")\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_float_dtype(df[c]): df[c] = df[c].astype(np.float32)\n",
    "        elif pd.api.types.is_integer_dtype(df[c]) and df[c].max() <= np.iinfo(np.int32).max:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "    return df\n",
    "\n",
    "def detect_label_col(df):\n",
    "    for k in [\"y\",\"label\",\"is_suspicious\",\"target\"]:\n",
    "        if k in df.columns: return k\n",
    "    return None\n",
    "\n",
    "def detect_group_col(df):\n",
    "    for k in [\"mmsi\",\"group\",\"ship_id\"]:\n",
    "        if k in df.columns: return k\n",
    "    return None\n",
    "\n",
    "def pick_valid(base: Path, *candidates):\n",
    "    # exactos primero\n",
    "    for n in candidates:\n",
    "        if \"*\" not in n and \"?\" not in n and \"[\" not in n:\n",
    "            p = base / n\n",
    "            if p.exists() and is_valid_parquet(p): return p\n",
    "    # luego patrones (elige el mayor por tamaÃ±o que sea vÃ¡lido)\n",
    "    for patt in candidates:\n",
    "        if any(ch in patt for ch in \"*?[]\"):\n",
    "            matches = list(base.glob(patt))\n",
    "            matches.sort(key=lambda x: x.stat().st_size if x.exists() else 0, reverse=True)\n",
    "            for m in matches:\n",
    "                if is_valid_parquet(m): \n",
    "                    print(f\"[pick_valid] â†’ {m.name} (pattern {patt})\"); \n",
    "                    return m\n",
    "    return None\n",
    "\n",
    "# --- TRAIN (ventanas normales) ---\n",
    "train_path = pick_valid(\n",
    "    DATA_DIR,\n",
    "    \"windows_aligned_normal.parquet\",\n",
    "    \"norm_windows_flat.parquet\",\n",
    "    \"ais_norm_windows.parquet\",\n",
    "    \"*windows_aligned_norm*.parquet\",\n",
    "    \"*norm*windows*.parquet\",\n",
    "    \"*ais_norm_windows*.parquet\",\n",
    ")\n",
    "if train_path is None:\n",
    "    raise FileNotFoundError(\"No encontrÃ© TRAIN normal vÃ¡lido en la ruta de datos.\")\n",
    "df_tr = read_parquet_min(train_path)\n",
    "\n",
    "ycol_tr = detect_label_col(df_tr)\n",
    "gcol_tr = detect_group_col(df_tr)\n",
    "drop_common = {\"lat\",\"lon\",\"idx\",\"idx_end\",\"window_id\"}\n",
    "drop_train = set([c for c in [ycol_tr, gcol_tr] if c]) | drop_common\n",
    "feat_tr = [c for c in df_tr.columns if c not in drop_train]\n",
    "X_train = df_tr[feat_tr].to_numpy(dtype=np.float32)\n",
    "groups_train = df_tr[gcol_tr].to_numpy() if gcol_tr else None\n",
    "print(\"TRAIN ->\", train_path.name, \"| X_train:\", X_train.shape)\n",
    "\n",
    "# --- EVAL (Ãºnico o split) ---\n",
    "eval_single = pick_valid(DATA_DIR,\n",
    "    \"windows_with_labels_aligned.parquet\", \"*windows_with_labels_aligned*.parquet\",\n",
    "    \"eval_windows_aligned.parquet\", \"*eval_windows_aligned*.parquet\",\n",
    "    \"windows_with_labels.parquet\", \"*windows_with_labels*.parquet\",\n",
    ")\n",
    "eval_wl_norm = pick_valid(DATA_DIR, \"windows_with_labels_aligned_normal.parquet\",\n",
    "                          \"*windows_with_labels_aligned_norm*.parquet\", \"*windows_with_labels_aligned_normal*.parquet\")\n",
    "eval_wl_anom = pick_valid(DATA_DIR, \"windows_with_labels_aligned_anom.parquet\",\n",
    "                          \"*windows_with_labels_aligned_anom*.parquet\")\n",
    "eval_norm    = pick_valid(DATA_DIR, \"eval_windows_aligned_normal.parquet\",\n",
    "                          \"*eval_windows_aligned_norm*.parquet\", \"*eval_windows_aligned_normal*.parquet\")\n",
    "eval_anom    = pick_valid(DATA_DIR, \"eval_windows_aligned_anom.parquet\",\n",
    "                          \"*eval_windows_aligned_anom*.parquet\")\n",
    "labels_any   = pick_valid(DATA_DIR, \"eval_labels_aligned.parquet\", \"*eval_labels_aligned*.parquet\",\n",
    "                          \"labels.parquet\", \"labels_anom.parquet\", \"*labels*.parquet\")\n",
    "\n",
    "if eval_single is not None:\n",
    "    df_ev = read_parquet_min(eval_single)\n",
    "    ycol_ev = detect_label_col(df_ev)\n",
    "    gcol_ev = detect_group_col(df_ev)\n",
    "    if ycol_ev is None:\n",
    "        if labels_any is None: raise FileNotFoundError(\"Eval Ãºnico sin etiquetas y no hay archivo de labels.\")\n",
    "        df_y = read_parquet_min(labels_any)\n",
    "        ycol_y = detect_label_col(df_y) or df_y.select_dtypes(include=[\"int8\",\"int16\",\"int32\"]).columns[-1]\n",
    "        if len(df_y) != len(df_ev): raise ValueError(f\"DesalineaciÃ³n eval vs labels: {len(df_ev)} vs {len(df_y)}\")\n",
    "        drop_eval = set([gcol_ev]) | drop_common\n",
    "        feat_ev = [c for c in df_ev.columns if c not in drop_eval]\n",
    "        X_eval = df_ev[feat_ev].to_numpy(dtype=np.float32)\n",
    "        y_eval = df_y[ycol_y].astype(np.int8).to_numpy()\n",
    "    else:\n",
    "        drop_eval = set([ycol_ev, gcol_ev]) | drop_common\n",
    "        feat_ev = [c for c in df_ev.columns if c not in drop_eval]\n",
    "        X_eval = df_ev[feat_ev].to_numpy(dtype=np.float32)\n",
    "        y_eval = df_ev[ycol_ev].astype(np.int8).to_numpy()\n",
    "    groups_eval = df_ev[gcol_ev].to_numpy() if gcol_ev else None\n",
    "    print(\"EVAL ->\", eval_single.name, \"| X_eval:\", X_eval.shape, \"| y_eval:\", y_eval.shape)\n",
    "\n",
    "elif eval_wl_norm is not None and eval_wl_anom is not None:\n",
    "    dn, da = read_parquet_min(eval_wl_norm), read_parquet_min(eval_wl_anom)\n",
    "    common = [c for c in dn.columns if c in da.columns]\n",
    "    dn, da = dn[common], da[common]\n",
    "    ycol_ev, gcol_ev = detect_label_col(dn), detect_group_col(dn)\n",
    "    drop_eval = set([ycol_ev, gcol_ev]) | drop_common\n",
    "    feat_ev = [c for c in common if c not in drop_eval]\n",
    "    X_eval = pd.concat([dn[feat_ev], da[feat_ev]], ignore_index=True).to_numpy(np.float32)\n",
    "    y_eval = pd.concat([dn[ycol_ev], da[ycol_ev]], ignore_index=True).astype(np.int8).to_numpy()\n",
    "    groups_eval = (pd.concat([dn[gcol_ev], da[gcol_ev]], ignore_index=True).to_numpy() if gcol_ev else None)\n",
    "    print(\"EVAL ->\", eval_wl_norm.name, \"+\", eval_wl_anom.name, \"| X_eval:\", X_eval.shape, \"| y_eval:\", y_eval.shape)\n",
    "\n",
    "elif eval_norm is not None and eval_anom is not None:\n",
    "    if labels_any is None: raise FileNotFoundError(\"Eval split sin labels embebidas y no hay archivo de labels.\")\n",
    "    dn, da, dy = read_parquet_min(eval_norm), read_parquet_min(eval_anom), read_parquet_min(labels_any)\n",
    "    common = [c for c in dn.columns if c in da.columns]\n",
    "    dn, da = dn[common], da[common]\n",
    "    gcol_ev = detect_group_col(dn)\n",
    "    drop_eval = set([gcol_ev]) | drop_common\n",
    "    feat_ev = [c for c in common if c not in drop_eval]\n",
    "    df_concat = pd.concat([dn[feat_ev], da[feat_ev]], ignore_index=True)\n",
    "    X_eval = df_concat.to_numpy(np.float32)\n",
    "    ycol_y = detect_label_col(dy) or dy.select_dtypes(include=[\"int8\",\"int16\",\"int32\"]).columns[-1]\n",
    "    if len(dy) != len(df_concat): raise ValueError(f\"DesalineaciÃ³n eval concat vs labels: {len(df_concat)} vs {len(dy)}\")\n",
    "    y_eval = dy[ycol_y].astype(np.int8).to_numpy()\n",
    "    groups_eval = (pd.concat([dn[gcol_ev], da[gcol_ev]], ignore_index=True).to_numpy() if gcol_ev else None)\n",
    "    print(\"EVAL ->\", eval_norm.name, \"+\", eval_anom.name, \"| X_eval:\", X_eval.shape, \"| y_eval:\", y_eval.shape)\n",
    "\n",
    "else:\n",
    "    raise FileNotFoundError(\"No se pudo resolver un set de EVAL vÃ¡lido.\")\n",
    "\n",
    "del df_tr; gc.collect()\n",
    "print(\"Train -> X:\", X_train.shape, \"| groups:\", None if groups_train is None else len(groups_train))\n",
    "print(\"Eval  -> X:\", X_eval.shape,  \"| y:\", y_eval.shape, \"| groups:\", None if groups_eval is None else len(groups_eval))\n",
    "print(\"N feats: train\", X_train.shape[1], \"| eval\", X_eval.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sampled: (799920, 19)\n",
      "Scaled train: (799920, 19)\n"
     ]
    }
   ],
   "source": [
    "# ImputaciÃ³n + estÃ¡ndar + muestreo (in-place, memory-safe)\n",
    "import numpy as np, os, gc\n",
    "\n",
    "def sample_by_group(n_max, X, groups):\n",
    "    if (n_max is None) or (X.shape[0] <= n_max):\n",
    "        idx = np.arange(X.shape[0]); return X, (groups if groups is not None else None), idx\n",
    "    rng = np.random.default_rng(42)\n",
    "    if groups is None:\n",
    "        idx = rng.choice(X.shape[0], n_max, replace=False); return X[idx], None, idx\n",
    "    uniq = np.unique(groups); per_g = max(1, n_max // len(uniq)); take = []\n",
    "    for g in uniq:\n",
    "        g_idx = np.where(groups == g)[0]\n",
    "        take.extend(rng.choice(g_idx, min(per_g, g_idx.size), replace=False).tolist())\n",
    "    take = np.array(take)\n",
    "    if take.size > n_max: take = rng.choice(take, n_max, replace=False)\n",
    "    return X[take], groups[take], take\n",
    "\n",
    "def colwise_nanmedian(X):\n",
    "    Xc = X.copy(); Xc[~np.isfinite(Xc)] = np.nan\n",
    "    med = np.nanmedian(Xc, axis=0)\n",
    "    med = np.where(np.isfinite(med), med, 0.0).astype(np.float32)\n",
    "    return med\n",
    "\n",
    "def impute_inplace(X, medians):\n",
    "    bad = ~np.isfinite(X)\n",
    "    if bad.any():\n",
    "        cols = np.where(bad)[1]\n",
    "        X[bad] = medians[cols]\n",
    "\n",
    "def fit_standardizer(X):\n",
    "    mean = X.mean(axis=0).astype(np.float32)\n",
    "    var  = X.var(axis=0).astype(np.float32)\n",
    "    std  = np.sqrt(var, dtype=np.float32); std[std == 0.0] = 1.0\n",
    "    return mean, std\n",
    "\n",
    "def apply_standardizer_inplace(X, mean, std):\n",
    "    X -= mean; X /= std\n",
    "\n",
    "X_train_s, groups_train_s, _ = sample_by_group(CFG[\"max_train_samples\"], X_train, groups_train)\n",
    "print(\"Train sampled:\", X_train_s.shape)\n",
    "\n",
    "X_train_s = X_train_s.astype(np.float32, copy=False)\n",
    "X_train_s[~np.isfinite(X_train_s)] = np.nan\n",
    "train_medians = colwise_nanmedian(X_train_s)\n",
    "impute_inplace(X_train_s, train_medians)\n",
    "train_mean, train_std = fit_standardizer(X_train_s)\n",
    "apply_standardizer_inplace(X_train_s, train_mean, train_std)\n",
    "\n",
    "X_train_sc = X_train_s\n",
    "groups_train = groups_train_s\n",
    "\n",
    "# Guardar preprocesamiento\n",
    "np.save(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_imputer_medians.npy\"), train_medians)\n",
    "np.savez(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_scaler_params.npz\"), mean=train_mean, std=train_std)\n",
    "\n",
    "# Transformador por lotes para eval\n",
    "def transform_eval_in_batches(X, batch_size=CFG[\"eval_batch_size\"]):\n",
    "    n = X.shape[0]\n",
    "    for s in range(0, n, batch_size):\n",
    "        e = min(s + batch_size, n)\n",
    "        Xe = X[s:e].astype(np.float32, copy=False)\n",
    "        Xe[~np.isfinite(Xe)] = np.nan\n",
    "        impute_inplace(Xe, train_medians)\n",
    "        apply_standardizer_inplace(Xe, train_mean, train_std)\n",
    "        yield s, e, Xe\n",
    "\n",
    "gc.collect()\n",
    "print(\"Scaled train:\", X_train_sc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=31)]: Using backend LokyBackend with 31 concurrent workers.\n",
      "[Parallel(n_jobs=31)]: Done   3 out of   6 | elapsed: 72.9min remaining: 72.9min\n",
      "[Parallel(n_jobs=31)]: Done   6 out of   6 | elapsed: 124.8min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>rate_mean</th>\n",
       "      <th>rate_std</th>\n",
       "      <th>obj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'gamma': 0.01, 'nu': 0.05}</td>\n",
       "      <td>0.051925</td>\n",
       "      <td>0.007809</td>\n",
       "      <td>0.009734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'gamma': 'scale', 'nu': 0.05}</td>\n",
       "      <td>0.056986</td>\n",
       "      <td>0.015195</td>\n",
       "      <td>0.022181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'gamma': 0.01, 'nu': 0.01}</td>\n",
       "      <td>0.010304</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>0.041470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'gamma': 'scale', 'nu': 0.01}</td>\n",
       "      <td>0.015864</td>\n",
       "      <td>0.010365</td>\n",
       "      <td>0.044501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'gamma': 0.01, 'nu': 0.1}</td>\n",
       "      <td>0.103243</td>\n",
       "      <td>0.018722</td>\n",
       "      <td>0.071965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'gamma': 'scale', 'nu': 0.1}</td>\n",
       "      <td>0.106106</td>\n",
       "      <td>0.020934</td>\n",
       "      <td>0.077039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           params  rate_mean  rate_std       obj\n",
       "4     {'gamma': 0.01, 'nu': 0.05}   0.051925  0.007809  0.009734\n",
       "1  {'gamma': 'scale', 'nu': 0.05}   0.056986  0.015195  0.022181\n",
       "3     {'gamma': 0.01, 'nu': 0.01}   0.010304  0.001774  0.041470\n",
       "0  {'gamma': 'scale', 'nu': 0.01}   0.015864  0.010365  0.044501\n",
       "5      {'gamma': 0.01, 'nu': 0.1}   0.103243  0.018722  0.071965\n",
       "2   {'gamma': 'scale', 'nu': 0.1}   0.106106  0.020934  0.077039"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'gamma': 0.01, 'nu': 0.05} | splits: 5 | search_subset: (399960, 19)\n"
     ]
    }
   ],
   "source": [
    "# HP search paralela (nu, gamma) minimizando |outlier_rate - 5%|\n",
    "import numpy as np, pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import GroupKFold, KFold, ParameterGrid\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "param_grid = list(ParameterGrid({\"nu\": CFG[\"svm_nu_grid\"], \"gamma\": CFG[\"svm_gamma_grid\"]}))\n",
    "target_outlier_rate = 0.05\n",
    "\n",
    "def build_search_subset(X, groups, n_max):\n",
    "    if (n_max is None) or (X.shape[0] <= n_max): return X, groups\n",
    "    rng = np.random.default_rng(123)\n",
    "    if groups is None:\n",
    "        idx = rng.choice(X.shape[0], n_max, replace=False); return X[idx], None\n",
    "    uniq = np.unique(groups); per_g = max(1, n_max // len(uniq)); take = []\n",
    "    for g in uniq:\n",
    "        g_idx = np.where(groups == g)[0]\n",
    "        take.extend(rng.choice(g_idx, min(per_g, g_idx.size), replace=False).tolist())\n",
    "    take = np.array(take)\n",
    "    if take.size > n_max: take = rng.choice(take, n_max, replace=False)\n",
    "    return X[take], groups[take]\n",
    "\n",
    "X_search, groups_search = build_search_subset(X_train_sc, groups_train, CFG[\"max_search_samples\"])\n",
    "\n",
    "if (groups_search is not None) and (len(np.unique(groups_search)) >= 2):\n",
    "    n_splits = min(CFG[\"kfold_splits\"], len(np.unique(groups_search)))\n",
    "    splitter = GroupKFold(n_splits=n_splits); split_args = dict(X=X_search, y=None, groups=groups_search)\n",
    "else:\n",
    "    n_splits = max(2, CFG[\"kfold_splits\"])\n",
    "    splitter = KFold(n_splits=n_splits, shuffle=True, random_state=42); split_args = dict(X=X_search, y=None)\n",
    "\n",
    "def outlier_rate(pred): return float((pred == -1).mean())\n",
    "\n",
    "def eval_param(p):\n",
    "    rates = []\n",
    "    for tr_idx, va_idx in splitter.split(**split_args):\n",
    "        Xtr, Xva = X_search[tr_idx], X_search[va_idx]\n",
    "        m = OneClassSVM(kernel=CFG[\"kernel\"], nu=p[\"nu\"], gamma=p[\"gamma\"],\n",
    "                        cache_size=2048, tol=1e-3, shrinking=True)\n",
    "        m.fit(Xtr)\n",
    "        rates.append(outlier_rate(m.predict(Xva)))\n",
    "    rate_mean, rate_std = float(np.mean(rates)), float(np.std(rates))\n",
    "    obj = abs(rate_mean - target_outlier_rate) + rate_std\n",
    "    return {\"params\": p, \"rate_mean\": rate_mean, \"rate_std\": rate_std, \"obj\": obj}\n",
    "\n",
    "rows = Parallel(n_jobs=N_JOBS, prefer=\"processes\", verbose=5)(\n",
    "    delayed(eval_param)(p) for p in param_grid\n",
    ")\n",
    "\n",
    "res_df = pd.DataFrame(rows).sort_values(\"obj\")\n",
    "best_cfg = res_df.iloc[0][\"params\"]\n",
    "display(res_df.head(10))\n",
    "print(\"Best params:\", best_cfg, \"| splits:\", n_splits, \"| search_subset:\", X_search.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model trained. Params: {'gamma': 0.01, 'nu': 0.05}\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento final OC-SVM (cache grande)\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "if \"best_cfg\" not in globals() or not best_cfg:  # fallback\n",
    "    best_cfg = {\"nu\": 0.05, \"gamma\": \"scale\"}\n",
    "\n",
    "final_model = OneClassSVM(\n",
    "    kernel=CFG.get(\"kernel\",\"rbf\"),\n",
    "    nu=best_cfg[\"nu\"],\n",
    "    gamma=best_cfg[\"gamma\"],\n",
    "    cache_size=8192,   # 8 GB cache para libSVM\n",
    "    tol=1e-3,\n",
    "    shrinking=True\n",
    ")\n",
    "final_model.fit(X_train_sc)\n",
    "print(\"Final model trained. Params:\", best_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESUME] next_start = 27,789,660/27,789,660\n",
      "ROC-AUC: nan | PR-AUC: nan | AP: nan\n",
      "Scores memmap: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_eval_scores_mm.dat\n"
     ]
    }
   ],
   "source": [
    "# EvaluaciÃ³n por lotes con REANUDACIÃ“N + memmap (rÃ¡pida)\n",
    "import os, json, time, pickle, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"]); OUT.mkdir(parents=True, exist_ok=True)\n",
    "n_eval = X_eval.shape[0]\n",
    "scores_path = OUT / f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"\n",
    "progress_path = OUT / f\"{CFG['artifact_prefix']}_progress.json\"\n",
    "expected_bytes = n_eval * 4\n",
    "\n",
    "# memmap correcto o recrea\n",
    "if scores_path.exists() and scores_path.stat().st_size != expected_bytes:\n",
    "    print(f\"[WARN] Memmap {scores_path.stat().st_size} != {expected_bytes} -> recreando.\")\n",
    "    scores_path.unlink()\n",
    "\n",
    "mode = \"r+\" if scores_path.exists() else \"w+\"\n",
    "scores_mm = np.memmap(scores_path, dtype=np.float32, mode=mode, shape=(n_eval,))\n",
    "if mode == \"w+\":\n",
    "    scores_mm[:] = np.nan\n",
    "    scores_mm.flush()\n",
    "\n",
    "# cargar preprocesamiento por si el kernel cambiÃ³\n",
    "medians = np.load(OUT / f\"{CFG['artifact_prefix']}_imputer_medians.npy\")\n",
    "sp = np.load(OUT / f\"{CFG['artifact_prefix']}_scaler_params.npz\")\n",
    "train_mean, train_std = sp[\"mean\"], sp[\"std\"]\n",
    "train_std = train_std.copy(); train_std[train_std==0]=1.0\n",
    "\n",
    "def impute_inplace(X, med):\n",
    "    bad = ~np.isfinite(X)\n",
    "    if bad.any(): X[bad] = med[np.where(bad)[1]]\n",
    "def standardize_inplace(X, m, s):\n",
    "    X -= m; X /= s\n",
    "\n",
    "# reanudaciÃ³n\n",
    "start = 0\n",
    "if progress_path.exists():\n",
    "    try: start = int(json.loads(progress_path.read_text()).get(\"next_start\", 0))\n",
    "    except: start = 0\n",
    "if start <= 0:\n",
    "    nan_mask = np.isnan(scores_mm)\n",
    "    start = int(np.argmax(nan_mask)) if nan_mask.any() else n_eval\n",
    "print(f\"[RESUME] next_start = {start:,}/{n_eval:,}\")\n",
    "\n",
    "bs = int(CFG.get(\"eval_batch_size\", 2_000_000))\n",
    "t0 = time.time(); last = t0\n",
    "\n",
    "try:\n",
    "    for s in range(start, n_eval, bs):\n",
    "        e = min(s + bs, n_eval)\n",
    "        Xe = X_eval[s:e].astype(np.float32, copy=False)\n",
    "        Xe[~np.isfinite(Xe)] = np.nan\n",
    "        impute_inplace(Xe, medians)\n",
    "        standardize_inplace(Xe, train_mean, train_std)\n",
    "\n",
    "        scores_mm[s:e] = -final_model.decision_function(Xe)\n",
    "        scores_mm.flush()\n",
    "        progress_path.write_text(json.dumps({\"next_start\": e}))\n",
    "\n",
    "        now = time.time()\n",
    "        if now - last > 10:\n",
    "            rate = (e - start) / max(1e-6, (now - t0))\n",
    "            print(f\"Progress: {100*e/n_eval:5.2f}% | {e:,}/{n_eval:,} | ~{rate:,.0f} rows/s\")\n",
    "            last = now\n",
    "except KeyboardInterrupt:\n",
    "    scores_mm.flush(); progress_path.write_text(json.dumps({\"next_start\": e})); print(\"\\n[INTERRUPTED] Guardado.\")\n",
    "    raise\n",
    "\n",
    "scores = scores_mm\n",
    "yb = y_eval.astype(int)\n",
    "if len(np.unique(yb)) > 1:\n",
    "    roc = roc_auc_score(yb, scores)\n",
    "    prec, rec, _ = precision_recall_curve(yb, scores); pr_auc = auc(rec, prec)\n",
    "    ap = average_precision_score(yb, scores)\n",
    "else:\n",
    "    roc = pr_auc = ap = np.nan\n",
    "\n",
    "print(f\"ROC-AUC: {roc:.4f} | PR-AUC: {pr_auc:.4f} | AP: {ap:.4f}\")\n",
    "print(\"Scores memmap:\", str(scores_path))\n",
    "\n",
    "# Guardar modelo + config + mÃ©tricas\n",
    "with open(OUT / f\"{CFG['artifact_prefix']}_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_model, f)\n",
    "with open(OUT / f\"{CFG['artifact_prefix']}_config.json\", \"w\") as f:\n",
    "    json.dump(CFG | {\"best_params\": best_cfg,\n",
    "                     \"metrics\": {\"roc_auc\": float(roc), \"pr_auc\": float(pr_auc), \"ap\": float(ap)}},\n",
    "              f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TOP-K: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_topk_1pct.parquet | rows: 277896\n",
      "@k=1.0% -> P:0.000 | R:0.000 | F1:0.000  (k=277896)\n",
      "Saved MMSI agg: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_mmsi_agg.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmsi</th>\n",
       "      <th>n_win</th>\n",
       "      <th>anom_win</th>\n",
       "      <th>anom_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>33266086194351</td>\n",
       "      <td>428760</td>\n",
       "      <td>34116</td>\n",
       "      <td>0.079569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>49534994750419</td>\n",
       "      <td>374280</td>\n",
       "      <td>18958</td>\n",
       "      <td>0.050652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>77832927010710</td>\n",
       "      <td>423960</td>\n",
       "      <td>20276</td>\n",
       "      <td>0.047825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>87919276942456</td>\n",
       "      <td>567520</td>\n",
       "      <td>25836</td>\n",
       "      <td>0.045524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>100771710683634</td>\n",
       "      <td>32440</td>\n",
       "      <td>1381</td>\n",
       "      <td>0.042571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12639560807591</td>\n",
       "      <td>23520</td>\n",
       "      <td>924</td>\n",
       "      <td>0.039286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>103576446797335</td>\n",
       "      <td>148920</td>\n",
       "      <td>5618</td>\n",
       "      <td>0.037725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>77182424306278</td>\n",
       "      <td>169660</td>\n",
       "      <td>5875</td>\n",
       "      <td>0.034628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23770783250938</td>\n",
       "      <td>473600</td>\n",
       "      <td>14403</td>\n",
       "      <td>0.030412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>95062718521348</td>\n",
       "      <td>169480</td>\n",
       "      <td>4468</td>\n",
       "      <td>0.026363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mmsi   n_win  anom_win  anom_rate\n",
       "12   33266086194351  428760     34116   0.079569\n",
       "19   49534994750419  374280     18958   0.050652\n",
       "28   77832927010710  423960     20276   0.047825\n",
       "33   87919276942456  567520     25836   0.045524\n",
       "40  100771710683634   32440      1381   0.042571\n",
       "1    12639560807591   23520       924   0.039286\n",
       "42  103576446797335  148920      5618   0.037725\n",
       "27   77182424306278  169660      5875   0.034628\n",
       "7    23770783250938  473600     14403   0.030412\n",
       "37   95062718521348  169480      4468   0.026363"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Top-K y agregados por MMSI\n",
    "import numpy as np, pandas as pd, os\n",
    "\n",
    "scores = np.memmap(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"),\n",
    "                   dtype=np.float32, mode=\"r\", shape=(X_eval.shape[0],))\n",
    "\n",
    "k_rate = 0.01\n",
    "k = max(1, int(len(scores) * k_rate))\n",
    "thr_k = np.partition(scores, -k)[-k]\n",
    "pred_topk = (scores >= thr_k).astype(np.int8)\n",
    "\n",
    "topk_idx = np.where(pred_topk == 1)[0]\n",
    "topk_df = pd.DataFrame({\n",
    "    \"idx\": topk_idx.astype(np.int64),\n",
    "    \"anomaly_score\": scores[topk_idx].astype(np.float32),\n",
    "    \"y_eval\": y_eval[topk_idx].astype(np.int8)\n",
    "})\n",
    "if 'groups_eval' in globals() and groups_eval is not None:\n",
    "    topk_df[\"mmsi\"] = groups_eval[topk_idx].astype(np.int64)\n",
    "\n",
    "topk_path = os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_topk_{int(k_rate*100)}pct.parquet\")\n",
    "topk_df.to_parquet(topk_path, index=False)\n",
    "print(\"Saved TOP-K:\", topk_path, \"| rows:\", len(topk_df))\n",
    "\n",
    "# MÃ©tricas @k\n",
    "yb = y_eval.astype(int)\n",
    "tp = int(((pred_topk==1) & (yb==1)).sum())\n",
    "fp = int(((pred_topk==1) & (yb==0)).sum())\n",
    "fn = int(((pred_topk==0) & (yb==1)).sum())\n",
    "prec_k = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "rec_k  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "f1_k   = 2*prec_k*rec_k/(prec_k+rec_k) if (prec_k+rec_k)>0 else 0.0\n",
    "print(f\"@k={k_rate*100:.1f}% -> P:{prec_k:.3f} | R:{rec_k:.3f} | F1:{f1_k:.3f}  (k={k})\")\n",
    "\n",
    "# Agregado por MMSI\n",
    "if 'groups_eval' in globals() and groups_eval is not None:\n",
    "    mmsi_all, n_by_mmsi = np.unique(groups_eval, return_counts=True)\n",
    "    mmsi_top, n_top_by_mmsi = np.unique(groups_eval[pred_topk==1], return_counts=True)\n",
    "    top_map = dict(zip(mmsi_top.tolist(), n_top_by_mmsi.tolist()))\n",
    "    anom_win = np.array([top_map.get(m, 0) for m in mmsi_all], dtype=np.int32)\n",
    "    anom_rate = anom_win / n_by_mmsi\n",
    "    agg_df = pd.DataFrame({\"mmsi\": mmsi_all, \"n_win\": n_by_mmsi, \"anom_win\": anom_win, \"anom_rate\": anom_rate})\n",
    "    agg_path = os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_mmsi_agg.parquet\")\n",
    "    agg_df.to_parquet(agg_path, index=False)\n",
    "    print(\"Saved MMSI agg:\", agg_path)\n",
    "    display(agg_df.sort_values(\"anom_rate\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š === MÃ©tricas finales OC-SVM ===\n",
      "ROC-AUC : nan\n",
      "PR-AUC  : nan\n",
      "AP Score: nan\n",
      "\n",
      "ðŸ”§ Mejor configuraciÃ³n encontrada:\n",
      "{\n",
      "  \"gamma\": 0.01,\n",
      "  \"nu\": 0.05\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_632cf\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_632cf_level0_col0\" class=\"col_heading level0 col0\" >Modelo</th>\n",
       "      <th id=\"T_632cf_level0_col1\" class=\"col_heading level0 col1\" >Kernel</th>\n",
       "      <th id=\"T_632cf_level0_col2\" class=\"col_heading level0 col2\" >nu</th>\n",
       "      <th id=\"T_632cf_level0_col3\" class=\"col_heading level0 col3\" >gamma</th>\n",
       "      <th id=\"T_632cf_level0_col4\" class=\"col_heading level0 col4\" >ROC-AUC</th>\n",
       "      <th id=\"T_632cf_level0_col5\" class=\"col_heading level0 col5\" >PR-AUC</th>\n",
       "      <th id=\"T_632cf_level0_col6\" class=\"col_heading level0 col6\" >AP</th>\n",
       "      <th id=\"T_632cf_level0_col7\" class=\"col_heading level0 col7\" >N ventanas (train)</th>\n",
       "      <th id=\"T_632cf_level0_col8\" class=\"col_heading level0 col8\" >N ventanas (eval)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_632cf_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_632cf_row0_col0\" class=\"data row0 col0\" >One-Class SVM (RBF)</td>\n",
       "      <td id=\"T_632cf_row0_col1\" class=\"data row0 col1\" >rbf</td>\n",
       "      <td id=\"T_632cf_row0_col2\" class=\"data row0 col2\" >0.050000</td>\n",
       "      <td id=\"T_632cf_row0_col3\" class=\"data row0 col3\" >0.010000</td>\n",
       "      <td id=\"T_632cf_row0_col4\" class=\"data row0 col4\" >nan</td>\n",
       "      <td id=\"T_632cf_row0_col5\" class=\"data row0 col5\" >nan</td>\n",
       "      <td id=\"T_632cf_row0_col6\" class=\"data row0 col6\" >nan</td>\n",
       "      <td id=\"T_632cf_row0_col7\" class=\"data row0 col7\" >800000</td>\n",
       "      <td id=\"T_632cf_row0_col8\" class=\"data row0 col8\" >400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f1f3ef876e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- MÃ©tricas finales resumen (para informe) ---\n",
    "import json, numpy as np, os, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"])\n",
    "cfg_path = OUT / f\"{CFG['artifact_prefix']}_config.json\"\n",
    "scores_path = OUT / f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"\n",
    "\n",
    "if not cfg_path.exists():\n",
    "    raise FileNotFoundError(\"No se encontrÃ³ el archivo de configuraciÃ³n con mÃ©tricas guardadas.\")\n",
    "\n",
    "# Cargar configuraciÃ³n y mÃ©tricas\n",
    "with open(cfg_path, \"r\") as f:\n",
    "    cfg_data = json.load(f)\n",
    "\n",
    "metrics = cfg_data.get(\"metrics\", {})\n",
    "roc_auc = metrics.get(\"roc_auc\", np.nan)\n",
    "pr_auc  = metrics.get(\"pr_auc\", np.nan)\n",
    "ap      = metrics.get(\"ap\", np.nan)\n",
    "best_params = cfg_data.get(\"best_params\", {})\n",
    "\n",
    "print(\"ðŸ“Š === MÃ©tricas finales OC-SVM ===\")\n",
    "print(f\"ROC-AUC : {roc_auc:.4f}\")\n",
    "print(f\"PR-AUC  : {pr_auc:.4f}\")\n",
    "print(f\"AP Score: {ap:.4f}\")\n",
    "print()\n",
    "print(\"ðŸ”§ Mejor configuraciÃ³n encontrada:\")\n",
    "print(json.dumps(best_params, indent=2))\n",
    "\n",
    "# Resumen para tabla del informe\n",
    "summary_df = pd.DataFrame([{\n",
    "    \"Modelo\": \"One-Class SVM (RBF)\",\n",
    "    \"Kernel\": cfg_data.get(\"kernel\", \"rbf\"),\n",
    "    \"nu\": best_params.get(\"nu\"),\n",
    "    \"gamma\": best_params.get(\"gamma\"),\n",
    "    \"ROC-AUC\": roc_auc,\n",
    "    \"PR-AUC\": pr_auc,\n",
    "    \"AP\": ap,\n",
    "    \"N ventanas (train)\": int(cfg_data.get(\"max_train_samples\", 0)),\n",
    "    \"N ventanas (eval)\": int(cfg_data.get(\"max_search_samples\", 0))\n",
    "}])\n",
    "display(summary_df.style.format({\"ROC-AUC\": \"{:.4f}\", \"PR-AUC\": \"{:.4f}\", \"AP\": \"{:.4f}\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (y \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m), info\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 1) Mapear etiquetas a binario (1=anÃ³malo)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m yb, map_info \u001b[38;5;241m=\u001b[39m map_labels_to_binary(\u001b[43my_eval\u001b[49m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistribuciÃ³n original:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39munique(y_eval, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistribuciÃ³n mapeada :\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39munique(yb, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_eval' is not defined"
     ]
    }
   ],
   "source": [
    "# --- DiagnÃ³stico de etiquetas y recÃ¡lculo de mÃ©tricas @k con mapeo robusto ---\n",
    "\n",
    "import numpy as np, pandas as pd, os, json\n",
    "from pathlib import Path\n",
    "\n",
    "def map_labels_to_binary(y):\n",
    "    \"\"\"Devuelve (yb, info) con yb en {0,1} donde 1=anÃ³malo.\"\"\"\n",
    "    vals = np.unique(y)\n",
    "    s = set(vals.tolist())\n",
    "    info = {\"original_values\": vals.tolist(), \"mapping\": None}\n",
    "    # Caso estÃ¡ndar\n",
    "    if s == {0, 1}:\n",
    "        info[\"mapping\"] = \"0=normal, 1=anÃ³malo (sin cambio)\"\n",
    "        return y.astype(int), info\n",
    "    # Muy comÃºn en detecciÃ³n: -1 anÃ³malo, +1 normal\n",
    "    if s == {-1, 1}:\n",
    "        info[\"mapping\"] = \"-1=anÃ³malo, +1=normal -> mapeado a {0,1}\"\n",
    "        return (y == -1).astype(int), info\n",
    "    # A veces hay {0,1,-1}; asumimos 1=anÃ³malo, 0=normal, -1=desconocido -> lo tratamos como 0 (conservador)\n",
    "    if s == {0, 1, -1}:\n",
    "        info[\"mapping\"] = \"1=anÃ³malo, 0=normal, -1=desconocido -> mapeado con -1->0\"\n",
    "        y2 = y.copy()\n",
    "        y2[y2 == -1] = 0\n",
    "        return y2.astype(int), info\n",
    "    # Fallback conservador: cualquier valor >0 lo consideramos anÃ³malo\n",
    "    info[\"mapping\"] = f\"Fallback: valores {sorted(s)} -> (y>0) como anÃ³malo\"\n",
    "    return (y > 0).astype(int), info\n",
    "\n",
    "# 1) Mapear etiquetas a binario (1=anÃ³malo)\n",
    "yb, map_info = map_labels_to_binary(y_eval)\n",
    "print(\"DistribuciÃ³n original:\", np.unique(y_eval, return_counts=True))\n",
    "print(\"DistribuciÃ³n mapeada :\", np.unique(yb, return_counts=True))\n",
    "print(\"Mapping usado:\", map_info[\"mapping\"])\n",
    "\n",
    "# 2) Recalcular mÃ©tricas @k con el mapeo correcto\n",
    "scores = np.memmap(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"),\n",
    "                   dtype=np.float32, mode=\"r\", shape=(X_eval.shape[0],))\n",
    "\n",
    "k_rate = 0.01\n",
    "k = max(1, int(len(scores) * k_rate))\n",
    "thr_k = np.partition(scores, -k)[-k]\n",
    "pred_topk = (scores >= thr_k).astype(np.int8)\n",
    "\n",
    "tp = int(((pred_topk==1) & (yb==1)).sum())\n",
    "fp = int(((pred_topk==1) & (yb==0)).sum())\n",
    "fn = int(((pred_topk==0) & (yb==1)).sum())\n",
    "prec_k = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "rec_k  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "f1_k   = 2*prec_k*rec_k/(prec_k+rec_k) if (prec_k+rec_k)>0 else 0.0\n",
    "\n",
    "print(f\"@k={k_rate*100:.1f}% -> P:{prec_k:.4f} | R:{rec_k:.4f} | F1:{f1_k:.4f}  (k={k})\")\n",
    "\n",
    "# 3) Guardar mÃ©tricas @k junto a las globales para el informe\n",
    "cfg_path = Path(CFG[\"out_dir\"]) / f\"{CFG['artifact_prefix']}_config.json\"\n",
    "if cfg_path.exists():\n",
    "    cfg = json.loads(cfg_path.read_text())\n",
    "else:\n",
    "    cfg = {\"metrics\": {}}\n",
    "cfg.setdefault(\"metrics_at_k\", {})[str(k_rate)] = {\n",
    "    \"k\": int(k),\n",
    "    \"precision\": float(prec_k),\n",
    "    \"recall\": float(rec_k),\n",
    "    \"f1\": float(f1_k),\n",
    "    \"label_mapping\": map_info[\"mapping\"],\n",
    "    \"label_values_original\": map_info[\"original_values\"],\n",
    "}\n",
    "cfg_path.write_text(json.dumps(cfg, indent=2))\n",
    "print(\"ðŸ“ MÃ©tricas @k aÃ±adidas a:\", cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores cargados: (27789660,)\n",
      "DistribuciÃ³n etiquetas mapeadas: {0: 27789660}\n",
      "ROC-AUC: nan | PR-AUC: nan | AP: nan\n",
      "@k=1.0% -> P:0.0000 | R:0.0000 | F1:0.0000  (k=277896)\n",
      "Mapping etiquetas: fallback>0 anÃ³malo (vals=[0])\n",
      "âœ… MÃ©tricas actualizadas en: data/ocsvm_runs/ocsvm_rbf_config.json\n"
     ]
    }
   ],
   "source": [
    "# === RECUPERAR MÃ‰TRICAS DESDE ARTEFACTOS (sin re-entrenar) ===\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "\n",
    "OUT = Path(\"data/ocsvm_runs\")\n",
    "cfg_path = OUT / \"ocsvm_rbf_config.json\"\n",
    "assert cfg_path.exists(), \"No encuentro data/ocsvm_runs/ocsvm_rbf_config.json\"\n",
    "\n",
    "# 1) Cargar CFG guardado y paths bÃ¡sicos\n",
    "CFG = json.loads(cfg_path.read_text())\n",
    "DATA_DIR = Path(CFG.get(\"external_data_dir\", \"/teamspace/studios/this_studio/data\")).resolve()\n",
    "scores_path = OUT / f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"\n",
    "assert scores_path.exists(), f\"No existe memmap de scores: {scores_path}\"\n",
    "\n",
    "# 2) Cargar scores desde memmap (inferimos N por tamaÃ±o)\n",
    "n_eval = scores_path.stat().st_size // 4  # float32\n",
    "scores = np.memmap(scores_path, dtype=np.float32, mode=\"r\", shape=(n_eval,))\n",
    "print(f\"Scores cargados: {scores.shape}\")\n",
    "\n",
    "# 3) Encontrar archivo(s) de etiquetas y leer SOLO la columna de label\n",
    "def first_valid_label_col(pqfile: pq.ParquetFile, prefer=(\"is_suspicious\",\"label\",\"y\",\"target\")):\n",
    "    cols = [name for name in pqfile.schema.names]\n",
    "    for c in prefer:\n",
    "        if c in cols: return c\n",
    "    # fallback: primera columna entera corta\n",
    "    for c in cols:\n",
    "        t = pqfile.schema.field(c).type\n",
    "        if str(t).startswith((\"int8\",\"int16\",\"int32\")): return c\n",
    "    # si no, Ãºltima columna\n",
    "    return cols[-1]\n",
    "\n",
    "def read_label_series(path: Path) -> pd.Series:\n",
    "    pf = pq.ParquetFile(path)\n",
    "    col = first_valid_label_col(pf)\n",
    "    tbl = pf.read(columns=[col])\n",
    "    s = tbl.to_pandas()[col]\n",
    "    return s\n",
    "\n",
    "# candidatos de eval\n",
    "single = DATA_DIR / \"windows_with_labels_aligned.parquet\"\n",
    "wl_norm = DATA_DIR / \"windows_with_labels_aligned_normal.parquet\"\n",
    "wl_anom = DATA_DIR / \"windows_with_labels_aligned_anom.parquet\"\n",
    "eval_norm = DATA_DIR / \"eval_windows_aligned_normal.parquet\"\n",
    "eval_anom = DATA_DIR / \"eval_windows_aligned_anom.parquet\"\n",
    "labels_a = DATA_DIR / \"eval_labels_aligned.parquet\"\n",
    "labels_b = DATA_DIR / \"labels.parquet\"\n",
    "\n",
    "y = None\n",
    "if single.exists():\n",
    "    y = read_label_series(single)\n",
    "elif wl_norm.exists() and wl_anom.exists():\n",
    "    y = pd.concat([read_label_series(wl_norm), read_label_series(wl_anom)], ignore_index=True)\n",
    "elif eval_norm.exists() and eval_anom.exists():\n",
    "    # etiquetas por archivo aparte\n",
    "    lab_path = labels_a if labels_a.exists() else (labels_b if labels_b.exists() else None)\n",
    "    assert lab_path is not None, \"No encontrÃ© archivo de labels para eval split.\"\n",
    "    y = read_label_series(lab_path)\n",
    "else:\n",
    "    # buscar genÃ©rico por patrÃ³n\n",
    "    pats = [\"*with_labels*aligned*.parquet\", \"*eval*labels*aligned*.parquet\", \"*labels*.parquet\"]\n",
    "    for patt in pats:\n",
    "        cands = sorted(DATA_DIR.glob(patt))\n",
    "        if cands:\n",
    "            y = read_label_series(cands[0]); break\n",
    "\n",
    "assert y is not None, f\"No encontrÃ© etiquetas en {DATA_DIR}\"\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "# 4) Alinear longitudes y mapear etiquetas a {0,1} (1 = anÃ³malo)\n",
    "assert len(y) == n_eval, f\"DesalineaciÃ³n: labels={len(y)} vs scores={n_eval}\"\n",
    "\n",
    "def map_labels_to_binary(y_arr):\n",
    "    vals = np.unique(y_arr)\n",
    "    s = set(vals.tolist())\n",
    "    if s == {0,1}:\n",
    "        return y_arr.astype(int), \"0=normal, 1=anÃ³malo\"\n",
    "    if s == {-1,1}:\n",
    "        return (y_arr == -1).astype(int), \"-1=anÃ³malo, +1=normal\"\n",
    "    if s == {0,1,-1}:\n",
    "        y2 = y_arr.copy(); y2[y2==-1]=0\n",
    "        return y2.astype(int), \"1=anÃ³malo, 0=normal, -1â†’0\"\n",
    "    # fallback: todo >0 es anÃ³malo\n",
    "    return (y_arr>0).astype(int), f\"fallback>0 anÃ³malo (vals={sorted(s)})\"\n",
    "\n",
    "yb, mapping_info = map_labels_to_binary(y.to_numpy())\n",
    "print(\"DistribuciÃ³n etiquetas mapeadas:\", dict(zip(*np.unique(yb, return_counts=True))))\n",
    "\n",
    "# 5) MÃ©tricas globales\n",
    "if len(np.unique(yb)) > 1:\n",
    "    roc = roc_auc_score(yb, scores)\n",
    "    prec, rec, _ = precision_recall_curve(yb, scores); pr = auc(rec, prec)\n",
    "    ap = average_precision_score(yb, scores)\n",
    "else:\n",
    "    roc = pr = ap = float(\"nan\")\n",
    "\n",
    "# 6) MÃ©tricas @k (1%)\n",
    "k_rate = 0.01\n",
    "k = max(1, int(n_eval * k_rate))\n",
    "thr = np.partition(scores, -k)[-k]\n",
    "pred_topk = (scores >= thr).astype(np.int8)\n",
    "\n",
    "tp = int(((pred_topk==1) & (yb==1)).sum())\n",
    "fp = int(((pred_topk==1) & (yb==0)).sum())\n",
    "fn = int(((pred_topk==0) & (yb==1)).sum())\n",
    "prec_k = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "rec_k  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "f1_k   = 2*prec_k*rec_k/(prec_k+rec_k) if (prec_k+rec_k)>0 else 0.0\n",
    "\n",
    "print(f\"ROC-AUC: {roc:.4f} | PR-AUC: {pr:.4f} | AP: {ap:.4f}\")\n",
    "print(f\"@k={k_rate*100:.1f}% -> P:{prec_k:.4f} | R:{rec_k:.4f} | F1:{f1_k:.4f}  (k={k})\")\n",
    "print(\"Mapping etiquetas:\", mapping_info)\n",
    "\n",
    "# 7) Persistir mÃ©tricas actualizadas al JSON (idempotente)\n",
    "cfg = CFG.copy()\n",
    "cfg[\"metrics\"] = {\"roc_auc\": float(roc), \"pr_auc\": float(pr), \"ap\": float(ap)}\n",
    "cfg.setdefault(\"metrics_at_k\", {})[str(k_rate)] = {\n",
    "    \"k\": int(k), \"precision\": float(prec_k), \"recall\": float(rec_k), \"f1\": float(f1_k),\n",
    "    \"label_mapping\": mapping_info\n",
    "}\n",
    "cfg_path.write_text(json.dumps(cfg, indent=2))\n",
    "print(\"âœ… MÃ©tricas actualizadas en:\", cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>candidates</th>\n",
       "      <th>n_rows_scanned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>windows_with_labels_aligned.parquet</td>\n",
       "      <td>[]</td>\n",
       "      <td>500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>windows_with_labels.parquet</td>\n",
       "      <td>[]</td>\n",
       "      <td>500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eval_windows_aligned.parquet</td>\n",
       "      <td>[]</td>\n",
       "      <td>500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eval_labels_aligned.parquet</td>\n",
       "      <td>[]</td>\n",
       "      <td>500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>labels.parquet</td>\n",
       "      <td>[]</td>\n",
       "      <td>500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>labels_anom.parquet</td>\n",
       "      <td>[]</td>\n",
       "      <td>131420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  file candidates  n_rows_scanned\n",
       "0  windows_with_labels_aligned.parquet         []          500000\n",
       "1          windows_with_labels.parquet         []          500000\n",
       "2         eval_windows_aligned.parquet         []          500000\n",
       "3          eval_labels_aligned.parquet         []          500000\n",
       "4                       labels.parquet         []          500000\n",
       "5                  labels_anom.parquet         []          131420"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‰ Elige (archivo, columna) con 0/1 o -1/1 (aunque sea float). Si nada aparece, usamos labels_anom como Ã­ndices.\n"
     ]
    }
   ],
   "source": [
    "# AuditorÃ­a de etiquetas en /data (incluye floats ~binarios)\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(CFG[\"external_data_dir\"])\n",
    "cands = [\n",
    "    \"windows_with_labels_aligned.parquet\",\n",
    "    \"windows_with_labels.parquet\",\n",
    "    \"eval_windows_aligned.parquet\",\n",
    "    \"eval_windows_aligned_normal.parquet\",\n",
    "    \"eval_windows_aligned_anom.parquet\",\n",
    "    \"eval_labels_aligned.parquet\",\n",
    "    \"labels.parquet\",\n",
    "    \"labels_anom.parquet\",\n",
    "]\n",
    "\n",
    "def try_counts(p: Path, max_rows=500_000, bin_tol=1e-6):\n",
    "    try:\n",
    "        pf = pq.ParquetFile(p)\n",
    "    except Exception as e:\n",
    "        return {\"file\": p.name, \"error\": f\"no parquet ({type(e).__name__})\"}\n",
    "    schema = pf.schema_arrow\n",
    "    cols = schema.names\n",
    "\n",
    "    labelish = []\n",
    "    for field in schema:\n",
    "        t = str(field.type).lower()\n",
    "        if any(x in t for x in [\"int8\",\"int16\",\"int32\",\"int64\",\"bool\",\"float16\",\"float32\",\"float64\"]):\n",
    "            labelish.append(field.name)\n",
    "\n",
    "    out = []\n",
    "    for c in labelish:\n",
    "        try:\n",
    "            tbl = pf.read(columns=[c], max_rows=max_rows)\n",
    "            s = pd.to_numeric(tbl.to_pandas()[c], errors=\"coerce\")\n",
    "            s = s.dropna()\n",
    "            if s.empty: \n",
    "                continue\n",
    "            # Â¿binario exacto?\n",
    "            uniq = np.unique(s.values)\n",
    "            uniq_set = set(np.round(uniq, 6))\n",
    "            is_binary_exact = uniq_set <= {0,1} or uniq_set <= {-1,1} or uniq_set <= {0,1,-1}\n",
    "            # Â¿casi binario? (tolerancia)\n",
    "            is_binary_close = np.all((np.abs(s - 0) < bin_tol) | (np.abs(s - 1) < bin_tol) | (np.abs(s + 1) < bin_tol))\n",
    "            if is_binary_exact or is_binary_close:\n",
    "                vc = s.value_counts().head(5).to_dict()\n",
    "                out.append((c, vc, str(schema.field(c).type)))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return {\n",
    "        \"file\": p.name,\n",
    "        \"candidates\": out,\n",
    "        \"n_rows_scanned\": int(min(max_rows, pf.metadata.num_rows))\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "for name in cands:\n",
    "    paths = []\n",
    "    if (DATA_DIR / name).exists():\n",
    "        paths.append(DATA_DIR / name)\n",
    "    else:\n",
    "        paths += sorted(DATA_DIR.glob(f\"*{name.replace('.parquet','')}*.parquet\"))\n",
    "    for p in paths:\n",
    "        rows.append(try_counts(p))\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "pd.set_option(\"display.max_colwidth\", 180)\n",
    "display(df.fillna(\"\"))\n",
    "print(\"ðŸ‘‰ Elige (archivo, columna) con 0/1 o -1/1 (aunque sea float). Si nada aparece, usamos labels_anom como Ã­ndices.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruido y desde labels_anom.parquet usando columna 'window_id' | positivos: 9120 / 27789660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC:0.1978 | PR-AUC:0.0004 | AP:0.0004\n",
      "@1% -> P:0.0032 | R:0.0967 | F1:0.0061 (k=277896)\n",
      "âœ… MÃ©tricas guardadas en: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_config.json\n"
     ]
    }
   ],
   "source": [
    "# Recalcular mÃ©tricas desde scores con (archivo,columna) o con labels_anom como Ã­ndices\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"])\n",
    "DATA_DIR = Path(CFG[\"external_data_dir\"])\n",
    "\n",
    "scores_path = OUT / f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"\n",
    "assert scores_path.exists(), \"No existe memmap de scores.\"\n",
    "n_eval = scores_path.stat().st_size // 4\n",
    "scores = np.memmap(scores_path, dtype=np.float32, mode=\"r\", shape=(n_eval,))\n",
    "\n",
    "# === 1) EDITA si ya identificaste archivo/columna de etiqueta binaria ===\n",
    "LABEL_FILE = None          # ej \"eval_labels_aligned.parquet\" o \"windows_with_labels_aligned.parquet\"\n",
    "LABEL_COL  = None          # ej \"is_suspicious\" (None = autodetectar)\n",
    "\n",
    "def read_binary_labels_from_file(path: Path, col: str|None):\n",
    "    pf = pq.ParquetFile(path)\n",
    "    if col is None:\n",
    "        # auto: preferidas luego cualquiera ~binaria\n",
    "        prefer = [\"is_suspicious\",\"label\",\"y\",\"target\"]\n",
    "        cols = pf.schema_arrow.names\n",
    "        for c in prefer:\n",
    "            if c in cols:\n",
    "                col = c; break\n",
    "        if col is None:\n",
    "            for f in pf.schema_arrow:\n",
    "                if str(f.type).lower() in (\"int8\",\"int16\",\"int32\",\"int64\",\"bool\",\"float16\",\"float32\",\"float64\"):\n",
    "                    col = f.name; break\n",
    "    tbl = pf.read(columns=[col])\n",
    "    y = pd.to_numeric(tbl.to_pandas()[col], errors=\"coerce\").fillna(0).to_numpy()\n",
    "    # mapear a {0,1}\n",
    "    vals = set(np.round(np.unique(y), 6).tolist())\n",
    "    if vals <= {0,1}:\n",
    "        yb = (np.abs(y - 1.0) < 1e-6).astype(int)\n",
    "        mapping = \"float/int 0/1 -> 1=anomalo\"\n",
    "    elif vals <= {-1,1} or vals <= {0,1,-1}:\n",
    "        yb = (np.abs(y + 1.0) < 1e-6).astype(int)  # -1 -> 1\n",
    "        mapping = \"float/int -1/1 -> -1=anomalo\"\n",
    "    else:\n",
    "        # fallback: >0 anÃ³malo\n",
    "        yb = (y > 0).astype(int)\n",
    "        mapping = f\"fallback (>0 anÃ³malo), vals={sorted(vals)}\"\n",
    "    return yb, mapping, col\n",
    "\n",
    "def metrics_from_y(scores, yb, k_rate=0.01):\n",
    "    if len(np.unique(yb)) > 1:\n",
    "        roc = roc_auc_score(yb, scores)\n",
    "        prec, rec, _ = precision_recall_curve(yb, scores); pr = auc(rec, prec)\n",
    "        ap = average_precision_score(yb, scores)\n",
    "    else:\n",
    "        roc = pr = ap = float(\"nan\")\n",
    "    k = max(1, int(len(scores) * k_rate))\n",
    "    thr = np.partition(scores, -k)[-k]\n",
    "    pred_topk = (scores >= thr).astype(np.int8)\n",
    "    tp = int(((pred_topk==1) & (yb==1)).sum())\n",
    "    fp = int(((pred_topk==1) & (yb==0)).sum())\n",
    "    fn = int(((pred_topk==0) & (yb==1)).sum())\n",
    "    prec_k = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "    rec_k  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "    f1_k   = 2*prec_k*rec_k/(prec_k+rec_k) if (prec_k+rec_k)>0 else 0.0\n",
    "    return roc, pr, ap, k, prec_k, rec_k, f1_k\n",
    "\n",
    "def persist_metrics(cfg_extra: dict, metrics, k_rate=0.01):\n",
    "    roc, pr, ap, k, pk, rk, f1k = metrics\n",
    "    cfg_path = OUT / f\"{CFG['artifact_prefix']}_config.json\"\n",
    "    cfg = json.loads(cfg_path.read_text()) if cfg_path.exists() else {}\n",
    "    cfg.update(cfg_extra)\n",
    "    cfg[\"metrics\"] = {\"roc_auc\": float(roc), \"pr_auc\": float(pr), \"ap\": float(ap)}\n",
    "    cfg.setdefault(\"metrics_at_k\", {})[str(k_rate)] = {\n",
    "        \"k\": int(k), \"precision\": float(pk), \"recall\": float(rk), \"f1\": float(f1k)\n",
    "    }\n",
    "    cfg_path.write_text(json.dumps(cfg, indent=2))\n",
    "    print(\"âœ… MÃ©tricas guardadas en:\", cfg_path)\n",
    "\n",
    "used = False\n",
    "if LABEL_FILE is not None:\n",
    "    lab_path = DATA_DIR / LABEL_FILE\n",
    "    assert lab_path.exists(), f\"No existe {lab_path}\"\n",
    "    yb, mapping, used_col = read_binary_labels_from_file(lab_path, LABEL_COL)\n",
    "    assert len(yb) == n_eval, f\"DesalineaciÃ³n: labels={len(yb)} vs scores={n_eval}\"\n",
    "    print(f\"Usando {LABEL_FILE} :: {used_col}  ({mapping})\")\n",
    "    M = metrics_from_y(scores, yb, k_rate=0.01)\n",
    "    print(f\"ROC-AUC:{M[0]:.4f} | PR-AUC:{M[1]:.4f} | AP:{M[2]:.4f}\")\n",
    "    print(f\"@1% -> P:{M[4]:.4f} | R:{M[5]:.4f} | F1:{M[6]:.4f} (k={M[3]})\")\n",
    "    persist_metrics({\"label_file\": LABEL_FILE, \"label_col\": used_col, \"label_mapping\": mapping}, M)\n",
    "    used = True\n",
    "\n",
    "# === 2) Si no se definiÃ³ LABEL_FILE o no trae positivos, intentar construir y desde labels_anom como ÃNDICES\n",
    "if not used:\n",
    "    la = None\n",
    "    for name in [\"labels_anom.parquet\", *list(DATA_DIR.glob(\"*labels_anom*.parquet\"))]:\n",
    "        p = name if isinstance(name, Path) else (DATA_DIR / name)\n",
    "        if Path(p).exists():\n",
    "            la = Path(p); break\n",
    "    assert la is not None, \"No encontrÃ© labels_anom.parquet para reconstrucciÃ³n por Ã­ndices.\"\n",
    "\n",
    "    pf = pq.ParquetFile(la)\n",
    "    cols = pf.schema_arrow.names\n",
    "    # heurÃ­stica: elegir columna Ã­ndice\n",
    "    key_candidates = [c for c in [\"idx\",\"window_id\",\"idx_end\",\"row\",\"row_id\"] if c in cols]\n",
    "    if not key_candidates:\n",
    "        # si solo hay una columna numÃ©rica, usarla como Ã­ndices\n",
    "        num_cols = [f.name for f in pf.schema_arrow if \"int\" in str(f.type).lower()]\n",
    "        assert num_cols, f\"No hallÃ© columnas numÃ©ricas en {la}\"\n",
    "        key = num_cols[0]\n",
    "    else:\n",
    "        key = key_candidates[0]\n",
    "\n",
    "    s = pf.read(columns=[key]).to_pandas()[key]\n",
    "    idx = pd.to_numeric(s, errors=\"coerce\").dropna().astype(int).to_numpy()\n",
    "\n",
    "    # Ajuste 0/1-based: si el mÃ¡ximo es == n_eval y el mÃ­nimo es 1, usamos 1-based\n",
    "    if idx.min() >= 1 and idx.max() <= n_eval and (1 in idx):\n",
    "        idx0 = idx - 1\n",
    "    else:\n",
    "        idx0 = idx\n",
    "    idx0 = idx0[(idx0 >= 0) & (idx0 < n_eval)]\n",
    "\n",
    "    yb = np.zeros(n_eval, dtype=int)\n",
    "    yb[idx0] = 1\n",
    "    print(f\"Reconstruido y desde {la.name} usando columna '{key}' | positivos: {yb.sum()} / {n_eval}\")\n",
    "\n",
    "    M = metrics_from_y(scores, yb, k_rate=0.01)\n",
    "    print(f\"ROC-AUC:{M[0]:.4f} | PR-AUC:{M[1]:.4f} | AP:{M[2]:.4f}\")\n",
    "    print(f\"@1% -> P:{M[4]:.4f} | R:{M[5]:.4f} | F1:{M[6]:.4f} (k={M[3]})\")\n",
    "    persist_metrics({\"label_file\": la.name, \"label_col\": key, \"label_mapping\": \"indices (1-based auto-ajustado)\"}, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: (27789660,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas construidas: positivos=1003200 / 27789660  | fuente=JOIN on window_id from labels_anom.parquet\n",
      "Scores tal cual   -> ROC:0.5069 | PR-AUC:0.0389 | AP:0.0367\n",
      "Scores invertidos -> ROC:0.4931 | PR-AUC:0.0357 | AP:0.0369\n",
      "ðŸ‘‰ Usa la versiÃ³n con mejores mÃ©tricas.\n"
     ]
    }
   ],
   "source": [
    "# --- Test de polaridad (auto, sin depender de X_eval) ---\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "\n",
    "# 0) Cargar CFG desde JSON si no estÃ¡ en memoria\n",
    "if 'CFG' not in globals():\n",
    "    cfg_path_guess = Path(\"data/ocsvm_runs/ocsvm_rbf_config.json\")\n",
    "    assert cfg_path_guess.exists(), \"No encontrÃ© data/ocsvm_runs/ocsvm_rbf_config.json\"\n",
    "    CFG = json.loads(cfg_path_guess.read_text())\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"])\n",
    "DATA_DIR = Path(CFG.get(\"external_data_dir\", \"/teamspace/studios/this_studio/data\"))\n",
    "scores_path = OUT / f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"\n",
    "assert scores_path.exists(), f\"No existe memmap de scores: {scores_path}\"\n",
    "\n",
    "# 1) Cargar scores y n_eval a partir del tamaÃ±o de archivo\n",
    "n_eval = scores_path.stat().st_size // 4  # float32\n",
    "scores = np.memmap(scores_path, dtype=np.float32, mode=\"r\", shape=(n_eval,))\n",
    "print(\"Scores:\", scores.shape)\n",
    "\n",
    "# -------- Helpers --------\n",
    "def metrics_from(scores, yb):\n",
    "    if len(np.unique(yb)) > 1:\n",
    "        roc = roc_auc_score(yb, scores)\n",
    "        prec, rec, _ = precision_recall_curve(yb, scores); pr_auc = auc(rec, prec)\n",
    "        ap = average_precision_score(yb, scores)\n",
    "    else:\n",
    "        roc = pr_auc = ap = float('nan')\n",
    "    return roc, pr_auc, ap\n",
    "\n",
    "def build_y_from_label_file(label_file:str, label_col:str|None):\n",
    "    p = DATA_DIR / label_file\n",
    "    assert p.exists(), f\"No existe {p}\"\n",
    "    pf = pq.ParquetFile(p)\n",
    "    col = label_col\n",
    "    if col is None:\n",
    "        # autodetectar columna \"binaria\" (int/bool/float 0/1/-1)\n",
    "        pref = [\"is_suspicious\",\"label\",\"y\",\"target\"]\n",
    "        cols = pf.schema_arrow.names\n",
    "        for c in pref:\n",
    "            if c in cols: col = c; break\n",
    "        if col is None:\n",
    "            # como fallback, primera numÃ©rica\n",
    "            for f in pf.schema_arrow:\n",
    "                t = str(f.type).lower()\n",
    "                if any(x in t for x in [\"int\",\"bool\",\"float\"]):\n",
    "                    col = f.name; break\n",
    "    tbl = pf.read(columns=[col])\n",
    "    y = pd.to_numeric(tbl.to_pandas()[col], errors=\"coerce\").fillna(0).to_numpy()\n",
    "    assert len(y) == n_eval, f\"DesalineaciÃ³n labels={len(y)} vs scores={n_eval}\"\n",
    "    vals = set(np.round(np.unique(y),6).tolist())\n",
    "    if vals <= {0,1}:\n",
    "        yb = (np.abs(y - 1.0) < 1e-6).astype(int); mapping = \"0/1 -> 1=anÃ³malo\"\n",
    "    elif vals <= {-1,1} or vals <= {0,1,-1}:\n",
    "        yb = (np.abs(y + 1.0) < 1e-6).astype(int); mapping = \"-1/1 -> -1=anÃ³malo\"\n",
    "    else:\n",
    "        yb = (y > 0).astype(int); mapping = f\"fallback (>0 anÃ³malo), vals={sorted(vals)}\"\n",
    "    return yb, mapping\n",
    "\n",
    "def build_y_from_labels_anom_join():\n",
    "    # localizar parquet de eval para tomar el ORDEN real\n",
    "    eval_candidates = [\n",
    "        \"windows_with_labels_aligned.parquet\",\n",
    "        \"eval_windows_aligned.parquet\",\n",
    "        \"windows_with_labels.parquet\",\n",
    "        \"windows_with_labels_aligned_normal.parquet\",  # si es split, al menos nos da el orden de una mitad\n",
    "    ]\n",
    "    EVAL_PATH = None\n",
    "    for name in eval_candidates:\n",
    "        p = DATA_DIR / name\n",
    "        if p.exists(): EVAL_PATH = p; break\n",
    "    if EVAL_PATH is None:\n",
    "        cands = sorted(DATA_DIR.glob(\"*windows*aligned*.parquet\"),\n",
    "                       key=lambda x: x.stat().st_size if x.exists() else 0, reverse=True)\n",
    "        assert cands, \"No hallÃ© parquet de EVAL para inferir orden\"\n",
    "        EVAL_PATH = cands[0]\n",
    "\n",
    "    pf_eval = pq.ParquetFile(EVAL_PATH)\n",
    "    key_eval = next((k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in pf_eval.schema_arrow.names), None)\n",
    "    assert key_eval is not None, f\"No encontrÃ© columna clave en {EVAL_PATH.name}\"\n",
    "    eval_key = pf_eval.read(columns=[key_eval]).to_pandas()[key_eval].astype(np.int64).reset_index(drop=True)\n",
    "    assert len(eval_key) == n_eval, f\"DesalineaciÃ³n eval_key={len(eval_key)} vs scores={n_eval}\"\n",
    "\n",
    "    # cargar labels_anom\n",
    "    lab_anom = None\n",
    "    for p in [DATA_DIR / \"labels_anom.parquet\", *DATA_DIR.glob(\"*labels_anom*.parquet\")]:\n",
    "        if p.exists(): lab_anom = p; break\n",
    "    assert lab_anom is not None, \"No encontrÃ© labels_anom.parquet\"\n",
    "    pf_lab = pq.ParquetFile(lab_anom)\n",
    "    key_lab = key_eval if key_eval in pf_lab.schema_arrow.names else (\n",
    "        next((k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in pf_lab.schema_arrow.names), None)\n",
    "    )\n",
    "    assert key_lab is not None, f\"labels_anom no tiene clave compatible (busquÃ© window_id/idx/idx_end/row/row_id)\"\n",
    "    anom_keys = pf_lab.read(columns=[key_lab]).to_pandas()[key_lab].astype(np.int64).to_numpy()\n",
    "    anom_set = set(anom_keys.tolist())\n",
    "    yb = eval_key.isin(anom_set).astype(int).to_numpy()\n",
    "    return yb, f\"JOIN on {key_eval} from {lab_anom.name}\"\n",
    "\n",
    "# 2) Construir yb segÃºn lo que tengamos en config\n",
    "cfg_path = OUT / f\"{CFG['artifact_prefix']}_config.json\"\n",
    "cfg_json = json.loads(cfg_path.read_text()) if cfg_path.exists() else {}\n",
    "\n",
    "yb = None; mapping_src = None\n",
    "lab_file = cfg_json.get(\"label_file\")\n",
    "lab_col  = cfg_json.get(\"label_col\")\n",
    "lab_mapping_hint = cfg_json.get(\"label_mapping\", \"\")\n",
    "\n",
    "try:\n",
    "    if lab_file and lab_file != \"labels_anom.parquet\":\n",
    "        # usar archivo+columna si se registrÃ³ en config\n",
    "        yb, mapping_src = build_y_from_label_file(lab_file, lab_col)\n",
    "    else:\n",
    "        # por defecto (o si era labels_anom), usar JOIN por clave\n",
    "        yb, mapping_src = build_y_from_labels_anom_join()\n",
    "except Exception as e:\n",
    "    # fallback: intentar archivo estÃ¡ndar de labels\n",
    "    for candidate in [\"windows_with_labels_aligned.parquet\", \"eval_labels_aligned.parquet\"]:\n",
    "        p = DATA_DIR / candidate\n",
    "        if p.exists():\n",
    "            yb, mapping_src = build_y_from_label_file(candidate, None)\n",
    "            break\n",
    "    if yb is None:\n",
    "        # Ãºltimo recurso: indices desde labels_anom como posicional (puede desalinear)\n",
    "        la = DATA_DIR / \"labels_anom.parquet\"\n",
    "        assert la.exists(), \"No encontrÃ© labels_anom.parquet para fallback.\"\n",
    "        pf = pq.ParquetFile(la)\n",
    "        key = next((k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in pf.schema_arrow.names), pf.schema_arrow.names[0])\n",
    "        idx = pf.read(columns=[key]).to_pandas()[key].astype(int).to_numpy()\n",
    "        if idx.min()>=1 and idx.max()<=n_eval: idx = idx - 1\n",
    "        idx = idx[(idx>=0)&(idx<n_eval)]\n",
    "        yb = np.zeros(n_eval, dtype=int); yb[idx]=1\n",
    "        mapping_src = f\"indices from {la.name} (positional fallback)\"\n",
    "\n",
    "print(f\"Etiquetas construidas: positivos={yb.sum()} / {len(yb)}  | fuente={mapping_src}\")\n",
    "\n",
    "# 3) MÃ©tricas con scores directos e invertidos\n",
    "roc1, pr1, ap1 = metrics_from(scores, yb)\n",
    "roc2, pr2, ap2 = metrics_from(-scores, yb)\n",
    "\n",
    "print(f\"Scores tal cual   -> ROC:{roc1:.4f} | PR-AUC:{pr1:.4f} | AP:{ap1:.4f}\")\n",
    "print(f\"Scores invertidos -> ROC:{roc2:.4f} | PR-AUC:{pr2:.4f} | AP:{ap2:.4f}\")\n",
    "print(\"ðŸ‘‰ Usa la versiÃ³n con mejores mÃ©tricas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positivos (JOIN): 1003200 / 27789660\n",
      "ROC-AUC:0.5069 | PR-AUC:0.0389 | AP:0.0367\n",
      "@1% -> P:0.0423 | R:0.0117 | F1:0.0183 (k=277896)\n",
      "âœ… Actualizado: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_config.json\n"
     ]
    }
   ],
   "source": [
    "# === Persistir mÃ©tricas finales (polarity correcta + @k) ===\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"])\n",
    "DATA_DIR = Path(CFG[\"external_data_dir\"])\n",
    "scores_path = OUT / f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"\n",
    "cfg_path = OUT / f\"{CFG['artifact_prefix']}_config.json\"\n",
    "assert scores_path.exists(), \"No hay scores memmap.\"\n",
    "\n",
    "# 1) Cargar scores y construir y por JOIN con window_id (lo que ya validaste)\n",
    "n_eval = scores_path.stat().st_size // 4\n",
    "scores = np.memmap(scores_path, dtype=np.float32, mode=\"r\", shape=(n_eval,))\n",
    "\n",
    "# encontrar parquet de eval para tomar el ORDEN exacto\n",
    "eval_candidates = [\n",
    "    \"windows_with_labels_aligned.parquet\",\n",
    "    \"eval_windows_aligned.parquet\",\n",
    "    \"windows_with_labels.parquet\",\n",
    "    \"windows_with_labels_aligned_normal.parquet\",\n",
    "]\n",
    "EVAL_PATH = None\n",
    "for name in eval_candidates:\n",
    "    p = DATA_DIR / name\n",
    "    if p.exists():\n",
    "        EVAL_PATH = p; break\n",
    "if EVAL_PATH is None:\n",
    "    cands = sorted(DATA_DIR.glob(\"*windows*aligned*.parquet\"), key=lambda x: x.stat().st_size if x.exists() else 0, reverse=True)\n",
    "    assert cands, \"No encontrÃ© parquet de EVAL para inferir orden.\"\n",
    "    EVAL_PATH = cands[0]\n",
    "\n",
    "pf_eval = pq.ParquetFile(EVAL_PATH)\n",
    "key_eval = next((k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in pf_eval.schema_arrow.names), None)\n",
    "assert key_eval is not None, f\"{EVAL_PATH.name} no tiene columna clave esperada.\"\n",
    "eval_key = pf_eval.read(columns=[key_eval]).to_pandas()[key_eval].astype(np.int64).reset_index(drop=True)\n",
    "assert len(eval_key) == n_eval, f\"DesalineaciÃ³n: eval_key={len(eval_key)} vs scores={n_eval}\"\n",
    "\n",
    "# labels_anom como conjunto de claves\n",
    "lab_anom = None\n",
    "for p in [DATA_DIR / \"labels_anom.parquet\", *DATA_DIR.glob(\"*labels_anom*.parquet\")]:\n",
    "    if p.exists(): lab_anom = p; break\n",
    "assert lab_anom is not None, \"No hallÃ© labels_anom.parquet.\"\n",
    "pf_la = pq.ParquetFile(lab_anom)\n",
    "key_lab = key_eval if key_eval in pf_la.schema_arrow.names else (\n",
    "    next((k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in pf_la.schema_arrow.names), None)\n",
    ")\n",
    "assert key_lab is not None, \"labels_anom no tiene una clave compatible.\"\n",
    "anom_keys = pf_la.read(columns=[key_lab]).to_pandas()[key_lab].astype(np.int64).to_numpy()\n",
    "anom_set = set(anom_keys.tolist())\n",
    "\n",
    "yb = eval_key.isin(anom_set).astype(int).to_numpy()\n",
    "print(\"Positivos (JOIN):\", yb.sum(), \"/\", len(yb))\n",
    "\n",
    "# 2) MÃ©tricas con scores tal cual (polarity validada)\n",
    "def metrics(scores, yb, k_rate=0.01):\n",
    "    if len(np.unique(yb)) > 1:\n",
    "        roc = roc_auc_score(yb, scores)\n",
    "        prec, rec, _ = precision_recall_curve(yb, scores); pr_auc = auc(rec, prec)\n",
    "        ap = average_precision_score(yb, scores)\n",
    "    else:\n",
    "        roc = pr_auc = ap = float('nan')\n",
    "    k = max(1, int(len(scores) * k_rate))\n",
    "    thr = np.partition(scores, -k)[-k]\n",
    "    pred_topk = (scores >= thr).astype(np.int8)\n",
    "    tp = int(((pred_topk==1) & (yb==1)).sum())\n",
    "    fp = int(((pred_topk==1) & (yb==0)).sum())\n",
    "    fn = int(((pred_topk==0) & (yb==1)).sum())\n",
    "    prec_k = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "    rec_k  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "    f1_k   = 2*prec_k*rec_k/(prec_k+rec_k) if (prec_k+rec_k)>0 else 0.0\n",
    "    return {\"roc_auc\": float(roc), \"pr_auc\": float(pr_auc), \"ap\": float(ap),\n",
    "            \"k\": int(k), \"precision_k\": float(prec_k), \"recall_k\": float(rec_k), \"f1_k\": float(f1_k)}\n",
    "\n",
    "M = metrics(scores, yb, k_rate=0.01)\n",
    "print(f\"ROC-AUC:{M['roc_auc']:.4f} | PR-AUC:{M['pr_auc']:.4f} | AP:{M['ap']:.4f}\")\n",
    "print(f\"@1% -> P:{M['precision_k']:.4f} | R:{M['recall_k']:.4f} | F1:{M['f1_k']:.4f} (k={M['k']})\")\n",
    "\n",
    "# 3) Guardar en config.json (idempotente)\n",
    "cfg = json.loads(cfg_path.read_text()) if cfg_path.exists() else {}\n",
    "cfg[\"external_data_dir\"] = CFG[\"external_data_dir\"]\n",
    "cfg[\"kernel\"] = CFG.get(\"kernel\",\"rbf\")\n",
    "cfg[\"best_params\"] = cfg.get(\"best_params\", {})\n",
    "cfg[\"metrics\"] = {\"roc_auc\": M[\"roc_auc\"], \"pr_auc\": M[\"pr_auc\"], \"ap\": M[\"ap\"]}\n",
    "cfg.setdefault(\"metrics_at_k\", {})[\"0.01\"] = {\n",
    "    \"k\": M[\"k\"], \"precision\": M[\"precision_k\"], \"recall\": M[\"recall_k\"], \"f1\": M[\"f1_k\"]\n",
    "}\n",
    "cfg.update({\"label_file\": lab_anom.name, \"label_col\": key_lab, \"label_mapping\": f\"JOIN on {key_eval}\"})\n",
    "cfg_path.write_text(json.dumps(cfg, indent=2))\n",
    "print(\"âœ… Actualizado:\", cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f143c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f143c_level0_col0\" class=\"col_heading level0 col0\" >Modelo</th>\n",
       "      <th id=\"T_f143c_level0_col1\" class=\"col_heading level0 col1\" >nu</th>\n",
       "      <th id=\"T_f143c_level0_col2\" class=\"col_heading level0 col2\" >gamma</th>\n",
       "      <th id=\"T_f143c_level0_col3\" class=\"col_heading level0 col3\" >ROC-AUC</th>\n",
       "      <th id=\"T_f143c_level0_col4\" class=\"col_heading level0 col4\" >PR-AUC</th>\n",
       "      <th id=\"T_f143c_level0_col5\" class=\"col_heading level0 col5\" >AP</th>\n",
       "      <th id=\"T_f143c_level0_col6\" class=\"col_heading level0 col6\" >P@1%</th>\n",
       "      <th id=\"T_f143c_level0_col7\" class=\"col_heading level0 col7\" >R@1%</th>\n",
       "      <th id=\"T_f143c_level0_col8\" class=\"col_heading level0 col8\" >F1@1%</th>\n",
       "      <th id=\"T_f143c_level0_col9\" class=\"col_heading level0 col9\" >Labels fuente</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f143c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f143c_row0_col0\" class=\"data row0 col0\" >One-Class SVM (RBF)</td>\n",
       "      <td id=\"T_f143c_row0_col1\" class=\"data row0 col1\" >0.050000</td>\n",
       "      <td id=\"T_f143c_row0_col2\" class=\"data row0 col2\" >0.010000</td>\n",
       "      <td id=\"T_f143c_row0_col3\" class=\"data row0 col3\" >0.5069</td>\n",
       "      <td id=\"T_f143c_row0_col4\" class=\"data row0 col4\" >0.0389</td>\n",
       "      <td id=\"T_f143c_row0_col5\" class=\"data row0 col5\" >0.0367</td>\n",
       "      <td id=\"T_f143c_row0_col6\" class=\"data row0 col6\" >0.0423</td>\n",
       "      <td id=\"T_f143c_row0_col7\" class=\"data row0 col7\" >0.0117</td>\n",
       "      <td id=\"T_f143c_row0_col8\" class=\"data row0 col8\" >0.0183</td>\n",
       "      <td id=\"T_f143c_row0_col9\" class=\"data row0 col9\" >labels_anom.parquet Â· window_id (JOIN on window_id)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f1559798aa0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Resumen final (listo para el informe) ===\n",
    "import json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "cfg_path = Path(CFG[\"out_dir\"]) / f\"{CFG['artifact_prefix']}_config.json\"\n",
    "cfg = json.loads(cfg_path.read_text())\n",
    "\n",
    "metrics = cfg.get(\"metrics\", {})\n",
    "metrics_k = cfg.get(\"metrics_at_k\", {}).get(\"0.01\", {})\n",
    "best_params = cfg.get(\"best_params\", {})\n",
    "\n",
    "row = {\n",
    "    \"Modelo\": \"One-Class SVM (RBF)\",\n",
    "    \"nu\": best_params.get(\"nu\", CFG.get(\"svm_nu_grid\",[None])[0]),\n",
    "    \"gamma\": best_params.get(\"gamma\", CFG.get(\"svm_gamma_grid\",[None])[0]),\n",
    "    \"ROC-AUC\": metrics.get(\"roc_auc\"),\n",
    "    \"PR-AUC\": metrics.get(\"pr_auc\"),\n",
    "    \"AP\": metrics.get(\"ap\"),\n",
    "    \"P@1%\": metrics_k.get(\"precision\"),\n",
    "    \"R@1%\": metrics_k.get(\"recall\"),\n",
    "    \"F1@1%\": metrics_k.get(\"f1\"),\n",
    "    \"Labels fuente\": f\"{cfg.get('label_file','?')} Â· {cfg.get('label_col','?')} ({cfg.get('label_mapping','?')})\",\n",
    "}\n",
    "\n",
    "df = pd.DataFrame([row])\n",
    "display(df.style.format({\n",
    "    \"ROC-AUC\":\"{:.4f}\", \"PR-AUC\":\"{:.4f}\", \"AP\":\"{:.4f}\",\n",
    "    \"P@1%\":\"{:.4f}\", \"R@1%\":\"{:.4f}\", \"F1@1%\":\"{:.4f}\"\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlen\u001b[39m(scores), \u001b[38;5;241m20000\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m X2d \u001b[38;5;241m=\u001b[39m PCA(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX_eval\u001b[49m[idx])\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(X2d[:,\u001b[38;5;241m0\u001b[39m], X2d[:,\u001b[38;5;241m1\u001b[39m], c\u001b[38;5;241m=\u001b[39myb[idx], cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoolwarm\u001b[39m\u001b[38;5;124m'\u001b[39m, s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistribuciÃ³n de anomalÃ­as (labels_anom)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_eval' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "idx = np.random.choice(len(scores), 20000, replace=False)\n",
    "X2d = PCA(2).fit_transform(X_eval[idx])\n",
    "plt.scatter(X2d[:,0], X2d[:,1], c=yb[idx], cmap='coolwarm', s=2)\n",
    "plt.title(\"DistribuciÃ³n de anomalÃ­as (labels_anom)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
