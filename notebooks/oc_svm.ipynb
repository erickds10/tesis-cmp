{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Class SVM (OC-SVM) — AIS Anomaly Detection (Galápagos)\n",
    "\n",
    "**Objetivo:** Entrenar y evaluar un OC-SVM (RBF) usando **ventanas pre-generadas** del workspace externo (solo lectura):  \n",
    "`/teamspace/studios/profound-silver-kn8tf/ais_anomaly/data`\n",
    "\n",
    "**Principios clave**\n",
    "- **Nunca** escribimos en el workspace externo.  \n",
    "- Todos los artefactos nuevos del SVM se guardan en **`./data/ocsvm_runs`** del proyecto actual.  \n",
    "- Pipeline **memory-safe**: muestreo por MMSI para train, imputación/escala sobre muestras, evaluación por **lotes** con **memmap**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OC-SVM AIS Anomaly Detection — robusto y memory-safe (Lee de /data)\n",
    "\n",
    "- Lee **solo** desde `/data` (este Studio).\n",
    "- Artefactos nuevos del SVM se guardan en **`./data/ocsvm_runs`** (no se toca `/data`).\n",
    "- Evaluación por **lotes** con **memmap** para evitar picos de RAM (~30 GB límite)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:    erickdsuarez10\n",
      "Host:    computeinstance-e00exnkvr257g0k5f5\n",
      "Python:  /home/zeus/miniconda3/envs/cloudspace/bin/python\n",
      "CWD:     /teamspace/studios/this_studio\n",
      "/data exists?: False\n"
     ]
    }
   ],
   "source": [
    "# --- Diagnóstico del entorno ---\n",
    "import os, sys, getpass, socket\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"User:   \", getpass.getuser())\n",
    "print(\"Host:   \", socket.gethostname())\n",
    "print(\"Python: \", sys.executable)\n",
    "print(\"CWD:    \", os.getcwd())\n",
    "print(\"/data exists?:\", Path(\"/data\").exists())\n",
    "if Path(\"/data\").exists():\n",
    "    print(\"#parquets en /data:\", len(list(Path(\"/data\").glob(\"*.parquet\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LECTURA (read-only): /teamspace/studios/this_studio/data\n",
      "SALIDAS (local):     /teamspace/studios/this_studio/data/ocsvm_runs\n",
      "#parquets detectados en lectura: 17\n",
      "Config: {\n",
      "  \"external_data_dir\": \"/teamspace/studios/this_studio/data\",\n",
      "  \"out_dir\": \"/teamspace/studios/this_studio/data/ocsvm_runs\",\n",
      "  \"artifact_prefix\": \"ocsvm_rbf\",\n",
      "  \"svm_nu_grid\": [\n",
      "    0.01,\n",
      "    0.05,\n",
      "    0.1\n",
      "  ],\n",
      "  \"svm_gamma_grid\": [\n",
      "    \"scale\",\n",
      "    0.01\n",
      "  ],\n",
      "  \"kernel\": \"rbf\",\n",
      "  \"kfold_splits\": 5,\n",
      "  \"max_train_samples\": 500000,\n",
      "  \"max_search_samples\": 200000,\n",
      "  \"eval_batch_size\": 200000\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- Setup & Config (auto-resuelve la fuente de lectura) ---\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "\n",
    "# Candidatas en orden de preferencia\n",
    "CANDIDATES = [\n",
    "    Path(\"/data\").resolve(),                               # algunos entornos montan /data\n",
    "    Path(\"/teamspace/studios/this_studio/data\").resolve(), # data propia de este Studio\n",
    "    Path(\"./data\").resolve(),                              # por si copiamos aquí\n",
    "]\n",
    "\n",
    "def count_parquets(p: Path) -> int:\n",
    "    try:\n",
    "        return len(list(p.glob(\"*.parquet\"))) if p.exists() else 0\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "external = None\n",
    "for cand in CANDIDATES:\n",
    "    if cand.exists() and count_parquets(cand) > 0:\n",
    "        external = cand\n",
    "        break\n",
    "\n",
    "if external is None:\n",
    "    # No hay /data montado ni parquets en las otras candidatas\n",
    "    # Usa explícitamente la de este Studio (existe, aunque pueda tener 0 parquets)\n",
    "    external = Path(\"/teamspace/studios/this_studio/data\").resolve()\n",
    "    print(\"⚠️ No se hallaron parquets aún. Usando ruta por defecto del Studio:\", external)\n",
    "\n",
    "EXTERNAL_DATA_DIR = external\n",
    "\n",
    "# Carpeta local para salidas del OC-SVM (seguro)\n",
    "OUT_DIR = Path(\"data/ocsvm_runs\").resolve()\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "CFG = {\n",
    "    \"external_data_dir\": str(EXTERNAL_DATA_DIR),\n",
    "    \"out_dir\": str(OUT_DIR),\n",
    "    \"artifact_prefix\": \"ocsvm_rbf\",\n",
    "    \"svm_nu_grid\": [0.01, 0.05, 0.1],\n",
    "    \"svm_gamma_grid\": [\"scale\", 0.01],\n",
    "    \"kernel\": \"rbf\",\n",
    "    \"kfold_splits\": 5,\n",
    "    \"max_train_samples\": 500_000,\n",
    "    \"max_search_samples\": 200_000,\n",
    "    \"eval_batch_size\": 200_000,\n",
    "}\n",
    "\n",
    "print(\"LECTURA (read-only):\", CFG[\"external_data_dir\"])\n",
    "print(\"SALIDAS (local):    \", CFG[\"out_dir\"])\n",
    "print(\"#parquets detectados en lectura:\", count_parquets(Path(CFG[\"external_data_dir\"])))\n",
    "print(\"Config:\", json.dumps({k:v for k,v in CFG.items() if k not in []}, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: /teamspace/studios/this_studio/data\n",
      "Parquets disponibles:\n",
      " - ais_anom_enriched.parquet  (13.4 MB)\n",
      " - ais_anom_windows.parquet  (8.2 MB)\n",
      " - ais_norm_enriched.parquet  (2199.8 MB)\n",
      " - ais_norm_windows.parquet  (1264.8 MB)\n",
      " - anom_windows_flat.parquet  (8.2 MB)\n",
      " - eval_labels_aligned.parquet  (35.7 MB)\n",
      " - eval_windows_aligned.parquet  (399.1 MB)\n",
      " - eval_windows_aligned_norm.parquet  (15.8 MB)\n",
      " - labels.parquet  (118.7 MB)\n",
      " - labels_anom.parquet  (0.1 MB)\n",
      " - norm_windows_flat.parquet  (934.6 MB)\n",
      " - windows.parquet  (859.8 MB)\n",
      " - windows_aligned_anom.parquet  (8.2 MB)\n",
      " - windows_aligned_normal.parquet  (1268.4 MB)\n",
      " - windows_with_labels.parquet  (859.8 MB)\n",
      " - windows_with_labels_aligned.parquet  (1268.4 MB)\n",
      " - windows_with_labels_aligned_norm.parquet  (766.1 MB)\n",
      "TRAIN -> windows_aligned_normal.parquet | X_train: (27789660, 19)\n",
      "EVAL -> windows_with_labels_aligned.parquet | X_eval: (27789660, 19) | y_eval: (27789660,)\n",
      "Train -> X: (27789660, 19) | groups: 27789660\n",
      "Eval  -> X: (27789660, 19) | y: (27789660,) | groups: 27789660\n",
      "N feats: train 19 | eval 19\n"
     ]
    }
   ],
   "source": [
    "# --- Carga robusta desde /data ---\n",
    "# --- Carga robusta desde la ruta resuelta en CFG[\"external_data_dir\"] ---\n",
    "import os, gc, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(CFG[\"external_data_dir\"])\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "\n",
    "# Listado informativo (ayuda si algo falla)\n",
    "parquets = sorted(DATA_DIR.glob(\"*.parquet\"), key=lambda p: p.name.lower())\n",
    "print(\"Parquets disponibles:\")\n",
    "for p in parquets:\n",
    "    try:\n",
    "        print(f\" - {p.name}  ({p.stat().st_size/1e6:.1f} MB)\")\n",
    "    except Exception:\n",
    "        print(f\" - {p.name}\")\n",
    "\n",
    "def read_parquet_min(path: Path):\n",
    "    df = pd.read_parquet(path, engine=\"pyarrow\")\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_float_dtype(df[c]): df[c] = df[c].astype(np.float32)\n",
    "        elif pd.api.types.is_integer_dtype(df[c]) and df[c].max() <= np.iinfo(np.int32).max:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "    return df\n",
    "\n",
    "def detect_label_col(df):\n",
    "    for k in [\"y\",\"label\",\"is_suspicious\",\"target\"]:\n",
    "        if k in df.columns: return k\n",
    "    return None\n",
    "\n",
    "def detect_group_col(df):\n",
    "    for k in [\"mmsi\",\"group\",\"ship_id\"]:\n",
    "        if k in df.columns: return k\n",
    "    return None\n",
    "\n",
    "def pick_first_existing(base: Path, *names_or_patterns):\n",
    "    # 1) exactos\n",
    "    for n in names_or_patterns:\n",
    "        if \"*\" not in n and \"?\" not in n and \"[\" not in n:\n",
    "            p = base / n\n",
    "            if p.exists(): return p\n",
    "    # 2) patrones -> el más grande\n",
    "    for patt in names_or_patterns:\n",
    "        if any(ch in patt for ch in \"*?[]\"):\n",
    "            matches = list(base.glob(patt))\n",
    "            if matches:\n",
    "                matches = sorted(matches, key=lambda x: x.stat().st_size if x.exists() else 0, reverse=True)\n",
    "                return matches[0]\n",
    "    return None\n",
    "\n",
    "# --- TRAIN (solo normales) ---\n",
    "train_path = pick_first_existing(DATA_DIR,\n",
    "    \"windows_aligned_normal.parquet\", \"norm_windows_flat.parquet\", \"ais_norm_windows.parquet\",\n",
    "    \"*windows_aligned_normal*.parquet\", \"*norm*windows*.parquet\", \"*ais_norm_windows*.parquet\"\n",
    ")\n",
    "if train_path is None:\n",
    "    raise FileNotFoundError(\"No encontré TRAIN normal (p.ej. windows_aligned_normal.parquet) en /data.\")\n",
    "df_tr = read_parquet_min(train_path)\n",
    "\n",
    "ycol_tr = detect_label_col(df_tr)     # no debería importar; se excluye de X\n",
    "gcol_tr = detect_group_col(df_tr)\n",
    "drop_common = {\"lat\",\"lon\",\"idx\",\"idx_end\",\"window_id\"}\n",
    "drop_train = set([c for c in [ycol_tr, gcol_tr] if c]) | drop_common\n",
    "feat_tr = [c for c in df_tr.columns if c not in drop_train]\n",
    "X_train = df_tr[feat_tr].to_numpy(dtype=np.float32)\n",
    "groups_train = df_tr[gcol_tr].to_numpy() if gcol_tr else None\n",
    "print(\"TRAIN ->\", train_path.name, \"| X_train:\", X_train.shape)\n",
    "\n",
    "# --- EVAL (varios esquemas soportados) ---\n",
    "eval_single = pick_first_existing(DATA_DIR,\n",
    "    \"windows_with_labels_aligned.parquet\", \"*windows_with_labels_aligned*.parquet\",\n",
    "    \"eval_windows_aligned.parquet\", \"*eval_windows_aligned*.parquet\",\n",
    "    \"windows_with_labels.parquet\", \"*windows_with_labels*.parquet\"\n",
    ")\n",
    "eval_wl_norm = pick_first_existing(DATA_DIR, \"windows_with_labels_aligned_normal.parquet\", \"*windows_with_labels_aligned_normal*.parquet\")\n",
    "eval_wl_anom = pick_first_existing(DATA_DIR, \"windows_with_labels_aligned_anom.parquet\",   \"*windows_with_labels_aligned_anom*.parquet\")\n",
    "eval_norm    = pick_first_existing(DATA_DIR, \"eval_windows_aligned_normal.parquet\",        \"*eval_windows_aligned_normal*.parquet\")\n",
    "eval_anom    = pick_first_existing(DATA_DIR, \"eval_windows_aligned_anom.parquet\",          \"*eval_windows_aligned_anom*.parquet\")\n",
    "labels_any   = pick_first_existing(DATA_DIR, \"eval_labels_aligned.parquet\", \"*eval_labels_aligned*.parquet\", \"labels.parquet\", \"*labels*.parquet\")\n",
    "\n",
    "if eval_single is not None:\n",
    "    df_ev = read_parquet_min(eval_single)\n",
    "    ycol_ev = detect_label_col(df_ev)\n",
    "    gcol_ev = detect_group_col(df_ev)\n",
    "    if ycol_ev is None:\n",
    "        if labels_any is None:\n",
    "            raise FileNotFoundError(\"Eval único sin etiquetas embebidas y no hay archivo de labels en /data.\")\n",
    "        df_y = read_parquet_min(labels_any)\n",
    "        ycol_y = detect_label_col(df_y) or df_y.select_dtypes(include=[\"int32\",\"int16\",\"int8\"]).columns[-1]\n",
    "        if len(df_y) != len(df_ev): raise ValueError(f\"Desalineación eval vs labels: {len[df_ev]} vs {len[df_y]}\")\n",
    "        drop_eval = set([gcol_ev]) | drop_common\n",
    "        feat_ev = [c for c in df_ev.columns if c not in drop_eval]\n",
    "        X_eval = df_ev[feat_ev].to_numpy(dtype=np.float32)\n",
    "        y_eval = df_y[ycol_y].astype(np.int8).to_numpy()\n",
    "    else:\n",
    "        drop_eval = set([ycol_ev, gcol_ev]) | drop_common\n",
    "        feat_ev = [c for c in df_ev.columns if c not in drop_eval]\n",
    "        X_eval = df_ev[feat_ev].to_numpy(dtype=np.float32)\n",
    "        y_eval = df_ev[ycol_ev].astype(np.int8).to_numpy()\n",
    "    groups_eval = df_ev[gcol_ev].to_numpy() if gcol_ev else None\n",
    "    print(\"EVAL ->\", eval_single.name, \"| X_eval:\", X_eval.shape, \"| y_eval:\", y_eval.shape)\n",
    "\n",
    "elif eval_wl_norm is not None and eval_wl_anom is not None:\n",
    "    df_n = read_parquet_min(eval_wl_norm); df_a = read_parquet_min(eval_wl_anom)\n",
    "    common = [c for c in df_n.columns if c in df_a.columns]\n",
    "    df_n = df_n[common].copy(); df_a = df_a[common].copy()\n",
    "    ycol_ev = detect_label_col(df_n); gcol_ev = detect_group_col(df_n)\n",
    "    drop_eval = set([ycol_ev, gcol_ev]) | drop_common\n",
    "    feat_ev = [c for c in common if c not in drop_eval]\n",
    "    X_eval = pd.concat([df_n[feat_ev], df_a[feat_ev]], ignore_index=True).to_numpy(dtype=np.float32)\n",
    "    y_eval = pd.concat([df_n[ycol_ev], df_a[ycol_ev]], ignore_index=True).astype(np.int8).to_numpy()\n",
    "    groups_eval = (pd.concat([df_n[gcol_ev], df_a[gcol_ev]], ignore_index=True).to_numpy() if gcol_ev else None)\n",
    "    print(\"EVAL ->\", eval_wl_norm.name, \"+\", eval_wl_anom.name, \"| X_eval:\", X_eval.shape, \"| y_eval:\", y_eval.shape)\n",
    "\n",
    "elif eval_norm is not None and eval_anom is not None:\n",
    "    if labels_any is None:\n",
    "        raise FileNotFoundError(\"Eval normal/anom sin etiquetas embebidas y no hay archivo de labels en /data.\")\n",
    "    df_n = read_parquet_min(eval_norm); df_a = read_parquet_min(eval_anom); df_y = read_parquet_min(labels_any)\n",
    "    common = [c for c in df_n.columns if c in df_a.columns]\n",
    "    df_n = df_n[common].copy(); df_a = df_a[common].copy()\n",
    "    gcol_ev = detect_group_col(df_n)\n",
    "    drop_eval = set([gcol_ev]) | drop_common\n",
    "    feat_ev = [c for c in common if c not in drop_eval]\n",
    "    df_concat = pd.concat([df_n[feat_ev], df_a[feat_ev]], ignore_index=True)\n",
    "    X_eval = df_concat.to_numpy(dtype=np.float32)\n",
    "    ycol_y = detect_label_col(df_y) or df_y.select_dtypes(include=[\"int32\",\"int16\",\"int8\"]).columns[-1]\n",
    "    if len(df_y) != len(df_concat): raise ValueError(f\"Desalineación eval concat vs labels: {len(df_concat)} vs {len(df_y)}\")\n",
    "    y_eval = df_y[ycol_y].astype(np.int8).to_numpy()\n",
    "    groups_eval = (pd.concat([df_n[gcol_ev], df_a[gcol_ev]], ignore_index=True).to_numpy() if gcol_ev else None)\n",
    "    print(\"EVAL ->\", eval_norm.name, \"+\", eval_anom.name, \"| X_eval:\", X_eval.shape, \"| y_eval:\", y_eval.shape)\n",
    "\n",
    "else:\n",
    "    raise FileNotFoundError(\"No se pudo resolver un set de EVAL válido en /data.\")\n",
    "\n",
    "# Limpieza y reporte\n",
    "del df_tr; gc.collect()\n",
    "print(\"Train -> X:\", X_train.shape, \"| groups:\", None if groups_train is None else len(groups_train))\n",
    "print(\"Eval  -> X:\", X_eval.shape,  \"| y:\", y_eval.shape, \"| groups:\", None if groups_eval is None else len(groups_eval))\n",
    "print(\"N feats: train\", X_train.shape[1], \"| eval\", X_eval.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sampled: (499950, 19)\n",
      "Scaled train: (499950, 19)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Imputación + Escalado + Muestreo (memory-safe) ---\n",
    "import os, numpy as np, gc\n",
    "\n",
    "CFG.setdefault(\"max_train_samples\", 500_000)\n",
    "CFG.setdefault(\"max_search_samples\", 200_000)\n",
    "CFG.setdefault(\"eval_batch_size\", 200_000)\n",
    "os.makedirs(CFG[\"out_dir\"], exist_ok=True)\n",
    "\n",
    "def sample_by_group(n_max, X, groups):\n",
    "    if (n_max is None) or (X.shape[0] <= n_max):\n",
    "        idx = np.arange(X.shape[0]); return X, (groups if groups is not None else None), idx\n",
    "    rng = np.random.default_rng(42)\n",
    "    if groups is None:\n",
    "        idx = rng.choice(X.shape[0], n_max, replace=False); return X[idx], None, idx\n",
    "    uniq = np.unique(groups); per_g = max(1, n_max // len(uniq)); take = []\n",
    "    for g in uniq:\n",
    "        g_idx = np.where(groups == g)[0]\n",
    "        take.extend(rng.choice(g_idx, min(per_g, g_idx.size), replace=False).tolist())\n",
    "    take = np.array(take)\n",
    "    if take.size > n_max: take = rng.choice(take, n_max, replace=False)\n",
    "    return X[take], groups[take], take\n",
    "\n",
    "def colwise_nanmedian(X):\n",
    "    Xc = X.copy(); Xc[~np.isfinite(Xc)] = np.nan\n",
    "    med = np.nanmedian(Xc, axis=0)\n",
    "    med = np.where(np.isfinite(med), med, 0.0).astype(np.float32)\n",
    "    return med\n",
    "\n",
    "def impute_inplace(X, medians):\n",
    "    bad = ~np.isfinite(X)\n",
    "    if bad.any():\n",
    "        cols = np.where(bad)[1]\n",
    "        X[bad] = medians[cols]\n",
    "\n",
    "def fit_standardizer(X):\n",
    "    mean = X.mean(axis=0).astype(np.float32)\n",
    "    var  = X.var(axis=0).astype(np.float32)\n",
    "    std  = np.sqrt(var, dtype=np.float32); std[std == 0.0] = 1.0\n",
    "    return mean, std\n",
    "\n",
    "def apply_standardizer_inplace(X, mean, std):\n",
    "    X -= mean; X /= std\n",
    "\n",
    "# 1) Muestrear TRAIN\n",
    "X_train_s, groups_train_s, _ = sample_by_group(CFG[\"max_train_samples\"], X_train, groups_train)\n",
    "print(\"Train sampled:\", X_train_s.shape)\n",
    "\n",
    "# 2) Imputación + Escalado\n",
    "X_train_s = X_train_s.astype(np.float32, copy=False)\n",
    "X_train_s[~np.isfinite(X_train_s)] = np.nan\n",
    "train_medians = colwise_nanmedian(X_train_s)\n",
    "impute_inplace(X_train_s, train_medians)\n",
    "train_mean, train_std = fit_standardizer(X_train_s)\n",
    "apply_standardizer_inplace(X_train_s, train_mean, train_std)\n",
    "\n",
    "X_train_sc = X_train_s\n",
    "groups_train = groups_train_s\n",
    "print(\"Scaled train:\", X_train_sc.shape)\n",
    "\n",
    "# 3) Guardar parámetros para reproducibilidad\n",
    "np.save(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_imputer_medians.npy\"), train_medians)\n",
    "np.savez(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_scaler_params.npz\"), mean=train_mean, std=train_std)\n",
    "\n",
    "# 4) Generador para transformar EVAL por lotes (sin construir X_eval_sc completo)\n",
    "def transform_eval_in_batches(X, batch_size=CFG[\"eval_batch_size\"]):\n",
    "    n = X.shape[0]\n",
    "    for s in range(0, n, batch_size):\n",
    "        e = min(s + batch_size, n)\n",
    "        Xe = X[s:e].astype(np.float32, copy=False)\n",
    "        Xe[~np.isfinite(Xe)] = np.nan\n",
    "        impute_inplace(Xe, train_medians)\n",
    "        apply_standardizer_inplace(Xe, train_mean, train_std)\n",
    "        yield s, e, Xe\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>rate_mean</th>\n",
       "      <th>rate_std</th>\n",
       "      <th>obj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'gamma': 0.01, 'nu': 0.05}</td>\n",
       "      <td>0.051850</td>\n",
       "      <td>0.008740</td>\n",
       "      <td>0.010590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'gamma': 'scale', 'nu': 0.05}</td>\n",
       "      <td>0.056581</td>\n",
       "      <td>0.015771</td>\n",
       "      <td>0.022351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'gamma': 0.01, 'nu': 0.01}</td>\n",
       "      <td>0.010326</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.041539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'gamma': 'scale', 'nu': 0.01}</td>\n",
       "      <td>0.015872</td>\n",
       "      <td>0.009458</td>\n",
       "      <td>0.043586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'gamma': 0.01, 'nu': 0.1}</td>\n",
       "      <td>0.103270</td>\n",
       "      <td>0.018643</td>\n",
       "      <td>0.071913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           params  rate_mean  rate_std       obj\n",
       "4     {'gamma': 0.01, 'nu': 0.05}   0.051850  0.008740  0.010590\n",
       "1  {'gamma': 'scale', 'nu': 0.05}   0.056581  0.015771  0.022351\n",
       "3     {'gamma': 0.01, 'nu': 0.01}   0.010326  0.001866  0.041539\n",
       "0  {'gamma': 'scale', 'nu': 0.01}   0.015872  0.009458  0.043586\n",
       "5      {'gamma': 0.01, 'nu': 0.1}   0.103270  0.018643  0.071913"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'gamma': 0.01, 'nu': 0.05} | splits: 5 | search_subset: (199980, 19)\n"
     ]
    }
   ],
   "source": [
    "# --- Búsqueda de hiperparámetros (subset; objetivo = outlier-rate ~ 5%) ---\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import GroupKFold, KFold, ParameterGrid\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "param_grid = list(ParameterGrid({\"nu\": CFG[\"svm_nu_grid\"], \"gamma\": CFG[\"svm_gamma_grid\"]}))\n",
    "target_outlier_rate = 0.05\n",
    "\n",
    "def build_search_subset(X, groups, n_max):\n",
    "    if (n_max is None) or (X.shape[0] <= n_max): return X, groups\n",
    "    rng = np.random.default_rng(123)\n",
    "    if groups is None:\n",
    "        idx = rng.choice(X.shape[0], n_max, replace=False); return X[idx], None\n",
    "    uniq = np.unique(groups); per_g = max(1, n_max // len(uniq)); take = []\n",
    "    for g in uniq:\n",
    "        g_idx = np.where(groups == g)[0]\n",
    "        take.extend(rng.choice(g_idx, min(per_g, g_idx.size), replace=False).tolist())\n",
    "    take = np.array(take)\n",
    "    if take.size > n_max: take = rng.choice(take, n_max, replace=False)\n",
    "    return X[take], groups[take]\n",
    "\n",
    "X_search, groups_search = build_search_subset(X_train_sc, groups_train, CFG[\"max_search_samples\"])\n",
    "\n",
    "if (groups_search is not None) and (len(np.unique(groups_search)) >= 2):\n",
    "    n_splits = min(CFG[\"kfold_splits\"], len(np.unique(groups_search)))\n",
    "    splitter = GroupKFold(n_splits=n_splits); split_args = dict(X=X_search, y=None, groups=groups_search)\n",
    "else:\n",
    "    n_splits = max(2, CFG[\"kfold_splits\"])\n",
    "    splitter = KFold(n_splits=n_splits, shuffle=True, random_state=42); split_args = dict(X=X_search, y=None)\n",
    "\n",
    "def outlier_rate(pred): return float((pred == -1).mean())\n",
    "\n",
    "best_cfg, best_obj, rows = None, None, []\n",
    "for p in param_grid:\n",
    "    fold_rates = []\n",
    "    for tr_idx, va_idx in splitter.split(**split_args):\n",
    "        Xtr, Xva = X_search[tr_idx], X_search[va_idx]\n",
    "        m = OneClassSVM(kernel=CFG[\"kernel\"], nu=p[\"nu\"], gamma=p[\"gamma\"])\n",
    "        m.fit(Xtr)\n",
    "        pred = m.predict(Xva)   # +1 normal, -1 outlier\n",
    "        fold_rates.append(outlier_rate(pred))\n",
    "    rate_mean, rate_std = float(np.mean(fold_rates)), float(np.std(fold_rates))\n",
    "    obj = abs(rate_mean - target_outlier_rate) + rate_std\n",
    "    rows.append({\"params\": p, \"rate_mean\": rate_mean, \"rate_std\": rate_std, \"obj\": obj})\n",
    "    if (best_obj is None) or (obj < best_obj):\n",
    "        best_obj, best_cfg = obj, p\n",
    "\n",
    "res_df = pd.DataFrame(rows).sort_values(\"obj\")\n",
    "display(res_df.head(5))\n",
    "print(\"Best params:\", best_cfg, \"| splits:\", n_splits, \"| search_subset:\", X_search.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model trained.\n"
     ]
    }
   ],
   "source": [
    "# --- Entrenamiento final ---\n",
    "from sklearn.svm import OneClassSVM\n",
    "final_model = OneClassSVM(kernel=CFG[\"kernel\"], nu=best_cfg[\"nu\"], gamma=best_cfg[\"gamma\"])\n",
    "final_model.fit(X_train_sc)\n",
    "print(\"Final model trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/ocsvm_runs/ocsvm_rbf_model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Cargar modelo si no está en memoria\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_model\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mOUT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCFG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43martifact_prefix\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_model.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     20\u001b[0m         final_model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Necesitamos X_eval e y_eval en memoria\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/IPython/core/interactiveshell.py:308\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/ocsvm_runs/ocsvm_rbf_model.pkl'"
     ]
    }
   ],
   "source": [
    "# --- Evaluación por lotes con REANUDACIÓN (auto-setup si el kernel se reinició) ---\n",
    "import os, json, time, pickle, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "\n",
    "# --------- AUTO-SETUP (por si el kernel se reinició) ---------\n",
    "# CFG por defecto si no existe\n",
    "if 'CFG' not in globals():\n",
    "    CFG = {\n",
    "        \"out_dir\": \"data/ocsvm_runs\",\n",
    "        \"artifact_prefix\": \"ocsvm_rbf\",\n",
    "        \"eval_batch_size\": 200_000,\n",
    "    }\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"]); OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cargar modelo si no está en memoria\n",
    "if 'final_model' not in globals():\n",
    "    with open(OUT / f\"{CFG['artifact_prefix']}_model.pkl\", \"rb\") as f:\n",
    "        final_model = pickle.load(f)\n",
    "\n",
    "# Necesitamos X_eval e y_eval en memoria\n",
    "assert 'X_eval' in globals() and 'y_eval' in globals(), \\\n",
    "    \"Falta X_eval/y_eval en memoria. Re-ejecuta la celda de CARGA (la que arma X_train, X_eval, y_eval).\"\n",
    "\n",
    "# Cargar preprocesamiento (medianas + scaler) por si tampoco está en memoria\n",
    "try:\n",
    "    _medians = globals().get('train_medians', None)\n",
    "    _scaler_params = globals().get('train_mean', None), globals().get('train_std', None)\n",
    "    if _medians is None or _scaler_params[0] is None or _scaler_params[1] is None:\n",
    "        raise KeyError\n",
    "    medians, train_mean, train_std = _medians, _scaler_params[0], _scaler_params[1]\n",
    "except Exception:\n",
    "    medians = np.load(OUT / f\"{CFG['artifact_prefix']}_imputer_medians.npy\")\n",
    "    sp = np.load(OUT / f\"{CFG['artifact_prefix']}_scaler_params.npz\")\n",
    "    train_mean, train_std = sp[\"mean\"], sp[\"std\"]\n",
    "train_std = train_std.copy(); train_std[train_std == 0] = 1.0\n",
    "\n",
    "def impute_inplace(X, med):\n",
    "    bad = ~np.isfinite(X)\n",
    "    if bad.any():\n",
    "        X[bad] = med[np.where(bad)[1]]\n",
    "\n",
    "def standardize_inplace(X, m, s):\n",
    "    X -= m; X /= s\n",
    "\n",
    "# --------- REANUDACIÓN CON MEMMAP ---------\n",
    "n_eval = X_eval.shape[0]\n",
    "scores_path = OUT / f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"\n",
    "progress_path = OUT / f\"{CFG['artifact_prefix']}_progress.json\"\n",
    "expected_bytes = n_eval * 4  # float32\n",
    "\n",
    "# si hay memmap de tamaño incorrecto, recrear\n",
    "if scores_path.exists() and scores_path.stat().st_size != expected_bytes:\n",
    "    print(f\"[WARN] Memmap incorrecto ({scores_path.stat().st_size} vs {expected_bytes}) -> recreando.\")\n",
    "    scores_path.unlink()\n",
    "\n",
    "mode = \"r+\" if scores_path.exists() else \"w+\"\n",
    "scores_mm = np.memmap(scores_path, dtype=np.float32, mode=mode, shape=(n_eval,))\n",
    "if mode == \"w+\":\n",
    "    scores_mm[:] = np.nan\n",
    "    scores_mm.flush()\n",
    "\n",
    "# punto de reanudación\n",
    "start = 0\n",
    "if progress_path.exists():\n",
    "    try:\n",
    "        start = int(json.loads(progress_path.read_text()).get(\"next_start\", 0))\n",
    "    except Exception:\n",
    "        start = 0\n",
    "if start <= 0:\n",
    "    # buscar primer NaN\n",
    "    nan_mask = np.isnan(scores_mm)\n",
    "    start = int(np.argmax(nan_mask)) if nan_mask.any() else n_eval\n",
    "\n",
    "print(f\"[RESUME] next_start = {start:,}/{n_eval:,}\")\n",
    "\n",
    "bs = int(CFG.get(\"eval_batch_size\", 200_000))\n",
    "t0 = time.time(); last = t0\n",
    "\n",
    "try:\n",
    "    for s in range(start, n_eval, bs):\n",
    "        e = min(s + bs, n_eval)\n",
    "        Xe = X_eval[s:e].astype(np.float32, copy=False)\n",
    "        Xe[~np.isfinite(Xe)] = np.nan\n",
    "        impute_inplace(Xe, medians)\n",
    "        standardize_inplace(Xe, train_mean, train_std)\n",
    "\n",
    "        scores_mm[s:e] = -final_model.decision_function(Xe)\n",
    "        scores_mm.flush()\n",
    "        progress_path.write_text(json.dumps({\"next_start\": e}))\n",
    "\n",
    "        now = time.time()\n",
    "        if now - last > 10:\n",
    "            rate = (e - start) / max(1e-6, (now - t0))\n",
    "            print(f\"Progress: {100*e/n_eval:5.2f}% | {e:,}/{n_eval:,} | ~{rate:,.0f} rows/s\")\n",
    "            last = now\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    scores_mm.flush()\n",
    "    progress_path.write_text(json.dumps({\"next_start\": e}))\n",
    "    print(f\"\\n[INTERRUPTED] Progreso guardado. Reanuda desde idx={e}.\")\n",
    "    raise\n",
    "except Exception as ex:\n",
    "    scores_mm.flush()\n",
    "    progress_path.write_text(json.dumps({\"next_start\": s}))\n",
    "    print(f\"\\n[ERROR] Guardado progreso hasta idx={s}. Detalle: {ex}\")\n",
    "    raise\n",
    "\n",
    "# marcar completo\n",
    "progress_path.write_text(json.dumps({\"next_start\": n_eval}))\n",
    "scores = scores_mm\n",
    "\n",
    "# --------- MÉTRICAS + GUARDADOS ---------\n",
    "yb = y_eval.astype(int)\n",
    "if len(np.unique(yb)) > 1:\n",
    "    roc = roc_auc_score(yb, scores)\n",
    "    prec, rec, _ = precision_recall_curve(yb, scores); pr_auc = auc(rec, prec)\n",
    "    ap = average_precision_score(yb, scores)\n",
    "else:\n",
    "    roc = pr_auc = ap = np.nan\n",
    "\n",
    "print(f\"ROC-AUC: {roc:.4f} | PR-AUC: {pr_auc:.4f} | AP: {ap:.4f}\")\n",
    "print(\"Scores memmap:\", str(scores_path))\n",
    "\n",
    "# guardados idempotentes\n",
    "with open(OUT / f\"{CFG['artifact_prefix']}_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_model, f)\n",
    "with open(OUT / f\"{CFG['artifact_prefix']}_config.json\", \"w\") as f:\n",
    "    json.dump(CFG | {\"best_params\": globals().get(\"best_cfg\", {}),\n",
    "                     \"metrics\": {\"roc_auc\": float(roc), \"pr_auc\": float(pr_auc), \"ap\": float(ap)}},\n",
    "              f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score, average_precision_score, precision_recall_curve, auc\n\u001b[0;32m----> 5\u001b[0m n_eval \u001b[38;5;241m=\u001b[39m \u001b[43mX_eval\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m scores_path_mm \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124martifact_prefix\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_eval_scores_mm.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Si existe pero no coincide tamaño esperado, recrear\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_eval' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Evaluación por lotes (memmap) + métricas + guardados ---\n",
    "import numpy as np, os, json, pickle, time\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "\n",
    "n_eval = X_eval.shape[0]\n",
    "scores_path_mm = os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\")\n",
    "\n",
    "# Si existe pero no coincide tamaño esperado, recrear\n",
    "if os.path.exists(scores_path_mm):\n",
    "    expected = n_eval * 4  # float32\n",
    "    if os.stat(scores_path_mm).st_size != expected:\n",
    "        print(\"[WARN] memmap incompleto -> recreando.\")\n",
    "        os.remove(scores_path_mm)\n",
    "\n",
    "scores_mm = np.memmap(scores_path_mm, dtype=np.float32, mode=\"w+\", shape=(n_eval,))\n",
    "\n",
    "t0 = time.time(); last = t0; done = 0\n",
    "for s, e, Xe_sc in transform_eval_in_batches(X_eval):\n",
    "    scores_mm[s:e] = -final_model.decision_function(Xe_sc)\n",
    "    done = e\n",
    "    now = time.time()\n",
    "    if now - last > 10:\n",
    "        rate = done / max(1e-6, (now - t0))\n",
    "        print(f\"Progress: {100*done/n_eval:5.2f}% | {done:,}/{n_eval:,} | ~{rate:,.0f} rows/s\")\n",
    "        last = now\n",
    "\n",
    "scores = scores_mm\n",
    "yb = y_eval.astype(int)\n",
    "if len(np.unique(yb)) > 1:\n",
    "    roc = roc_auc_score(yb, scores)\n",
    "    prec, rec, _ = precision_recall_curve(yb, scores); pr_auc = auc(rec, prec)\n",
    "    ap = average_precision_score(yb, scores)\n",
    "else:\n",
    "    roc = pr_auc = ap = np.nan\n",
    "\n",
    "print(f\"ROC-AUC: {roc:.4f} | PR-AUC: {pr_auc:.4f} | AP: {ap:.4f}\")\n",
    "print(\"Scores memmap:\", scores_path_mm)\n",
    "\n",
    "# Guardar modelo + config + métricas\n",
    "with open(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(final_model, f)\n",
    "with open(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_config.json\"), \"w\") as f:\n",
    "    json.dump(CFG | {\"best_params\": best_cfg,\n",
    "                     \"metrics\": {\"roc_auc\": float(roc), \"pr_auc\": float(pr_auc), \"ap\": float(ap)}},\n",
    "              f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Top-K y agregados por MMSI ---\n",
    "import numpy as np, pandas as pd, os\n",
    "\n",
    "scores = np.memmap(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"),\n",
    "                   dtype=np.float32, mode=\"r\", shape=(X_eval.shape[0],))\n",
    "\n",
    "k_rate = 0.01  # 1%\n",
    "k = max(1, int(len(scores) * k_rate))\n",
    "thr_k = np.partition(scores, -k)[-k]\n",
    "pred_topk = (scores >= thr_k).astype(np.int8)\n",
    "\n",
    "# Detalle TOP-K\n",
    "topk_idx = np.where(pred_topk == 1)[0]\n",
    "topk_df = pd.DataFrame({\n",
    "    \"idx\": topk_idx.astype(np.int64),\n",
    "    \"anomaly_score\": scores[topk_idx].astype(np.float32),\n",
    "    \"y_eval\": y_eval[topk_idx].astype(np.int8)\n",
    "})\n",
    "if 'groups_eval' in globals() and groups_eval is not None:\n",
    "    topk_df[\"mmsi\"] = groups_eval[topk_idx].astype(np.int64)\n",
    "\n",
    "topk_path = os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_topk_{int(k_rate*100)}pct.parquet\")\n",
    "topk_df.to_parquet(topk_path, index=False)\n",
    "print(\"Saved TOP-K:\", topk_path, \"| rows:\", len(topk_df))\n",
    "\n",
    "# Métricas @k\n",
    "yb = y_eval.astype(int)\n",
    "tp = int(((pred_topk==1) & (yb==1)).sum())\n",
    "fp = int(((pred_topk==1) & (yb==0)).sum())\n",
    "fn = int(((pred_topk==0) & (yb==1)).sum())\n",
    "prec_k = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "rec_k  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "f1_k   = 2*prec_k*rec_k/(prec_k+rec_k) if (prec_k+rec_k)>0 else 0.0\n",
    "print(f\"@k={k_rate*100:.1f}% -> P: {prec_k:.3f} | R: {rec_k:.3f} | F1: {f1_k:.3f}  (k={k})\")\n",
    "\n",
    "# Agregado por MMSI\n",
    "if 'groups_eval' in globals() and groups_eval is not None:\n",
    "    mmsi_all, n_by_mmsi = np.unique(groups_eval, return_counts=True)\n",
    "    mmsi_top, n_top_by_mmsi = np.unique(groups_eval[pred_topk==1], return_counts=True)\n",
    "    top_map = dict(zip(mmsi_top.tolist(), n_top_by_mmsi.tolist()))\n",
    "    anom_win = np.array([top_map.get(m, 0) for m in mmsi_all], dtype=np.int32)\n",
    "    anom_rate = anom_win / n_by_mmsi\n",
    "    agg_df = pd.DataFrame({\"mmsi\": mmsi_all, \"n_win\": n_by_mmsi, \"anom_win\": anom_win, \"anom_rate\": anom_rate})\n",
    "    agg_path = os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_mmsi_agg.parquet\")\n",
    "    agg_df.to_parquet(agg_path, index=False)\n",
    "    print(\"Saved MMSI agg:\", agg_path)\n",
    "    display(agg_df.sort_values(\"anom_rate\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CFG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score, average_precision_score, precision_recall_curve, auc\n\u001b[0;32m----> 6\u001b[0m OUT \u001b[38;5;241m=\u001b[39m Path(\u001b[43mCFG\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      7\u001b[0m scores_mm_path \u001b[38;5;241m=\u001b[39m OUT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124martifact_prefix\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_eval_scores_mm.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_eval\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_eval\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-ejecuta la Celda 4 (Carga) antes de recuperar.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CFG' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Recuperación: re-scorear sin re-entrenar ---\n",
    "import os, numpy as np, pickle, json\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"])\n",
    "scores_mm_path = OUT / f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"\n",
    "\n",
    "assert 'X_eval' in globals() and 'y_eval' in globals(), \"Re-ejecuta la Celda 4 (Carga) antes de recuperar.\"\n",
    "\n",
    "# Cargar modelo + preprocesamiento\n",
    "with open(OUT / f\"{CFG['artifact_prefix']}_model.pkl\", \"rb\") as f:\n",
    "    final_model = pickle.load(f)\n",
    "medians = np.load(OUT / f\"{CFG['artifact_prefix']}_imputer_medians.npy\")\n",
    "scaler = np.load(OUT / f\"{CFG['artifact_prefix']}_scaler_params.npz\")\n",
    "mean, std = scaler[\"mean\"], scaler[\"std\"]; std = std.copy(); std[std==0]=1.0\n",
    "\n",
    "def impute_inplace(X, med):\n",
    "    bad = ~np.isfinite(X)\n",
    "    if bad.any(): X[bad] = np.take(med, np.where(bad)[1])\n",
    "def standardize_inplace(X, m, s):\n",
    "    X -= m; X /= s\n",
    "def transform_eval_in_batches_recover(X, batch_size=CFG[\"eval_batch_size\"]):\n",
    "    n = X.shape[0]\n",
    "    for s in range(0, n, batch_size):\n",
    "        e = min(s + batch_size, n)\n",
    "        Xe = X[s:e].astype(np.float32, copy=False)\n",
    "        Xe[~np.isfinite(Xe)] = np.nan\n",
    "        impute_inplace(Xe, medians); standardize_inplace(Xe, mean, std)\n",
    "        yield s, e, Xe\n",
    "\n",
    "# Memmap limpio si quedó incompleto\n",
    "if scores_mm_path.exists():\n",
    "    exp_bytes = X_eval.shape[0] * 4\n",
    "    if scores_mm_path.stat().st_size != exp_bytes:\n",
    "        print(\"[WARN] memmap incompleto -> borrando.\")\n",
    "        scores_mm_path.unlink()\n",
    "\n",
    "scores_mm = np.memmap(scores_mm_path, dtype=np.float32, mode=\"w+\", shape=(X_eval.shape[0],))\n",
    "for s, e, Xe_sc in transform_eval_in_batches_recover(X_eval):\n",
    "    scores_mm[s:e] = -final_model.decision_function(Xe_sc)\n",
    "\n",
    "scores = scores_mm; yb = y_eval.astype(int)\n",
    "if len(np.unique(yb)) > 1:\n",
    "    roc = roc_auc_score(yb, scores)\n",
    "    from sklearn.metrics import precision_recall_curve, auc, average_precision_score\n",
    "    prec, rec, _ = precision_recall_curve(yb, scores); pr_auc = auc(rec, prec)\n",
    "    ap = average_precision_score(yb, scores)\n",
    "else:\n",
    "    roc = pr_auc = ap = np.nan\n",
    "print(f\"[RECOVER] ROC-AUC: {roc:.4f} | PR-AUC: {pr_auc:.4f} | AP: {ap:.4f}\")\n",
    "print(\"OK ->\", scores_mm_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
