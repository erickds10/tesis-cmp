{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b8c7f97",
   "metadata": {},
   "source": [
    "\n",
    "# One‑Class SVM (OC‑SVM) — AIS Anomaly Detection (Galápagos)\n",
    "\n",
    "**Objetivo:** Entrenar y evaluar un detector no supervisado (One‑Class SVM, kernel RBF) para identificar anomalías en series de tiempo de trayectorias AIS de pesqueros alrededor de la RMG, usando **solo comportamiento normal** para el entrenamiento y validando contra puntos/ventanas marcadas como `is_suspicious = 1`.\n",
    "\n",
    "**Pautas (resumen de la planificación):**\n",
    "- Entrenar **solo con `is_suspicious = 0`** (comportamiento normal).\n",
    "- Ingeniería de variables por MMSI y orden temporal: lags, diferencias, SMA/EMA, distancia Haversine por segmento, delta de tiempo, aceleración, velocidad angular, y ratios como `distancia_a_costa/distancia_a_puerto`.\n",
    "- Serialización por **ventanas deslizantes** (tamaño `T`) y representación vectorizada `(N, T×F)` para modelos tabulares.\n",
    "- **Split por MMSI** (GroupKFold) para evitar fuga por identidad.\n",
    "- Métricas: ROC‑AUC, PR‑AUC, Precision@k, Recall@k, F1@k; análisis por ventana y por trayectoria.\n",
    "- Guardado de **artefactos**: scaler, modelo, configuración y scores para replicabilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9acd91e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output root: data -> /teamspace/studios/this_studio/data\n"
     ]
    }
   ],
   "source": [
    "# --- Ensure local data dir for outputs ---\n",
    "import os, pathlib, json\n",
    "SAVE_ROOT = \"data\"  # local to this notebook workspace\n",
    "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "print(f\"Output root: {SAVE_ROOT} -> {os.path.abspath(SAVE_ROOT)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21cd9a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {\n",
      "  \"input_parquet\": \"/teamspace/studios/profound-silver-kn8tf/ais_anomaly/data/ais_enriched.parquet\",\n",
      "  \"train_filter_col\": \"is_suspicious\",\n",
      "  \"mmsi_col\": \"mmsi\",\n",
      "  \"timestamp_col\": \"timestamp\",\n",
      "  \"base_features\": [\n",
      "    \"lat\",\n",
      "    \"lon\",\n",
      "    \"speed\",\n",
      "    \"course\",\n",
      "    \"depth\",\n",
      "    \"dist_coast_km\",\n",
      "    \"dist_port_km\"\n",
      "  ],\n",
      "  \"lags\": [\n",
      "    1,\n",
      "    2\n",
      "  ],\n",
      "  \"ema_spans\": [\n",
      "    3,\n",
      "    5\n",
      "  ],\n",
      "  \"window_size\": 10,\n",
      "  \"step_size\": 5,\n",
      "  \"svm_nu_grid\": [\n",
      "    0.01,\n",
      "    0.05,\n",
      "    0.1\n",
      "  ],\n",
      "  \"svm_gamma_grid\": [\n",
      "    \"scale\",\n",
      "    0.1,\n",
      "    0.01\n",
      "  ],\n",
      "  \"kernel\": \"rbf\",\n",
      "  \"kfold_splits\": 5,\n",
      "  \"out_dir\": \"data/ocsvm_runs\",\n",
      "  \"artifact_prefix\": \"ocsvm_rbf\",\n",
      "  \"save_root\": \"data\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Setup ---\n",
    "import os, math, json, gc, pickle, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import GroupKFold, ParameterGrid\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibilidad\n",
    "import random\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# --- Configuración global ---\n",
    "CFG = {\n",
    "    # Entradas (ajusta paths a tu estructura real)\n",
    "    \"input_parquet\": \"/teamspace/studios/profound-silver-kn8tf/ais_anomaly/data/ais_enriched.parquet\",        # Dataset enriquecido (AIS + contexto)\n",
    "    \"train_filter_col\": \"is_suspicious\",                  # Col de validación (no usada para entrenar)\n",
    "    \"mmsi_col\": \"mmsi\",\n",
    "    \"timestamp_col\": \"timestamp\",\n",
    "    # Variables base esperadas en el parquet (ajusta a tus nombres)\n",
    "    \"base_features\": [\"lat\", \"lon\", \"speed\", \"course\", \"depth\", \"dist_coast_km\", \"dist_port_km\"],\n",
    "    # Parámetros de ingeniería\n",
    "    \"lags\": [1, 2],\n",
    "    \"ema_spans\": [3, 5],\n",
    "    \"window_size\": 10,    # T\n",
    "    \"step_size\": 5,\n",
    "    # Modelado\n",
    "    \"svm_nu_grid\": [0.01, 0.05, 0.1],\n",
    "    \"svm_gamma_grid\": [\"scale\", 0.1, 0.01],\n",
    "    \"kernel\": \"rbf\",\n",
    "    # Evaluación\n",
    "    \"kfold_splits\": 5,\n",
    "    # Salidas\n",
    "    \"out_dir\": \"data/ocsvm_runs\",\n",
    "    \"artifact_prefix\": \"ocsvm_rbf\",\n",
    "    \"save_root\": \"data\"\n",
    "}\n",
    "\n",
    "os.makedirs(CFG[\"out_dir\"], exist_ok=True)\n",
    "print(\"Config:\", json.dumps(CFG, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29bc983a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dir: data/ocsvm_runs\n"
     ]
    }
   ],
   "source": [
    "# --- Safety guard: avoid writing into the source dataset directory ---\n",
    "from pathlib import Path\n",
    "inp = Path(CFG[\"input_parquet\"]).resolve()\n",
    "out = Path(CFG[\"out_dir\"]).resolve()\n",
    "if str(out).startswith(str(inp.parent)):\n",
    "    print(\"[SAFETY] Output dir overlaps input dir. Rerouting outputs to ./data/ocsvm_runs.\")\n",
    "    CFG[\"out_dir\"] = \"data/ocsvm_runs\"\n",
    "import os\n",
    "os.makedirs(CFG[\"out_dir\"], exist_ok=True)\n",
    "print(\"Output dir:\", CFG[\"out_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d0d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    # Coordinates in decimal degrees\n",
    "    R = 6371.0\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "def angular_diff_deg(a, b):\n",
    "    # minimal signed angle difference in degrees\n",
    "    diff = (a - b + 180) % 360 - 180\n",
    "    return diff\n",
    "\n",
    "def ensure_datetime(df, col):\n",
    "    if not np.issubdtype(df[col].dtype, np.datetime64):\n",
    "        df[col] = pd.to_datetime(df[col], utc=True, errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def per_mmsi_sort(df, mmsi_col, ts_col):\n",
    "    return df.sort_values([mmsi_col, ts_col]).reset_index(drop=True)\n",
    "\n",
    "def add_temporal_dynamics(df, mmsi_col, ts_col):\n",
    "    # Requires sorted df\n",
    "    df = df.copy()\n",
    "    # segment distance (km) between consecutive points\n",
    "    df[\"lat_prev\"] = df.groupby(mmsi_col)[\"lat\"].shift(1)\n",
    "    df[\"lon_prev\"] = df.groupby(mmsi_col)[\"lon\"].shift(1)\n",
    "    df[\"segment_km\"] = haversine_km(df[\"lat_prev\"], df[\"lon_prev\"], df[\"lat\"], df[\"lon\"])\n",
    "    df[\"segment_km\"] = df[\"segment_km\"].fillna(0.0)\n",
    "\n",
    "    # delta time (seconds)\n",
    "    df[\"t_prev\"] = df.groupby(mmsi_col)[ts_col].shift(1)\n",
    "    dt = (df[ts_col] - df[\"t_prev\"]).dt.total_seconds()\n",
    "    df[\"delta_t_s\"] = dt.fillna(0.0)\n",
    "\n",
    "    # speed (knots) -> m/s conversion if needed; assume 'speed' es nudo; 1 knot = 0.514444 m/s\n",
    "    # Si tu columna ya está en m/s, ajusta esta sección.\n",
    "    spd_ms = df[\"speed\"].astype(float) * 0.514444\n",
    "    df[\"acc_ms2\"] = (spd_ms - spd_ms.groupby(df[mmsi_col]).shift(1)) / df[\"delta_t_s\"].replace(0, np.nan)\n",
    "    df[\"acc_ms2\"] = df[\"acc_ms2\"].fillna(0.0)\n",
    "\n",
    "    # angular velocity (deg/s) con diferencia mínima de ángulo\n",
    "    df[\"course_prev\"] = df.groupby(mmsi_col)[\"course\"].shift(1)\n",
    "    dtheta = angular_diff_deg(df[\"course\"], df[\"course_prev\"])\n",
    "    df[\"ang_vel_deg_s\"] = (dtheta / df[\"delta_t_s\"].replace(0, np.nan)).fillna(0.0)\n",
    "\n",
    "    # ratios robustos\n",
    "    df[\"ratio_coast_port\"] = df[\"dist_coast_km\"] / (df[\"dist_port_km\"].replace(0, np.nan))\n",
    "    df[\"ratio_coast_port\"] = df[\"ratio_coast_port\"].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    df.drop(columns=[\"lat_prev\", \"lon_prev\", \"t_prev\", \"course_prev\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "def add_lags_ema(df, mmsi_col, cols, lags=[1,2], ema_spans=[3,5]):\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        for L in lags:\n",
    "            df[f\"{c}_lag{L}\"] = df.groupby(mmsi_col)[c].shift(L)\n",
    "        for span in ema_spans:\n",
    "            df[f\"{c}_ema{span}\"] = df.groupby(mmsi_col)[c].transform(lambda s: s.ewm(span=span, adjust=False).mean())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac25629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_windows(df, mmsi_col, ts_col, feature_cols, T=10, step=5):\n",
    "    # df sorted per mmsi, ts; we assume it\n",
    "    X = []\n",
    "    groups = []\n",
    "    idx_rows = []  # track terminal row index per window if useful\n",
    "    by_mmsi = df.groupby(mmsi_col, sort=False)\n",
    "    for mmsi, g in by_mmsi:\n",
    "        g = g.reset_index(drop=True)\n",
    "        n = len(g)\n",
    "        for start in range(0, max(0, n - T + 1), step):\n",
    "            end = start + T\n",
    "            if end > n: break\n",
    "            win = g.iloc[start:end]\n",
    "            X.append(win[feature_cols].to_numpy().reshape(-1))  # flatten (T*F)\n",
    "            groups.append(mmsi)\n",
    "            idx_rows.append(win.index[-1])\n",
    "    X = np.array(X, dtype=np.float32) if len(X) else np.empty((0, len(feature_cols)*T), dtype=np.float32)\n",
    "    return X, np.array(groups), np.array(idx_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b543aca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: /teamspace/studios/profound-silver-kn8tf/ais_anomaly/data\n",
      "TRAIN windows  -> windows_aligned_normal.parquet\n",
      "EVAL windows   -> windows_with_labels_aligned.parquet\n",
      "EVAL labels    -> eval_labels_aligned.parquet\n",
      "Train  -> X: (27789660, 19) | groups: 27789660\n",
      "Eval   -> X: (27789660, 19) | y: (27789660,) | groups: 27789660\n",
      "N feats (train): 19 | N feats (eval): 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Carga de datos desde artefactos de Isolation Forest (memoria-eficiente) ---\n",
    "import os, gc, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 0) Carpeta base: tomamos la del parquet de entrada (IF) y derivamos /data allí mismo.\n",
    "IF_BASE = Path(CFG[\"input_parquet\"]).resolve().parent\n",
    "DATA_DIR = IF_BASE  # p.ej.: /teamspace/studios/profound-silver-kn8tf/ais_anomaly/data\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "\n",
    "# 1) Candidatos según los archivos que mostraste (intentamos en este orden):\n",
    "train_candidates = [\n",
    "    \"windows_aligned_normal.parquet\",   # ventanas normales alineadas (preferido)\n",
    "    \"norm_windows_flat.parquet\",        # normales flatten\n",
    "    \"ais_norm_windows.parquet\",         # normales (nombre alterno)\n",
    "]\n",
    "eval_candidates = [\n",
    "    \"windows_with_labels_aligned.parquet\",  # eval con labels en el mismo parquet (preferido)\n",
    "    \"eval_windows_aligned.parquet\",         # eval ventanas alineadas (labels aparte)\n",
    "    \"windows_with_labels.parquet\"           # eval con labels pero no alineadas (fallback)\n",
    "]\n",
    "label_candidates = [\n",
    "    \"eval_labels_aligned.parquet\",      # etiquetas para eval alineadas 1:1\n",
    "    \"labels.parquet\",                   # etiquetas generales (fallback)\n",
    "]\n",
    "\n",
    "def first_existing(base, names):\n",
    "    for n in names:\n",
    "        p = base / n\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "train_path = first_existing(DATA_DIR, train_candidates)\n",
    "eval_path  = first_existing(DATA_DIR, eval_candidates)\n",
    "y_path     = first_existing(DATA_DIR, label_candidates)\n",
    "\n",
    "if train_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No encontré parquet de TRAIN normal en {DATA_DIR}. \"\n",
    "        f\"Busqué: {train_candidates}\"\n",
    "    )\n",
    "if eval_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No encontré parquet de EVAL en {DATA_DIR}. \"\n",
    "        f\"Busqué: {eval_candidates}\"\n",
    "    )\n",
    "\n",
    "print(\"TRAIN windows  ->\", train_path.name)\n",
    "print(\"EVAL windows   ->\", eval_path.name)\n",
    "print(\"EVAL labels    ->\", y_path.name if y_path else \"(embebidas o no requeridas)\")\n",
    "\n",
    "# 2) Lectura con columnas mínimas y downcast\n",
    "def read_parquet_min(path):\n",
    "    # Leer todo y luego seleccionar; muchos artefactos vienen ya con columnas limpias\n",
    "    df = pd.read_parquet(path, engine=\"pyarrow\")\n",
    "    # downcast\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_float_dtype(df[c]):\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "        elif pd.api.types.is_integer_dtype(df[c]):\n",
    "            if c.lower() in (\"mmsi\",\"group\",\"idx_end\",\"idx\",\"id\"):\n",
    "                # mmsi suele caber en int32; group/idx/ids en int32/int64\n",
    "                if df[c].max() <= np.iinfo(np.int32).max:\n",
    "                    df[c] = df[c].astype(np.int32)\n",
    "            else:\n",
    "                df[c] = df[c].astype(np.int16)\n",
    "    return df\n",
    "\n",
    "df_tr = read_parquet_min(train_path)\n",
    "df_ev = read_parquet_min(eval_path)\n",
    "\n",
    "# 3) Detectar columnas especiales\n",
    "def detect_label_col(df):\n",
    "    for k in [\"y\", \"label\", \"is_suspicious\", \"target\"]:\n",
    "        if k in df.columns:\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "def detect_group_col(df):\n",
    "    for k in [\"mmsi\",\"group\",\"ship_id\"]:\n",
    "        if k in df.columns:\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "ycol_train = detect_label_col(df_tr)  # normalmente NO debería haber label en train normal\n",
    "ycol_eval  = detect_label_col(df_ev)\n",
    "gcol_train = detect_group_col(df_tr)\n",
    "gcol_eval  = detect_group_col(df_ev)\n",
    "\n",
    "# 4) Construir X_train (solo normales) y groups_train\n",
    "#    Asumimos que df_tr ya contiene SOLO ventanas normales (por el nombre).\n",
    "#    Si por alguna razón hay label, lo ignoramos para train.\n",
    "drop_cols_common = {\"lat\",\"lon\",\"idx\",\"idx_end\",\"window_id\"}\n",
    "drop_cols_train = set([c for c in [ycol_train, gcol_train] if c is not None]) | drop_cols_common\n",
    "feature_cols_tr = [c for c in df_tr.columns if c not in drop_cols_train]\n",
    "\n",
    "X_train = df_tr[feature_cols_tr].to_numpy(dtype=np.float32)\n",
    "groups_train = df_tr[gcol_train].to_numpy() if gcol_train else None\n",
    "\n",
    "# 5) Construir X_eval y y_eval\n",
    "if ycol_eval is not None:\n",
    "    # Etiquetas vienen dentro del mismo parquet de eval\n",
    "    drop_cols_eval = set([c for c in [ycol_eval, gcol_eval] if c is not None]) | drop_cols_common\n",
    "    feature_cols_ev = [c for c in df_ev.columns if c not in drop_cols_eval]\n",
    "    X_eval = df_ev[feature_cols_ev].to_numpy(dtype=np.float32)\n",
    "    y_eval = df_ev[ycol_eval].astype(np.int8).to_numpy()\n",
    "    groups_eval = df_ev[gcol_eval].to_numpy() if gcol_eval else None\n",
    "else:\n",
    "    # Etiquetas separadas\n",
    "    if y_path is None:\n",
    "        raise FileNotFoundError(\n",
    "            \"No encontré etiquetas para EVAL. Revisa si tienes windows_with_labels_aligned.parquet \"\n",
    "            \"o provee eval_labels_aligned.parquet / labels.parquet.\"\n",
    "        )\n",
    "    df_y = read_parquet_min(y_path)\n",
    "    # Detectar columna de etiqueta en el archivo de labels\n",
    "    ycol_y = detect_label_col(df_y) or (\"is_suspicious\" if \"is_suspicious\" in df_y.columns else None)\n",
    "    if ycol_y is None:\n",
    "        # fallback: si el archivo de labels es una serie de 0/1 con nombre genérico\n",
    "        ycol_y = df_y.select_dtypes(include=[\"int16\",\"int32\",\"int64\",\"int8\"]).columns[-1]\n",
    "    # Alineación: asumimos orden 1:1 (por eso usamos artefactos *_aligned*)\n",
    "    if len(df_y) != len(df_ev):\n",
    "        raise ValueError(f\"Desalineación: len(eval)={len(df_ev)} vs len(labels)={len(df_y)}\")\n",
    "    drop_cols_eval = set([gcol_eval]) | drop_cols_common\n",
    "    feature_cols_ev = [c for c in df_ev.columns if c not in drop_cols_eval]\n",
    "    X_eval = df_ev[feature_cols_ev].to_numpy(dtype=np.float32)\n",
    "    y_eval = df_y[ycol_y].astype(np.int8).to_numpy()\n",
    "    groups_eval = df_ev[gcol_eval].to_numpy() if gcol_eval else None\n",
    "\n",
    "# 6) Limpieza y reporte\n",
    "del df_tr; gc.collect()\n",
    "print(\"Train  -> X:\", X_train.shape, \"| groups:\", None if groups_train is None else len(groups_train))\n",
    "print(\"Eval   -> X:\", X_eval.shape,  \"| y:\", y_eval.shape, \"| groups:\", None if groups_eval is None else len(groups_eval))\n",
    "print(\"N feats (train):\", X_train.shape[1], \"| N feats (eval):\", X_eval.shape[1])\n",
    "\n",
    "# 7) Persistimos nombres de features para trazabilidad\n",
    "CFG[\"feature_cols_train\"] = feature_cols_tr\n",
    "CFG[\"feature_cols_eval\"]  = feature_cols_ev\n",
    "\n",
    "# 8) (Opcional) reducción extra de memoria antes de escalar/modelar\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31059658",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m T \u001b[38;5;241m=\u001b[39m CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]; step \u001b[38;5;241m=\u001b[39m CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Construimos ventanas en TODO el dataset — etiquetamos por la proporción de puntos sospechosos dentro de la ventana\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Para entrenamiento, luego filtramos a ventanas completamente normales (o con umbral bajo de sospechosos)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m X_all, groups_all, idx_rows_all \u001b[38;5;241m=\u001b[39m build_windows(\u001b[43mdf\u001b[49m, CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmsi_col\u001b[39m\u001b[38;5;124m\"\u001b[39m], CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp_col\u001b[39m\u001b[38;5;124m\"\u001b[39m], feature_cols, T, step)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Derivamos una etiqueta de ventana basada en 'is_suspicious' de los puntos contenidos\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Truco: usamos el índice de fila terminal de cada ventana para recuperar MMSI y tiempo aproximado para análisis adicional\u001b[39;00m\n\u001b[1;32m      9\u001b[0m y_point \u001b[38;5;241m=\u001b[39m df[CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_filter_col\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mto_numpy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "T = CFG[\"window_size\"]; step = CFG[\"step_size\"]\n",
    "\n",
    "# Construimos ventanas en TODO el dataset — etiquetamos por la proporción de puntos sospechosos dentro de la ventana\n",
    "# Para entrenamiento, luego filtramos a ventanas completamente normales (o con umbral bajo de sospechosos)\n",
    "X_all, groups_all, idx_rows_all = build_windows(df, CFG[\"mmsi_col\"], CFG[\"timestamp_col\"], feature_cols, T, step)\n",
    "\n",
    "# Derivamos una etiqueta de ventana basada en 'is_suspicious' de los puntos contenidos\n",
    "# Truco: usamos el índice de fila terminal de cada ventana para recuperar MMSI y tiempo aproximado para análisis adicional\n",
    "y_point = df[CFG[\"train_filter_col\"]].fillna(0).astype(int).to_numpy()\n",
    "\n",
    "# Para cada ventana, estimamos si contiene algún punto sospechoso dentro del rango [end-T+1, end]\n",
    "# Como no guardamos los índices de todos los puntos, aproximamos con una rolling sobre la serie original\n",
    "# Alternativa: reconstruir mejor el mapping índice->ventana si es necesario.\n",
    "# Aquí, simplificamos: una ventana es \"sospechosa\" si el punto terminal (end) es sospechoso (proxy razonable).\n",
    "y_win = y_point[idx_rows_all] if len(idx_rows_all) else np.array([], dtype=int)\n",
    "\n",
    "print(\"Ventanas totales:\", X_all.shape, \"Sospechosas (proxy):\", int(y_win.sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced03ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filtramos ventanas de entrenamiento: solo NORMAL (proxy)\n",
    "train_mask = (y_win == 0)\n",
    "X_train = X_all[train_mask]\n",
    "groups_train = groups_all[train_mask]\n",
    "\n",
    "# Para evaluación mantendremos todas las ventanas\n",
    "X_eval = X_all\n",
    "y_eval = y_win\n",
    "groups_eval = groups_all\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"X_eval:\", X_eval.shape)\n",
    "\n",
    "# Escalado: fit en TRAIN NORMAL, transform en ambos\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_eval_sc = scaler.transform(X_eval)\n",
    "\n",
    "# Guardamos scaler\n",
    "with open(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_scaler.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28979fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Buscamos hiperparámetros por pseudo-validación \"contaminada\" mínima:\n",
    "# Al no tener negativos en train, usamos la estabilidad de scores y ratio de outliers producidos como heurística.\n",
    "# Alternativa: usar un hold-out de ventanas con baja proporción (o 0) de sospechosos para early stopping.\n",
    "param_grid = list(ParameterGrid({\n",
    "    \"nu\": CFG[\"svm_nu_grid\"],\n",
    "    \"gamma\": CFG[\"svm_gamma_grid\"]\n",
    "}))\n",
    "\n",
    "def outlier_rate(scores):\n",
    "    # decision_function: mayores -> más normal. score_samples similar. Aquí usamos predicciones del modelo.\n",
    "    # Usaremos 'predict' que devuelve +1 normal, -1 outlier.\n",
    "    neg = (scores == -1).mean()\n",
    "    return float(neg)\n",
    "\n",
    "best_cfg = None\n",
    "best_obj = None  # minimizamos desviación del target_outlier_rate (heurística) y varianza entre folds\n",
    "target_outlier_rate = 0.05\n",
    "\n",
    "gkf = GroupKFold(n_splits=min(CFG[\"kfold_splits\"], len(np.unique(groups_train))))\n",
    "results = []\n",
    "\n",
    "for p in param_grid:\n",
    "    fold_rates = []\n",
    "    for tr_idx, va_idx in gkf.split(X_train_sc, groups=groups_train):\n",
    "        Xtr, Xva = X_train_sc[tr_idx], X_train_sc[va_idx]\n",
    "        model = OneClassSVM(kernel=CFG[\"kernel\"], nu=p[\"nu\"], gamma=p[\"gamma\"])\n",
    "        model.fit(Xtr)\n",
    "        pred = model.predict(Xva)  # +1 normal, -1 outlier\n",
    "        rate = outlier_rate(pred)\n",
    "        fold_rates.append(rate)\n",
    "    rate_mean = float(np.mean(fold_rates))\n",
    "    rate_std = float(np.std(fold_rates))\n",
    "    obj = abs(rate_mean - target_outlier_rate) + rate_std\n",
    "    results.append({\"params\": p, \"rate_mean\": rate_mean, \"rate_std\": rate_std, \"obj\": obj})\n",
    "    if best_obj is None or obj < best_obj:\n",
    "        best_obj = obj\n",
    "        best_cfg = p\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values(\"obj\")\n",
    "print(\"Top 5 configs by heuristic objective:\")\n",
    "display(res_df.head(5))\n",
    "print(\"Best params:\", best_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a0b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_model = OneClassSVM(kernel=CFG[\"kernel\"], nu=best_cfg[\"nu\"], gamma=best_cfg[\"gamma\"])\n",
    "final_model.fit(X_train_sc)\n",
    "\n",
    "# Scores en eval\n",
    "# decision_function: valores altos = más normal. Convertimos a \"anomaly_score\" = -decision\n",
    "decision = final_model.decision_function(X_eval_sc)\n",
    "anomaly_score = -decision\n",
    "\n",
    "# Curvas y métricas (necesita y_eval binaria; 1=sospechoso)\n",
    "roc = roc_auc_score(y_eval, anomaly_score) if len(np.unique(y_eval)) > 1 else np.nan\n",
    "ap = average_precision_score(y_eval, anomaly_score) if len(np.unique(y_eval)) > 1 else np.nan\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_eval, anomaly_score)\n",
    "pr_auc = auc(rec, prec) if len(rec) > 1 else np.nan\n",
    "\n",
    "print(f\"ROC-AUC: {roc:.4f} | PR-AUC: {pr_auc:.4f} | AP: {ap:.4f}\")\n",
    "\n",
    "# Guardamos artefactos\n",
    "with open(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "with open(os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_config.json\"), \"w\") as f:\n",
    "    json.dump(CFG | {\"best_params\": best_cfg, \"metrics\": {\"roc_auc\": float(roc), \"pr_auc\": float(pr_auc), \"ap\": float(ap)}}, f, indent=2)\n",
    "\n",
    "# Guardamos scores por ventana\n",
    "out_df = pd.DataFrame({\n",
    "    \"anomaly_score\": anomaly_score,\n",
    "    \"y_eval\": y_eval.astype(int),\n",
    "    \"mmsi\": groups_eval,\n",
    "    \"idx_end\": idx_rows_all\n",
    "})\n",
    "out_path = os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_eval_scores.parquet\")\n",
    "out_df.to_parquet(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560672e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Plots ---\n",
    "# 1) Precision-Recall curve\n",
    "plt.figure()\n",
    "plt.plot(rec, prec, linewidth=2)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall curve (Eval)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 2) Score distributions por etiqueta\n",
    "plt.figure()\n",
    "plt.hist(anomaly_score[y_eval==0], bins=50, alpha=0.6, label=\"Normal (win)\")\n",
    "plt.hist(anomaly_score[y_eval==1], bins=50, alpha=0.6, label=\"Sospechosa (win)\")\n",
    "plt.xlabel(\"Anomaly score (−decision)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribución de scores por etiqueta (Eval)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec85bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Selección de umbral por top‑k (p.ej., 1% de ventanas más anómalas)\n",
    "k_rate = 0.01\n",
    "k = max(1, int(len(anomaly_score) * k_rate))\n",
    "thr_k = np.partition(anomaly_score, -k)[-k]\n",
    "pred_k = (anomaly_score >= thr_k).astype(int)  # 1 = anómala\n",
    "\n",
    "# Métricas @k\n",
    "tp = int(((pred_k==1) & (y_eval==1)).sum())\n",
    "fp = int(((pred_k==1) & (y_eval==0)).sum())\n",
    "fn = int(((pred_k==0) & (y_eval==1)).sum())\n",
    "prec_k = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "rec_k  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "f1_k   = 2*prec_k*rec_k/(prec_k+rec_k) if (prec_k+rec_k)>0 else 0.0\n",
    "\n",
    "print(f\"@k={k_rate*100:.1f}% -> P: {prec_k:.3f} | R: {rec_k:.3f} | F1: {f1_k:.3f}  (k={k})\")\n",
    "\n",
    "# Guardamos predicciones @k\n",
    "out_topk = out_df.copy()\n",
    "out_topk[\"pred_topk\"] = pred_k\n",
    "out_topk_path = os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_preds_topk.parquet\")\n",
    "out_topk.to_parquet(out_topk_path, index=False)\n",
    "print(\"Saved:\", out_topk_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea68340",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Agregamos por mmsi para ver tasa de detección por trayectoria (porcentaje de ventanas anómalas)\n",
    "agg = out_df.assign(anom=(anomaly_score>=thr_k).astype(int)).groupby(\"mmsi\").agg(\n",
    "    n_win=(\"anomaly_score\", \"size\"),\n",
    "    anom_win=(\"anom\", \"sum\"),\n",
    "    mean_score=(\"anomaly_score\", \"mean\")\n",
    ").reset_index()\n",
    "agg[\"anom_rate\"] = agg[\"anom_win\"] / agg[\"n_win\"]\n",
    "display(agg.sort_values(\"anom_rate\", ascending=False).head(10))\n",
    "\n",
    "agg_path = os.path.join(CFG[\"out_dir\"], f\"{CFG['artifact_prefix']}_mmsi_agg.parquet\")\n",
    "agg.to_parquet(agg_path, index=False)\n",
    "print(\"Saved:\", agg_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50506455",
   "metadata": {},
   "source": [
    "\n",
    "## Cómo ejecutar con tus datos\n",
    "\n",
    "1. Asegúrate de colocar tu **Parquet enriquecido** en `data/ais_enriched.parquet` (o ajusta `CFG[\"input_parquet\"]`). Debe contener:\n",
    "   - Identificador: `mmsi`\n",
    "   - Tiempo: `timestamp` (timezone aware)\n",
    "   - Variables base: `lat`, `lon`, `speed`, `course`, `depth`, `dist_coast_km`, `dist_port_km`\n",
    "   - Etiqueta de validación: `is_suspicious` (0/1), solo usada para evaluación.\n",
    "\n",
    "2. Ajusta tamaños de **ventana `T`** y `step` en `CFG`.\n",
    "\n",
    "3. Ejecuta todas las celdas. Los artefactos se guardarán en `runs/ocsvm/`:\n",
    "   - `*_scaler.pkl`, `*_model.pkl`, `*_config.json`\n",
    "   - `*_eval_scores.parquet`, `*_preds_topk.parquet`, `*_mmsi_agg.parquet`\n",
    "\n",
    "4. Opcional: Integra mapas (GeoPandas + shapely) para estudiar casos dentro/de fuera de la RMG y generar figuras para el informe.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
