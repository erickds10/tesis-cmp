{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97be90a6",
   "metadata": {},
   "source": [
    "# One-Class SVM (OC-SVM) — AIS Anomaly Detection (Galápagos)\n",
    "\n",
    "**Objetivo:** Entrenar un modelo no supervisado (OC-SVM RBF) usando **ventanas pre-generadas** normal (train) y con etiquetas (eval). Evaluación por **lotes** (memmap) para evitar picos de memoria; artefactos bajo `./data/ocsvm_runs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06621de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output root: /teamspace/studios/this_studio/data\n"
     ]
    }
   ],
   "source": [
    "import os, json, numpy as np\n",
    "SAVE_ROOT = 'data'\n",
    "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
    "print('Output root:', os.path.abspath(SAVE_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a069d364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {\n",
      "  \"input_parquet\": \"/teamspace/studios/profound-silver-kn8tf/ais_anomaly/data/ais_enriched.parquet\",\n",
      "  \"train_filter_col\": \"is_suspicious\",\n",
      "  \"mmsi_col\": \"mmsi\",\n",
      "  \"timestamp_col\": \"timestamp\",\n",
      "  \"svm_nu_grid\": [\n",
      "    0.01,\n",
      "    0.05,\n",
      "    0.1\n",
      "  ],\n",
      "  \"svm_gamma_grid\": [\n",
      "    \"scale\",\n",
      "    0.01\n",
      "  ],\n",
      "  \"kernel\": \"rbf\",\n",
      "  \"kfold_splits\": 5,\n",
      "  \"max_train_samples\": 500000,\n",
      "  \"max_search_samples\": 200000,\n",
      "  \"eval_batch_size\": 200000,\n",
      "  \"out_dir\": \"data/ocsvm_runs\",\n",
      "  \"artifact_prefix\": \"ocsvm_rbf\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os, gc, json, pickle, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import GroupKFold, KFold, ParameterGrid\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "\n",
    "import random\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "CFG = {\n",
    "  'input_parquet': '/teamspace/studios/profound-silver-kn8tf/ais_anomaly/data/ais_enriched.parquet',\n",
    "  'train_filter_col': 'is_suspicious',\n",
    "  'mmsi_col': 'mmsi',\n",
    "  'timestamp_col': 'timestamp',\n",
    "  'svm_nu_grid': [0.01, 0.05, 0.1],\n",
    "  'svm_gamma_grid': ['scale', 0.01],\n",
    "  'kernel': 'rbf',\n",
    "  'kfold_splits': 5,\n",
    "  'max_train_samples': 500_000,\n",
    "  'max_search_samples': 200_000,\n",
    "  'eval_batch_size': 200_000,\n",
    "  'out_dir': 'data/ocsvm_runs',\n",
    "  'artifact_prefix': 'ocsvm_rbf'\n",
    "}\n",
    "os.makedirs(CFG['out_dir'], exist_ok=True)\n",
    "print('Config:', json.dumps(CFG, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffa17cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dir: data/ocsvm_runs\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "inp = Path(CFG['input_parquet']).resolve()\n",
    "out = Path(CFG['out_dir']).resolve()\n",
    "if str(out).startswith(str(inp.parent)):\n",
    "    print('[SAFETY] Rerouting outputs to ./data/ocsvm_runs')\n",
    "    CFG['out_dir'] = 'data/ocsvm_runs'\n",
    "os.makedirs(CFG['out_dir'], exist_ok=True)\n",
    "print('Output dir:', CFG['out_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d60da9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXTERNAL_DATA_DIR (read-only): /teamspace/studios/profound-silver-kn8tf/ais_anomaly/data\n",
      "LOCAL OUT_DIR (safe writes):   /teamspace/studios/this_studio/data/ocsvm_runs\n",
      "Config: {\n",
      "  \"svm_nu_grid\": [\n",
      "    0.01,\n",
      "    0.05,\n",
      "    0.1\n",
      "  ],\n",
      "  \"svm_gamma_grid\": [\n",
      "    \"scale\",\n",
      "    0.01\n",
      "  ],\n",
      "  \"kernel\": \"rbf\",\n",
      "  \"kfold_splits\": 5,\n",
      "  \"max_train_samples\": 500000,\n",
      "  \"max_search_samples\": 200000,\n",
      "  \"eval_batch_size\": 200000,\n",
      "  \"out_dir\": \"/teamspace/studios/this_studio/data/ocsvm_runs\",\n",
      "  \"artifact_prefix\": \"ocsvm_rbf\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- Setup & Config (reemplazo) ---\n",
    "import os, json, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Ruta del OTRO workspace (solo lectura)\n",
    "EXTERNAL_DATA_DIR = Path(\"/teamspace/studios/profound-silver-kn8tf/ais_anomaly/data\").resolve()\n",
    "\n",
    "# Carpeta local del proyecto actual (escritura segura para OC-SVM)\n",
    "LOCAL_OUT_DIR = Path(\"data/ocsvm_runs\").resolve()\n",
    "os.makedirs(LOCAL_OUT_DIR, exist_ok=True)\n",
    "\n",
    "CFG = {\n",
    "    # Entradas (solo para trazabilidad, NO escribimos ahí)\n",
    "    \"external_data_dir\": str(EXTERNAL_DATA_DIR),\n",
    "\n",
    "    # Hiperparámetros del SVM\n",
    "    \"svm_nu_grid\": [0.01, 0.05, 0.1],\n",
    "    \"svm_gamma_grid\": [\"scale\", 0.01],\n",
    "    \"kernel\": \"rbf\",\n",
    "    \"kfold_splits\": 5,\n",
    "\n",
    "    # Muestreos y lotes\n",
    "    \"max_train_samples\": 500_000,    # cap de entrenamiento\n",
    "    \"max_search_samples\": 200_000,   # cap para HP search\n",
    "    \"eval_batch_size\": 200_000,      # lotes para eval\n",
    "\n",
    "    # Salidas (siempre al proyecto actual)\n",
    "    \"out_dir\": str(LOCAL_OUT_DIR),\n",
    "    \"artifact_prefix\": \"ocsvm_rbf\"\n",
    "}\n",
    "\n",
    "print(\"EXTERNAL_DATA_DIR (read-only):\", CFG[\"external_data_dir\"])\n",
    "print(\"LOCAL OUT_DIR (safe writes):  \", CFG[\"out_dir\"])\n",
    "print(\"Config:\", json.dumps({k: v for k, v in CFG.items() if k not in [\"external_data_dir\"]}, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65f8d002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: /teamspace/studios/profound-silver-kn8tf/ais_anomaly/data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "La ruta externa no existe: /teamspace/studios/profound-silver-kn8tf/ais_anomaly/data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATA_DIR:\u001b[39m\u001b[38;5;124m\"\u001b[39m, DATA_DIR)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DATA_DIR\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLa ruta externa no existe: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Listado informativo (por si toca debug)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m parquets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m DATA_DIR\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)], key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m p: p\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mlower())\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: La ruta externa no existe: /teamspace/studios/profound-silver-kn8tf/ais_anomaly/data"
     ]
    }
   ],
   "source": [
    "# --- Carga ROBUSTA de artefactos de ventanas (del otro workspace) ---\n",
    "import os, gc, re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(CFG[\"external_data_dir\"])\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"La ruta externa no existe: {DATA_DIR}\")\n",
    "\n",
    "# Listado informativo (por si toca debug)\n",
    "parquets = sorted([p for p in DATA_DIR.glob(\"*.parquet\")], key=lambda p: p.name.lower())\n",
    "print(\"Parquets disponibles:\")\n",
    "for p in parquets:\n",
    "    try:\n",
    "        sz = p.stat().st_size/1e6\n",
    "        print(f\" - {p.name}  ({sz:.1f} MB)\")\n",
    "    except Exception:\n",
    "        print(f\" - {p.name}\")\n",
    "\n",
    "# Candidatos por prioridad (exactos y patrones)\n",
    "TRAIN_EXACT = [\n",
    "    \"windows_aligned_normal.parquet\",\n",
    "    \"norm_windows_flat.parquet\",\n",
    "    \"ais_norm_windows.parquet\",\n",
    "]\n",
    "EVAL_EXACT  = [\n",
    "    \"windows_with_labels_aligned.parquet\",\n",
    "    \"eval_windows_aligned.parquet\",\n",
    "    \"windows_with_labels.parquet\",\n",
    "]\n",
    "LABEL_EXACT = [\n",
    "    \"eval_labels_aligned.parquet\",\n",
    "    \"labels.parquet\",\n",
    "    \"labels_anom.parquet\",\n",
    "]\n",
    "\n",
    "TRAIN_PATTERNS = [\n",
    "    \"*windows_aligned_normal*.parquet\",\n",
    "    \"*norm*_windows*.parquet\",\n",
    "    \"*ais_norm_windows*.parquet\",\n",
    "]\n",
    "EVAL_PATTERNS = [\n",
    "    \"*windows_with_labels_aligned*.parquet\",\n",
    "    \"*eval_windows_aligned*.parquet\",\n",
    "    \"*windows_with_labels*.parquet\",\n",
    "]\n",
    "LABEL_PATTERNS = [\n",
    "    \"*eval_labels_aligned*.parquet\",\n",
    "    \"labels*.parquet\",\n",
    "]\n",
    "\n",
    "def pick_file(base: Path, exact_list, pattern_list, purpose):\n",
    "    \"\"\"Primero intenta coincidencias exactas; si no, busca por patrón.\n",
    "       Si hay varias coincidencias, prioriza por orden y luego por tamaño.\"\"\"\n",
    "    # 1) Exactos en orden\n",
    "    for name in exact_list:\n",
    "        p = base / name\n",
    "        if p.exists():\n",
    "            return p\n",
    "\n",
    "    # 2) Patrones en orden; si múltiples, elige el más grande\n",
    "    for patt in pattern_list:\n",
    "        matches = list(base.glob(patt))\n",
    "        if matches:\n",
    "            # tamaño desc\n",
    "            matches = sorted(matches, key=lambda x: x.stat().st_size if x.exists() else 0, reverse=True)\n",
    "            print(f\"[pick_file:{purpose}] Elegido por patrón '{patt}':\", matches[0].name)\n",
    "            return matches[0]\n",
    "    return None\n",
    "\n",
    "train_path = pick_file(DATA_DIR, TRAIN_EXACT, TRAIN_PATTERNS, \"train\")\n",
    "eval_path  = pick_file(DATA_DIR, EVAL_EXACT,  EVAL_PATTERNS,  \"eval\")\n",
    "y_path     = pick_file(DATA_DIR, LABEL_EXACT, LABEL_PATTERNS, \"labels\")\n",
    "\n",
    "if train_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"No encontré parquet de TRAIN (normal). \"\n",
    "        f\"Busqué exactos {TRAIN_EXACT} y patrones {TRAIN_PATTERNS} en {DATA_DIR}\"\n",
    "    )\n",
    "if eval_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"No encontré parquet de EVAL (ventanas). \"\n",
    "        f\"Busqué exactos {EVAL_EXACT} y patrones {EVAL_PATTERNS} en {DATA_DIR}\"\n",
    "    )\n",
    "\n",
    "print(\"TRAIN windows ->\", train_path.name)\n",
    "print(\"EVAL windows  ->\", eval_path.name)\n",
    "print(\"EVAL labels   ->\", y_path.name if y_path else \"(embebidas)\")\n",
    "\n",
    "def read_parquet_min(path: Path):\n",
    "    df = pd.read_parquet(path, engine=\"pyarrow\")\n",
    "    # downcast básico\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_float_dtype(df[c]): df[c] = df[c].astype(np.float32)\n",
    "        elif pd.api.types.is_integer_dtype(df[c]) and df[c].max() <= np.iinfo(np.int32).max:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "    return df\n",
    "\n",
    "df_tr = read_parquet_min(train_path)\n",
    "df_ev = read_parquet_min(eval_path)\n",
    "\n",
    "def detect_label_col(df):\n",
    "    for k in [\"y\",\"label\",\"is_suspicious\",\"target\"]:\n",
    "        if k in df.columns: return k\n",
    "    return None\n",
    "\n",
    "def detect_group_col(df):\n",
    "    for k in [\"mmsi\",\"group\",\"ship_id\"]:\n",
    "        if k in df.columns: return k\n",
    "    return None\n",
    "\n",
    "ycol_train = detect_label_col(df_tr)   # train normal no debería tener; si tiene, se ignora\n",
    "ycol_eval  = detect_label_col(df_ev)\n",
    "gcol_train = detect_group_col(df_tr)\n",
    "gcol_eval  = detect_group_col(df_ev)\n",
    "\n",
    "# columnas que NO van a X (coordenadas crudas, índices auxiliares, ids de ventana)\n",
    "drop_common = {\"lat\",\"lon\",\"idx\",\"idx_end\",\"window_id\"}\n",
    "\n",
    "# ---- TRAIN (solo normales) ----\n",
    "drop_train = set([c for c in [ycol_train, gcol_train] if c]) | drop_common\n",
    "feat_tr = [c for c in df_tr.columns if c not in drop_train]\n",
    "X_train = df_tr[feat_tr].to_numpy(dtype=np.float32)\n",
    "groups_train = df_tr[gcol_train].to_numpy() if gcol_train else None\n",
    "\n",
    "# ---- EVAL (con o sin etiquetas embebidas) ----\n",
    "if ycol_eval is not None:\n",
    "    drop_eval = set([c for c in [ycol_eval, gcol_eval] if c]) | drop_common\n",
    "    feat_ev = [c for c in df_ev.columns if c not in drop_eval]\n",
    "    X_eval = df_ev[feat_ev].to_numpy(dtype=np.float32)\n",
    "    y_eval = df_ev[ycol_eval].astype(np.int8).to_numpy()\n",
    "    groups_eval = df_ev[gcol_eval].to_numpy() if gcol_eval else None\n",
    "else:\n",
    "    if y_path is None:\n",
    "        raise FileNotFoundError(\n",
    "            \"No hay etiquetas para eval: esperaba windows_with_labels_aligned*.parquet \"\n",
    "            \"o eval_windows_aligned*.parquet + eval_labels_aligned*.parquet\"\n",
    "        )\n",
    "    df_y = read_parquet_min(y_path)\n",
    "    # detectar columna de etiqueta:\n",
    "    ycol_y = detect_label_col(df_y)\n",
    "    if ycol_y is None:\n",
    "        # fallback: última entera\n",
    "        ints = df_y.select_dtypes(include=[\"int32\",\"int16\",\"int8\"]).columns\n",
    "        if len(ints) == 0:\n",
    "            raise ValueError(\"No se detecta columna de etiqueta en el archivo de labels.\")\n",
    "        ycol_y = ints[-1]\n",
    "    if len(df_y) != len(df_ev):\n",
    "        raise ValueError(f\"Desalineación: len(eval)={len(df_ev)} vs len(labels)={len(df_y)}\")\n",
    "    drop_eval = set([gcol_eval]) | drop_common\n",
    "    feat_ev = [c for c in df_ev.columns if c not in drop_eval]\n",
    "    X_eval = df_ev[feat_ev].to_numpy(dtype=np.float32)\n",
    "    y_eval = df_y[ycol_y].astype(np.int8).to_numpy()\n",
    "    groups_eval = df_ev[gcol_eval].to_numpy() if gcol_eval else None\n",
    "\n",
    "del df_tr; gc.collect()\n",
    "print(\"Train -> X:\", X_train.shape, \"| groups:\", None if groups_train is None else len(groups_train))\n",
    "print(\"Eval  -> X:\", X_eval.shape,  \"| y:\", y_eval.shape, \"| groups:\", None if groups_eval is None else len(groups_eval))\n",
    "print(\"N feats (train):\", X_train.shape[1], \" | N feats (eval):\", X_eval.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c886b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, os\n",
    "\n",
    "def sample_by_group(n_max, X, groups):\n",
    "    if n_max is None or X.shape[0] <= n_max:\n",
    "        idx = np.arange(X.shape[0]); return X, (groups if groups is not None else None), idx\n",
    "    rng = np.random.default_rng(42)\n",
    "    if groups is None:\n",
    "        idx = rng.choice(X.shape[0], n_max, replace=False); return X[idx], None, idx\n",
    "    uniq = np.unique(groups); per_g = max(1, n_max // len(uniq)); take = []\n",
    "    for g in uniq:\n",
    "        g_idx = np.where(groups == g)[0]\n",
    "        if g_idx.size > per_g: take.extend(rng.choice(g_idx, per_g, replace=False).tolist())\n",
    "        else: take.extend(g_idx.tolist())\n",
    "    take = np.array(take)\n",
    "    if take.size > n_max: take = rng.choice(take, n_max, replace=False)\n",
    "    return X[take], groups[take], take\n",
    "\n",
    "def colwise_nanmedian(X):\n",
    "    Xc = X.copy(); Xc[~np.isfinite(Xc)] = np.nan\n",
    "    med = np.nanmedian(Xc, axis=0); med = np.where(np.isfinite(med), med, 0.0).astype(np.float32)\n",
    "    return med\n",
    "\n",
    "def impute_inplace(X, medians):\n",
    "    bad = ~np.isfinite(X)\n",
    "    if bad.any(): X[bad] = np.take(medians, np.where(bad)[1])\n",
    "\n",
    "def fit_standardizer(X):\n",
    "    mean = X.mean(axis=0).astype(np.float32)\n",
    "    var  = X.var(axis=0).astype(np.float32)\n",
    "    std  = np.sqrt(var, dtype=np.float32); std[std == 0.0] = 1.0\n",
    "    return mean, std\n",
    "\n",
    "def apply_standardizer_inplace(X, mean, std):\n",
    "    X -= mean; X /= std\n",
    "\n",
    "# 1) Sample train first\n",
    "X_train_sampled, groups_train_sampled, train_sel_idx = sample_by_group(CFG['max_train_samples'], X_train, groups_train)\n",
    "print('Train sampled:', X_train_sampled.shape, '| groups:', None if groups_train_sampled is None else len(groups_train_sampled))\n",
    "\n",
    "# 2) Impute & scale on sampled train\n",
    "X_train_sampled = X_train_sampled.astype(np.float32, copy=False)\n",
    "X_train_sampled[~np.isfinite(X_train_sampled)] = np.nan\n",
    "train_medians = colwise_nanmedian(X_train_sampled)\n",
    "impute_inplace(X_train_sampled, train_medians)\n",
    "train_mean, train_std = fit_standardizer(X_train_sampled)\n",
    "apply_standardizer_inplace(X_train_sampled, train_mean, train_std)\n",
    "\n",
    "X_train_sc = X_train_sampled\n",
    "groups_train = groups_train_sampled\n",
    "print('Scaled train shape:', X_train_sc.shape)\n",
    "\n",
    "# Save params\n",
    "os.makedirs(CFG['out_dir'], exist_ok=True)\n",
    "with open(os.path.join(CFG['out_dir'], f\"{CFG['artifact_prefix']}_imputer_medians.npy\"), 'wb') as f:\n",
    "    np.save(f, train_medians)\n",
    "with open(os.path.join(CFG['out_dir'], f\"{CFG['artifact_prefix']}_scaler_params.npz\"), 'wb') as f:\n",
    "    np.savez(f, mean=train_mean, std=train_std)\n",
    "\n",
    "def transform_eval_in_batches(X, batch_size=CFG['eval_batch_size']):\n",
    "    n = X.shape[0]\n",
    "    for s in range(0, n, batch_size):\n",
    "        e = min(s + batch_size, n)\n",
    "        Xe = X[s:e].astype(np.float32, copy=False)\n",
    "        Xe[~np.isfinite(Xe)] = np.nan\n",
    "        impute_inplace(Xe, train_medians)\n",
    "        apply_standardizer_inplace(Xe, train_mean, train_std)\n",
    "        yield s, e, Xe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1297cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "param_grid = list(ParameterGrid({'nu': CFG['svm_nu_grid'], 'gamma': CFG['svm_gamma_grid']}))\n",
    "target_outlier_rate = 0.05\n",
    "\n",
    "def build_search_subset(X, groups, n_max):\n",
    "    if n_max is None or X.shape[0] <= n_max: return X, groups\n",
    "    rng = np.random.default_rng(123)\n",
    "    if groups is None:\n",
    "        idx = rng.choice(X.shape[0], n_max, replace=False); return X[idx], None\n",
    "    uniq = np.unique(groups); per_g = max(1, n_max // len(uniq)); take = []\n",
    "    for g in uniq:\n",
    "        g_idx = np.where(groups == g)[0]\n",
    "        if g_idx.size > per_g: take.extend(rng.choice(g_idx, per_g, replace=False).tolist())\n",
    "        else: take.extend(g_idx.tolist())\n",
    "    take = np.array(take)\n",
    "    if take.size > n_max: take = rng.choice(take, n_max, replace=False)\n",
    "    return X[take], groups[take]\n",
    "\n",
    "X_search, groups_search = build_search_subset(X_train_sc, groups_train, CFG['max_search_samples'])\n",
    "\n",
    "if groups_search is not None and len(np.unique(groups_search)) >= 2:\n",
    "    n_splits = min(CFG['kfold_splits'], len(np.unique(groups_search)))\n",
    "    splitter = GroupKFold(n_splits=n_splits); split_args = dict(X=X_search, y=None, groups=groups_search)\n",
    "else:\n",
    "    n_splits = max(2, CFG['kfold_splits'])\n",
    "    splitter = KFold(n_splits=n_splits, shuffle=True, random_state=42); split_args = dict(X=X_search, y=None)\n",
    "\n",
    "def outlier_rate(pred): return float((pred == -1).mean())\n",
    "\n",
    "best_cfg, best_obj, results = None, None, []\n",
    "for p in param_grid:\n",
    "    fold_rates = []\n",
    "    for tr_idx, va_idx in splitter.split(**split_args):\n",
    "        Xtr, Xva = X_search[tr_idx], X_search[va_idx]\n",
    "        m = OneClassSVM(kernel=CFG['kernel'], nu=p['nu'], gamma=p['gamma'])\n",
    "        m.fit(Xtr)\n",
    "        pred = m.predict(Xva)\n",
    "        fold_rates.append(outlier_rate(pred))\n",
    "    rate_mean, rate_std = float(np.mean(fold_rates)), float(np.std(fold_rates))\n",
    "    obj = abs(rate_mean - target_outlier_rate) + rate_std\n",
    "    results.append({'params': p, 'rate_mean': rate_mean, 'rate_std': rate_std, 'obj': obj})\n",
    "    if best_obj is None or obj < best_obj:\n",
    "        best_obj, best_cfg = obj, p\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values('obj')\n",
    "display(res_df.head(5))\n",
    "print('Best params:', best_cfg, '| splits:', n_splits, '| search_subset:', X_search.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = OneClassSVM(kernel=CFG['kernel'], nu=best_cfg['nu'], gamma=best_cfg['gamma'])\n",
    "final_model.fit(X_train_sc)\n",
    "print('Final model trained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9096ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, os, json, pickle\n",
    "\n",
    "n_eval = X_eval.shape[0]\n",
    "scores_path_mm = os.path.join(CFG['out_dir'], f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\")\n",
    "anomaly_scores_mm = np.memmap(scores_path_mm, dtype=np.float32, mode='w+', shape=(n_eval,))\n",
    "\n",
    "for s, e, Xe_sc in transform_eval_in_batches(X_eval):\n",
    "    anomaly_scores_mm[s:e] = -final_model.decision_function(Xe_sc)\n",
    "\n",
    "scores = anomaly_scores_mm\n",
    "yb = y_eval.astype(int)\n",
    "if len(np.unique(yb)) > 1:\n",
    "    roc = roc_auc_score(yb, scores)\n",
    "    prec, rec, thr = precision_recall_curve(yb, scores)\n",
    "    pr_auc = auc(rec, prec)\n",
    "    ap = average_precision_score(yb, scores)\n",
    "else:\n",
    "    roc = pr_auc = ap = np.nan\n",
    "\n",
    "print(f'ROC-AUC: {roc:.4f} | PR-AUC: {pr_auc:.4f} | AP: {ap:.4f}')\n",
    "\n",
    "with open(os.path.join(CFG['out_dir'], f\"{CFG['artifact_prefix']}_model.pkl\"), 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "with open(os.path.join(CFG['out_dir'], f\"{CFG['artifact_prefix']}_config.json\"), 'w') as f:\n",
    "    json.dump(CFG | {'best_params': best_cfg, 'metrics': {'roc_auc': float(roc), 'pr_auc': float(pr_auc), 'ap': float(ap)}}, f, indent=2)\n",
    "\n",
    "print('Memmap scores stored at:', scores_path_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5136f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, os\n",
    "\n",
    "scores = np.memmap(os.path.join(CFG['out_dir'], f\"{CFG['artifact_prefix']}_eval_scores_mm.dat\"),\n",
    "                   dtype=np.float32, mode='r', shape=(X_eval.shape[0],))\n",
    "\n",
    "k_rate = 0.01\n",
    "k = max(1, int(len(scores) * k_rate))\n",
    "thr_k = np.partition(scores, -k)[-k]\n",
    "pred_topk = (scores >= thr_k).astype(np.int8)\n",
    "\n",
    "topk_idx = np.where(pred_topk == 1)[0]\n",
    "topk_df = pd.DataFrame({'idx': topk_idx.astype(np.int64),\n",
    "                        'anomaly_score': scores[topk_idx].astype(np.float32),\n",
    "                        'y_eval': y_eval[topk_idx].astype(np.int8)})\n",
    "if 'groups_eval' in globals() and groups_eval is not None:\n",
    "    topk_df['mmsi'] = groups_eval[topk_idx].astype(np.int64)\n",
    "topk_path = os.path.join(CFG['out_dir'], f\"{CFG['artifact_prefix']}_topk_{int(k_rate*100)}pct.parquet\")\n",
    "topk_df.to_parquet(topk_path, index=False)\n",
    "print('Saved TOP-K detailed:', topk_path, '| rows:', len(topk_df))\n",
    "\n",
    "if 'y_eval' in globals():\n",
    "    yb = y_eval.astype(int)\n",
    "    tp = int(((pred_topk==1) & (yb==1)).sum())\n",
    "    fp = int(((pred_topk==1) & (yb==0)).sum())\n",
    "    fn = int(((pred_topk==0) & (yb==1)).sum())\n",
    "    prec_k = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "    rec_k  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "    f1_k   = 2*prec_k*rec_k/(prec_k+rec_k) if (prec_k+rec_k)>0 else 0.0\n",
    "    print(f\"@k={k_rate*100:.1f}% -> P: {prec_k:.3f} | R: {rec_k:.3f} | F1: {f1_k:.3f}  (k={k})\")\n",
    "\n",
    "if 'groups_eval' in globals() and groups_eval is not None:\n",
    "    mmsi_all, n_by_mmsi = np.unique(groups_eval, return_counts=True)\n",
    "    mmsi_top, n_top_by_mmsi = np.unique(groups_eval[pred_topk==1], return_counts=True)\n",
    "    top_map = dict(zip(mmsi_top.tolist(), n_top_by_mmsi.tolist()))\n",
    "    anom_win = np.array([top_map.get(m, 0) for m in mmsi_all], dtype=np.int32)\n",
    "    anom_rate = anom_win / n_by_mmsi\n",
    "    agg_df = pd.DataFrame({'mmsi': mmsi_all, 'n_win': n_by_mmsi, 'anom_win': anom_win, 'anom_rate': anom_rate})\n",
    "    agg_path = os.path.join(CFG['out_dir'], f\"{CFG['artifact_prefix']}_mmsi_agg.parquet\")\n",
    "    agg_df.to_parquet(agg_path, index=False)\n",
    "    print('Saved MMSI agg:', agg_path)\n",
    "    display(agg_df.sort_values('anom_rate', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c80de",
   "metadata": {},
   "source": [
    "**Listo.** Corre las celdas en orden. Este cuaderno evita construir matrices gigantes para eval, usa memmap para scores, y guarda artefactos en `./data/ocsvm_runs`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
