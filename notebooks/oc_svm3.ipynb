{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OC-SVM v2 (Runtime-Capped) — AIS Anomaly\n",
    "**Objetivo**: maximizar CPU/RAM y limitar runtime con:\n",
    "- Subsampling controlado para HP search (FAST/FULL).\n",
    "- RobustScaler (menos sensible a outliers).\n",
    "- (Opcional) PCA 32D para acelerar kernel sin perder señal.\n",
    "- HP search paralela (joblib) con grid acotado eficaz.\n",
    "- Evaluación por lotes **reanudable** (memmap) + métricas y @k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: erickdsuarez10\n",
      "Host: computeinstance-e00xe50gspktqg7q39\n",
      "Python: /home/zeus/miniconda3/envs/cloudspace/bin/python\n",
      "CWD: /teamspace/studios/this_studio\n",
      "Mode: FAST_SAFE | Cores: 32 | Threads BLAS: 31 | RAM: 135.1 GB\n",
      "{\n",
      "  \"external_data_dir\": \"/teamspace/studios/this_studio/data\",\n",
      "  \"out_dir\": \"/teamspace/studios/this_studio/data/ocsvm_runs\",\n",
      "  \"artifact_prefix\": \"ocsvm_v2\",\n",
      "  \"kernel\": \"rbf\",\n",
      "  \"max_train_samples\": 120000,\n",
      "  \"max_search_samples\": 60000,\n",
      "  \"kfold_splits\": 3,\n",
      "  \"eval_batch_size\": 2000000,\n",
      "  \"use_pca\": true,\n",
      "  \"pca_components\": 16,\n",
      "  \"svm_nu_grid\": [\n",
      "    0.02,\n",
      "    0.03,\n",
      "    0.05,\n",
      "    0.08\n",
      "  ],\n",
      "  \"svm_gamma_grid\": [\n",
      "    \"auto\",\n",
      "    \"scale\",\n",
      "    0.1,\n",
      "    0.03\n",
      "  ],\n",
      "  \"per_model_cache_mb\": 2048,\n",
      "  \"n_jobs_folds\": 3\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# === Config dinámica consciente de RAM (reemplaza tu Celda 2) ===\n",
    "import os, sys, json, getpass, socket, psutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"User:\", getpass.getuser())\n",
    "print(\"Host:\", socket.gethostname())\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"CWD:\", os.getcwd())\n",
    "\n",
    "# MODO por defecto (rápido y seguro). Si todo va bien, puedes subir a \"FULL_SAFE\".\n",
    "MODE = \"FAST_SAFE\"   # \"FAST_SAFE\" o \"FULL_SAFE\"\n",
    "\n",
    "RAM_GB = psutil.virtual_memory().total / 1e9\n",
    "N_CPU  = os.cpu_count()\n",
    "\n",
    "MODES = {\n",
    "    \"FAST_SAFE\": dict(\n",
    "        max_train_samples   = 120_000,   # OC-SVM kernelizado no escala más sin cluster.\n",
    "        max_search_samples  = 60_000,\n",
    "        kfold_splits        = 3,\n",
    "        eval_batch_size     = 2_000_000,\n",
    "        use_pca             = True,\n",
    "        pca_components      = 16,        # reducimos para acelerar kernel\n",
    "        svm_nu_grid         = [0.02, 0.03, 0.05, 0.08],\n",
    "        svm_gamma_grid      = [\"auto\", \"scale\", 0.1, 0.03],\n",
    "    ),\n",
    "    \"FULL_SAFE\": dict(\n",
    "        max_train_samples   = 180_000,\n",
    "        max_search_samples  = 100_000,\n",
    "        kfold_splits        = 4,\n",
    "        eval_batch_size     = 2_000_000,\n",
    "        use_pca             = True,\n",
    "        pca_components      = 24,\n",
    "        svm_nu_grid         = [0.01, 0.02, 0.03, 0.05, 0.08],\n",
    "        svm_gamma_grid      = [\"auto\", \"scale\", 0.3, 0.1, 0.03, 0.01],\n",
    "    )\n",
    "}\n",
    "\n",
    "# Hilos BLAS (no procesos) para compartir memoria y evitar forks gigantes:\n",
    "N_THREADS = max(1, N_CPU - 1)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(N_THREADS)\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(N_THREADS)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(N_THREADS)\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(N_THREADS)\n",
    "\n",
    "# Rutas\n",
    "EXTERNAL_DATA_DIR = Path(\"/teamspace/studios/this_studio/data\").resolve()\n",
    "OUT_DIR = Path(\"data/ocsvm_runs\").resolve(); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CFG = {\n",
    "    \"external_data_dir\": str(EXTERNAL_DATA_DIR),\n",
    "    \"out_dir\": str(OUT_DIR),\n",
    "    \"artifact_prefix\": \"ocsvm_v2\",\n",
    "    \"kernel\": \"rbf\",\n",
    "    **MODES[MODE],\n",
    "}\n",
    "\n",
    "# Tamaño de caché por modelo (MB) controlado por RAM y n_splits (no por #CPU):\n",
    "# Usamos ~6% de RAM total repartida entre folds en paralelo; límite superior de 2048 MB por modelo.\n",
    "per_model_cache_mb = int(min(2048, (RAM_GB * 0.06 * 1024) / CFG[\"kfold_splits\"]))\n",
    "CFG[\"per_model_cache_mb\"] = max(256, per_model_cache_mb)  # no menos de 256MB\n",
    "\n",
    "# Concurrencia: paralelizamos sobre FOLDS (no sobre parámetros), para acotar el peak de RAM.\n",
    "# n_jobs_folds <= kfold_splits y <= 6 para no saturar.\n",
    "CFG[\"n_jobs_folds\"] = int(min(CFG[\"kfold_splits\"], 6))\n",
    "\n",
    "print(\"Mode:\", MODE, \"| Cores:\", N_CPU, \"| Threads BLAS:\", N_THREADS, \"| RAM:\", f\"{RAM_GB:.1f} GB\")\n",
    "print(json.dumps(CFG, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN -> windows_aligned_normal.parquet | X: (27789660, 19)\n",
      "EVAL -> windows_with_labels_aligned.parquet | X: (27789660, 19) | y from: labels_anom.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, gc, pyarrow.parquet as pq\n",
    "\n",
    "DATA_DIR = Path(CFG[\"external_data_dir\"])\n",
    "assert DATA_DIR.exists(), f\"No existe: {DATA_DIR}\"\n",
    "\n",
    "def is_valid_parquet(p: Path):\n",
    "    try: pq.ParquetFile(p); return True\n",
    "    except: return False\n",
    "\n",
    "def read_min(p: Path) -> pd.DataFrame:\n",
    "    df = pd.read_parquet(p, engine=\"pyarrow\")\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_float_dtype(df[c]): df[c] = df[c].astype(np.float32)\n",
    "        elif pd.api.types.is_integer_dtype(df[c]) and df[c].max() <= np.iinfo(np.int32).max:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "    return df\n",
    "\n",
    "def detect_col(df, cands):\n",
    "    for c in cands:\n",
    "        if c in df.columns: return c\n",
    "    return None\n",
    "\n",
    "def pick(base: Path, *names):\n",
    "    # exactos\n",
    "    for n in names:\n",
    "        if \"*\" not in n:\n",
    "            p = base / n\n",
    "            if p.exists() and is_valid_parquet(p): return p\n",
    "    # patrones\n",
    "    for patt in names:\n",
    "        if \"*\" in patt:\n",
    "            ms = sorted(base.glob(patt), key=lambda x: x.stat().st_size if x.exists() else 0, reverse=True)\n",
    "            for m in ms:\n",
    "                if is_valid_parquet(m): return m\n",
    "    return None\n",
    "\n",
    "# TRAIN (normal windows)\n",
    "train_path = pick(DATA_DIR,\n",
    "                  \"windows_aligned_normal.parquet\",\n",
    "                  \"norm_windows_flat.parquet\",\n",
    "                  \"*windows_with_labels_aligned_norm*.parquet\",\n",
    "                  \"*eval_windows_aligned_norm*.parquet\")\n",
    "assert train_path, \"No encontré TRAIN normal.\"\n",
    "df_tr = read_min(train_path)\n",
    "gcol_tr = detect_col(df_tr, [\"mmsi\",\"group\",\"ship_id\"])\n",
    "\n",
    "drop_common = {\"lat\",\"lon\",\"idx\",\"idx_end\",\"window_id\"}\n",
    "feat_tr = [c for c in df_tr.columns if c not in set([gcol_tr]) | drop_common]\n",
    "X_train = df_tr[feat_tr].to_numpy(dtype=np.float32)\n",
    "groups_train = df_tr[gcol_tr].to_numpy() if gcol_tr else None\n",
    "print(\"TRAIN ->\", train_path.name, \"| X:\", X_train.shape)\n",
    "\n",
    "# EVAL (preferimos single alineado con labels embebidas; si no, tomamos el mayor aligned)\n",
    "eval_single = pick(DATA_DIR,\n",
    "    \"windows_with_labels_aligned.parquet\",\n",
    "    \"*windows_with_labels_aligned*.parquet\",\n",
    "    \"eval_windows_aligned.parquet\",\n",
    "    \"*eval_windows_aligned*.parquet\",\n",
    ")\n",
    "assert eval_single, \"No encontré EVAL aligned.\"\n",
    "df_ev = read_min(eval_single)\n",
    "gcol_ev = detect_col(df_ev, [\"mmsi\",\"group\",\"ship_id\"])\n",
    "ycol_ev = detect_col(df_ev, [\"is_suspicious\",\"label\",\"y\",\"target\"])  # puede no existir\n",
    "\n",
    "feat_ev = [c for c in df_ev.columns if c not in set([gcol_ev, ycol_ev]) | drop_common if c is not None]\n",
    "X_eval = df_ev[feat_ev].to_numpy(dtype=np.float32)\n",
    "groups_eval = df_ev[gcol_ev].to_numpy() if gcol_ev else None\n",
    "\n",
    "# y_eval: usaremos labels_anom por JOIN (más confiable); para métricas\n",
    "labels_anom = pick(DATA_DIR, \"labels_anom.parquet\", \"*labels_anom*.parquet\")\n",
    "assert labels_anom, \"No encontré labels_anom.parquet (necesario para métricas).\"\n",
    "\n",
    "print(\"EVAL ->\", eval_single.name, \"| X:\", X_eval.shape, \"| y from:\", labels_anom.name)\n",
    "del df_tr; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search subset: (59950, 19)\n",
      "PCA aplicado: 16 componentes  →  16 features\n",
      "Train (scaled): (27789660, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Preprocesamiento RAM-friendly con imputación robusta + RobustScaler + (opcional) PCA ---\n",
    "import numpy as np, gc\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def naninf_to_nan(arr):\n",
    "    \"\"\"Reemplaza ±Inf por NaN in-place (float32 friendly).\"\"\"\n",
    "    if not np.issubdtype(arr.dtype, np.floating):\n",
    "        arr = arr.astype(np.float32, copy=False)\n",
    "    # ±Inf -> NaN\n",
    "    mask_inf = ~np.isfinite(arr)\n",
    "    if mask_inf.any():\n",
    "        arr[mask_inf] = np.nan\n",
    "    return arr\n",
    "\n",
    "def sample_by_group(n_max, X, groups):\n",
    "    if (n_max is None) or (X.shape[0] <= n_max):\n",
    "        idx = np.arange(X.shape[0]); return X, (groups if groups is not None else None), idx\n",
    "    rng = np.random.default_rng(42)\n",
    "    if groups is None:\n",
    "        idx = rng.choice(X.shape[0], n_max, replace=False); return X[idx], None, idx\n",
    "    uniq = np.unique(groups); per_g = max(1, n_max // len(uniq)); take=[]\n",
    "    for g in uniq:\n",
    "        gi = np.where(groups==g)[0]\n",
    "        take.extend(rng.choice(gi, min(per_g, gi.size), replace=False).tolist())\n",
    "    take = np.array(take)\n",
    "    if take.size > n_max: take = rng.choice(take, n_max, replace=False)\n",
    "    return X[take], groups[take], take\n",
    "\n",
    "# 0) Subset para HP search\n",
    "X_train_s, groups_train_s, idx_train_s = sample_by_group(CFG[\"max_search_samples\"], X_train, groups_train)\n",
    "print(\"Search subset:\", X_train_s.shape)\n",
    "\n",
    "# 1) Limpieza de ±Inf -> NaN (subset y full)\n",
    "X_train_s = naninf_to_nan(X_train_s.astype(np.float32, copy=False))\n",
    "X_train    = naninf_to_nan(X_train.astype(np.float32, copy=False))\n",
    "\n",
    "# 2) Imputación por mediana (fit en subset para ser eficiente/estable)\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_s = imputer.fit_transform(X_train_s)\n",
    "X_train_i = imputer.transform(X_train)        # full train imputado\n",
    "\n",
    "# 3) RobustScaler (fit en subset imputado; apply a todo)\n",
    "scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0))\n",
    "X_train_s = scaler.fit_transform(X_train_s)\n",
    "X_train_sc = scaler.transform(X_train_i)      # full train escalado\n",
    "\n",
    "# 4) (Opcional) PCA para acelerar kernel\n",
    "USE_PCA = bool(CFG.get(\"use_pca\", True))\n",
    "if USE_PCA:\n",
    "    # Ajuste automático al máximo permitido (no más de n_features)\n",
    "    n_comp = min(int(CFG[\"pca_components\"]), X_train_s.shape[1])\n",
    "    if n_comp < 2:\n",
    "        USE_PCA = False\n",
    "        print(\"⚠️ PCA deshabilitado: menos de 2 features útiles.\")\n",
    "    else:\n",
    "        pca = PCA(n_components=n_comp, svd_solver=\"auto\", random_state=42, whiten=False)\n",
    "        X_train_s = pca.fit_transform(X_train_s)\n",
    "        X_train_sc = pca.transform(X_train_sc)\n",
    "        print(f\"PCA aplicado: {n_comp} componentes  →  {X_train_sc.shape[1]} features\")\n",
    "else:\n",
    "    print(\"PCA desactivado.\")\n",
    "# 5) Transformador por lotes para EVAL (misma imputación/escala/PCA)\n",
    "def transform_eval_batches(X, batch_size:int):\n",
    "    n = X.shape[0]\n",
    "    for s in range(0, n, batch_size):\n",
    "        e = min(s+batch_size, n)\n",
    "        Xe = X[s:e].astype(np.float32, copy=False)\n",
    "        Xe = naninf_to_nan(Xe)          # ±Inf -> NaN\n",
    "        Xe = imputer.transform(Xe)      # imputación con mediana\n",
    "        Xe = scaler.transform(Xe)       # robust scaling\n",
    "        if USE_PCA:\n",
    "            Xe = pca.transform(Xe)      # reducción\n",
    "        yield s, e, Xe.astype(np.float32, copy=False)\n",
    "\n",
    "print(\"Train (scaled):\", X_train_sc.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/16] {'gamma': 'auto', 'nu': 0.02} | rate_mean=0.4450 | rate_std=0.0335 | obj=0.4435 | 97.4s\n",
      "[2/16] {'gamma': 'auto', 'nu': 0.03} | rate_mean=0.4451 | rate_std=0.0333 | obj=0.4434 | 110.1s\n",
      "[3/16] {'gamma': 'auto', 'nu': 0.05} | rate_mean=0.4451 | rate_std=0.0335 | obj=0.4436 | 109.0s\n",
      "[4/16] {'gamma': 'auto', 'nu': 0.08} | rate_mean=0.4452 | rate_std=0.0334 | obj=0.4436 | 95.0s\n",
      "[5/16] {'gamma': 'scale', 'nu': 0.02} | rate_mean=0.0201 | rate_std=0.0037 | obj=0.0186 | 2.5s\n",
      "[6/16] {'gamma': 'scale', 'nu': 0.03} | rate_mean=0.0306 | rate_std=0.0058 | obj=0.0102 | 4.0s\n",
      "[7/16] {'gamma': 'scale', 'nu': 0.05} | rate_mean=0.0503 | rate_std=0.0083 | obj=0.0236 | 7.1s\n",
      "[8/16] {'gamma': 'scale', 'nu': 0.08} | rate_mean=0.0806 | rate_std=0.0110 | obj=0.0567 | 12.5s\n",
      "[9/16] {'gamma': 0.1, 'nu': 0.02} | rate_mean=0.5048 | rate_std=0.0374 | obj=0.5071 | 120.0s\n",
      "[10/16] {'gamma': 0.1, 'nu': 0.03} | rate_mean=0.5055 | rate_std=0.0372 | obj=0.5077 | 124.7s\n",
      "[11/16] {'gamma': 0.1, 'nu': 0.05} | rate_mean=0.5054 | rate_std=0.0373 | obj=0.5077 | 121.1s\n",
      "[12/16] {'gamma': 0.1, 'nu': 0.08} | rate_mean=0.5052 | rate_std=0.0376 | obj=0.5077 | 121.4s\n",
      "[13/16] {'gamma': 0.03, 'nu': 0.02} | rate_mean=0.3620 | rate_std=0.0270 | obj=0.3540 | 66.0s\n",
      "[14/16] {'gamma': 0.03, 'nu': 0.03} | rate_mean=0.3616 | rate_std=0.0270 | obj=0.3536 | 67.8s\n",
      "[15/16] {'gamma': 0.03, 'nu': 0.05} | rate_mean=0.3621 | rate_std=0.0271 | obj=0.3541 | 71.6s\n",
      "[16/16] {'gamma': 0.03, 'nu': 0.08} | rate_mean=0.3619 | rate_std=0.0271 | obj=0.3540 | 63.7s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>rate_mean</th>\n",
       "      <th>rate_std</th>\n",
       "      <th>obj</th>\n",
       "      <th>secs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'gamma': 'scale', 'nu': 0.03}</td>\n",
       "      <td>0.030601</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>0.010223</td>\n",
       "      <td>3.957843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'gamma': 'scale', 'nu': 0.02}</td>\n",
       "      <td>0.020063</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.018612</td>\n",
       "      <td>2.526136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'gamma': 'scale', 'nu': 0.05}</td>\n",
       "      <td>0.050280</td>\n",
       "      <td>0.008271</td>\n",
       "      <td>0.023551</td>\n",
       "      <td>7.097527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'gamma': 'scale', 'nu': 0.08}</td>\n",
       "      <td>0.080638</td>\n",
       "      <td>0.011033</td>\n",
       "      <td>0.056670</td>\n",
       "      <td>12.485001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'gamma': 0.03, 'nu': 0.03}</td>\n",
       "      <td>0.361640</td>\n",
       "      <td>0.027008</td>\n",
       "      <td>0.353648</td>\n",
       "      <td>67.754969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'gamma': 0.03, 'nu': 0.02}</td>\n",
       "      <td>0.361990</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.354016</td>\n",
       "      <td>66.003989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'gamma': 0.03, 'nu': 0.08}</td>\n",
       "      <td>0.361939</td>\n",
       "      <td>0.027108</td>\n",
       "      <td>0.354047</td>\n",
       "      <td>63.709982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'gamma': 0.03, 'nu': 0.05}</td>\n",
       "      <td>0.362056</td>\n",
       "      <td>0.027061</td>\n",
       "      <td>0.354117</td>\n",
       "      <td>71.579432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'gamma': 'auto', 'nu': 0.03}</td>\n",
       "      <td>0.445067</td>\n",
       "      <td>0.033348</td>\n",
       "      <td>0.443415</td>\n",
       "      <td>110.119922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'gamma': 'auto', 'nu': 0.02}</td>\n",
       "      <td>0.444965</td>\n",
       "      <td>0.033505</td>\n",
       "      <td>0.443470</td>\n",
       "      <td>97.429803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           params  rate_mean  rate_std       obj        secs\n",
       "0  {'gamma': 'scale', 'nu': 0.03}   0.030601  0.005824  0.010223    3.957843\n",
       "1  {'gamma': 'scale', 'nu': 0.02}   0.020063  0.003675  0.018612    2.526136\n",
       "2  {'gamma': 'scale', 'nu': 0.05}   0.050280  0.008271  0.023551    7.097527\n",
       "3  {'gamma': 'scale', 'nu': 0.08}   0.080638  0.011033  0.056670   12.485001\n",
       "4     {'gamma': 0.03, 'nu': 0.03}   0.361640  0.027008  0.353648   67.754969\n",
       "5     {'gamma': 0.03, 'nu': 0.02}   0.361990  0.027027  0.354016   66.003989\n",
       "6     {'gamma': 0.03, 'nu': 0.08}   0.361939  0.027108  0.354047   63.709982\n",
       "7     {'gamma': 0.03, 'nu': 0.05}   0.362056  0.027061  0.354117   71.579432\n",
       "8   {'gamma': 'auto', 'nu': 0.03}   0.445067  0.033348  0.443415  110.119922\n",
       "9   {'gamma': 'auto', 'nu': 0.02}   0.444965  0.033505  0.443470   97.429803"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: {'gamma': 'scale', 'nu': 0.03} | cache/worker: 2048 MB | folds_workers: 3\n"
     ]
    }
   ],
   "source": [
    "# === HP search estable (paralelismo por FOLDS; memoria acotada) — reemplaza tu Celda 5 ===\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import GroupKFold, KFold, ParameterGrid\n",
    "from sklearn.svm import OneClassSVM\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "import time, math, gc\n",
    "\n",
    "param_grid = list(ParameterGrid({\"nu\": CFG[\"svm_nu_grid\"], \"gamma\": CFG[\"svm_gamma_grid\"]}))\n",
    "X_search = X_train_s\n",
    "groups_search = groups_train_s\n",
    "\n",
    "# Splitter (group-aware si procede)\n",
    "if (groups_search is not None) and (len(np.unique(groups_search)) >= CFG[\"kfold_splits\"]):\n",
    "    splitter = GroupKFold(n_splits=CFG[\"kfold_splits\"])\n",
    "    split_args = dict(X=X_search, y=None, groups=groups_search)\n",
    "else:\n",
    "    splitter = KFold(n_splits=CFG[\"kfold_splits\"], shuffle=True, random_state=42)\n",
    "    split_args = dict(X=X_search, y=None)\n",
    "\n",
    "target_rate = 0.035  # ~3.5% prior\n",
    "cache_mb = int(CFG[\"per_model_cache_mb\"])\n",
    "n_jobs_folds = int(CFG[\"n_jobs_folds\"])\n",
    "\n",
    "def out_rate(pred):\n",
    "    return float((pred == -1).mean())\n",
    "\n",
    "def eval_one_fold(p, tr, va):\n",
    "    # Entrena un OCSVM y devuelve outlier_rate en va\n",
    "    m = OneClassSVM(\n",
    "        kernel=CFG[\"kernel\"],\n",
    "        nu=p[\"nu\"],\n",
    "        gamma=p[\"gamma\"],\n",
    "        cache_size=cache_mb,   # cache por modelo controlado\n",
    "        tol=1e-3,\n",
    "        shrinking=True\n",
    "    )\n",
    "    m.fit(X_search[tr])\n",
    "    pred = m.predict(X_search[va])\n",
    "    return out_rate(pred)\n",
    "\n",
    "def eval_param_stable(p):\n",
    "    t0 = time.time()\n",
    "    folds = list(splitter.split(**split_args))\n",
    "    # Paralelismo solo sobre folds, con backend \"threads\" para compartir memoria\n",
    "    with parallel_backend(\"threading\", n_jobs=n_jobs_folds):\n",
    "        rates = Parallel(verbose=0)(\n",
    "            delayed(eval_one_fold)(p, tr, va) for (tr, va) in folds\n",
    "        )\n",
    "    rmean, rstd = float(np.mean(rates)), float(np.std(rates))\n",
    "    obj = abs(rmean - target_rate) + rstd\n",
    "    dt = time.time() - t0\n",
    "    return {\"params\": p, \"rate_mean\": rmean, \"rate_std\": rstd, \"obj\": obj, \"secs\": dt}\n",
    "\n",
    "rows = []\n",
    "for i, p in enumerate(param_grid, 1):\n",
    "    row = eval_param_stable(p)\n",
    "    rows.append(row)\n",
    "    print(f\"[{i}/{len(param_grid)}] {row['params']} | rate_mean={row['rate_mean']:.4f} \"\n",
    "          f\"| rate_std={row['rate_std']:.4f} | obj={row['obj']:.4f} | {row['secs']:.1f}s\")\n",
    "    # Garbage collect para evitar crecimiento de memoria entre iteraciones\n",
    "    gc.collect()\n",
    "\n",
    "res = pd.DataFrame(rows).sort_values(\"obj\").reset_index(drop=True)\n",
    "display(res.head(10))\n",
    "best_cfg = res.iloc[0][\"params\"]\n",
    "print(\"Best:\", best_cfg, \"| cache/worker:\", cache_mb, \"MB\", \"| folds_workers:\", n_jobs_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG: {\n",
      "  \"artifact_prefix\": \"ocsvm_rbf\",\n",
      "  \"max_train_samples\": 800000,\n",
      "  \"kernel\": \"rbf\"\n",
      "}\n",
      "✅ Imputer reconstruido desde medianas: ocsvm_rbf_imputer_medians.npy | n_features: 19\n",
      "✅ Scaler cargado: ocsvm_rbf_scaler.pkl\n",
      "ℹ️ PCA: no encontrado (se trabajará sin PCA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ X_train listo: (27789660, 19) | features=19 | groups=yes\n"
     ]
    }
   ],
   "source": [
    "# === Celda de recuperación de artefactos (CFG + Imputer + Scaler + PCA + X_train) ===\n",
    "import os, json, numpy as np, pandas as pd, gc, joblib\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "OUT_DIR = Path(\"data/ocsvm_runs\").resolve()\n",
    "DATA_DIR = Path(\"/teamspace/studios/this_studio/data\").resolve()\n",
    "\n",
    "# 1) Cargar CFG\n",
    "cfg_candidates = [\"ocsvm_v2_config.json\", \"ocsvm_rbf_config.json\"]\n",
    "cfg_path = next((OUT_DIR / f for f in cfg_candidates if (OUT_DIR / f).exists()), None)\n",
    "assert cfg_path is not None, f\"No encontré config en {OUT_DIR}\"\n",
    "CFG = json.loads(cfg_path.read_text())\n",
    "print(\"CFG:\", json.dumps({k: CFG.get(k) for k in [\"artifact_prefix\",\"max_train_samples\",\"kernel\"]}, indent=2))\n",
    "\n",
    "prefix = CFG.get(\"artifact_prefix\", \"ocsvm_rbf\")\n",
    "\n",
    "# 2) Cargar / reconstruir IMPUTER\n",
    "imputer_pkl = OUT_DIR / f\"{prefix}_imputer.pkl\"\n",
    "imputer_meds = OUT_DIR / f\"{prefix}_imputer_medians.npy\"\n",
    "if imputer_pkl.exists():\n",
    "    imputer = joblib.load(imputer_pkl)\n",
    "    print(\"✅ Imputer cargado:\", imputer_pkl.name)\n",
    "elif imputer_meds.exists():\n",
    "    meds = np.load(imputer_meds).astype(np.float32)\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    # hack limpio para inicializar la forma y luego inyectar medianas\n",
    "    imputer.fit(np.zeros((1, meds.size), dtype=np.float32))\n",
    "    imputer.statistics_ = meds\n",
    "    print(\"✅ Imputer reconstruido desde medianas:\", imputer_meds.name, \"| n_features:\", meds.size)\n",
    "else:\n",
    "    raise FileNotFoundError(\"No hay imputer.pkl ni *_imputer_medians.npy en OUT_DIR.\")\n",
    "\n",
    "# 3) Cargar SCALER\n",
    "scaler_pkl = OUT_DIR / f\"{prefix}_scaler.pkl\"\n",
    "assert scaler_pkl.exists(), f\"No existe {scaler_pkl}\"\n",
    "scaler = joblib.load(scaler_pkl)\n",
    "print(\"✅ Scaler cargado:\", scaler_pkl.name)\n",
    "\n",
    "# 4) Cargar PCA (opcional)\n",
    "pca_pkl = OUT_DIR / f\"{prefix}_pca.pkl\"\n",
    "pca = joblib.load(pca_pkl) if pca_pkl.exists() else None\n",
    "print(\"ℹ️ PCA:\", \"cargado\" if pca is not None else \"no encontrado (se trabajará sin PCA)\")\n",
    "\n",
    "# 5) Cargar X_train rápido (para re-fit o SGD-RFF)\n",
    "train_path = DATA_DIR / \"windows_aligned_normal.parquet\"\n",
    "assert train_path.exists(), f\"No se encontró {train_path}\"\n",
    "df_tr = pq.read_table(train_path).to_pandas()\n",
    "for c in df_tr.columns:\n",
    "    if pd.api.types.is_float_dtype(df_tr[c]): df_tr[c] = df_tr[c].astype(np.float32)\n",
    "\n",
    "mmsi_col = \"mmsi\" if \"mmsi\" in df_tr.columns else None\n",
    "drop_cols = {\"lat\",\"lon\",\"idx\",\"idx_end\",\"window_id\"}\n",
    "feature_cols = [c for c in df_tr.columns if c not in drop_cols and c != mmsi_col]\n",
    "X_train = df_tr[feature_cols].to_numpy(dtype=np.float32)\n",
    "groups_train = df_tr[mmsi_col].to_numpy() if mmsi_col else None\n",
    "del df_tr; gc.collect()\n",
    "\n",
    "print(f\"✅ X_train listo: {X_train.shape} | features={len(feature_cols)} | groups={'yes' if groups_train is not None else 'no'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FIT-RFF] usando subset: 799,920 filas  |  RAM libre: 118.1 GB\n",
      "[FIT-RFF] shape tras prepro: (799920, 19)\n",
      "[FIT-RFF] D=625  |  gamma≈0.1\n",
      "[FIT-RFF] RFF listo en 2.1s  |  RAM libre: 118.0 GB\n",
      "[FIT-RFF] SGDOneClassSVM entrenado en 0.1 min  |  soporte: 625 de 625\n",
      "✅ Guardados: ocsvm_rbf_imputer.pkl ocsvm_rbf_robust_scaler.pkl ocsvm_rbf_rff.pkl ocsvm_rbf_model_sgd_rff.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "362"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 6B (autosuficiente): RFF + SGDOneClassSVM (rápido y estable) ===\n",
    "import numpy as np, time, joblib, gc, psutil, os\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import SGDOneClassSVM\n",
    "\n",
    "# ---- helpers (por si no están en memoria) ----\n",
    "def sample_by_group(n_max, X, groups):\n",
    "    if (n_max is None) or (X.shape[0] <= n_max):\n",
    "        idx = np.arange(X.shape[0]); return X, (groups if groups is not None else None), idx\n",
    "    rng = np.random.default_rng(42)\n",
    "    if groups is None:\n",
    "        idx = rng.choice(X.shape[0], n_max, replace=False); return X[idx], None, idx\n",
    "    uniq = np.unique(groups); per_g = max(1, n_max // len(uniq)); take=[]\n",
    "    for g in uniq:\n",
    "        gi = np.where(groups==g)[0]\n",
    "        take.extend(rng.choice(gi, min(per_g, gi.size), replace=False).tolist())\n",
    "    take = np.array(take)\n",
    "    if take.size > n_max: take = rng.choice(take, n_max, replace=False)\n",
    "    return X[take], (groups[take] if groups is not None else None), take\n",
    "\n",
    "def naninf_to_nan(arr):\n",
    "    if not np.issubdtype(arr.dtype, np.floating):\n",
    "        arr = arr.astype(np.float32, copy=False)\n",
    "    bad = ~np.isfinite(arr)\n",
    "    if bad.any(): arr[bad] = np.nan\n",
    "    return arr\n",
    "\n",
    "# ---- 1) Subset para el fit (respetando CFG) ----\n",
    "n_fit = int(CFG.get(\"max_train_samples\", 120_000))\n",
    "X_fit_raw, _, _ = sample_by_group(n_fit, X_train, groups_train)\n",
    "print(f\"[FIT-RFF] usando subset: {X_fit_raw.shape[0]:,} filas  |  RAM libre: {psutil.virtual_memory().available/1e9:.1f} GB\")\n",
    "\n",
    "# ---- 2) Prepro (imputer -> scaler -> (opcional) pca) ----\n",
    "X_fit = X_fit_raw.astype(np.float32, copy=False)\n",
    "X_fit = naninf_to_nan(X_fit)\n",
    "X_fit = imputer.transform(X_fit)\n",
    "X_fit = scaler.transform(X_fit)\n",
    "USE_PCA = 'pca' in globals() and (pca is not None)\n",
    "if USE_PCA:\n",
    "    X_fit = pca.transform(X_fit)\n",
    "print(\"[FIT-RFF] shape tras prepro:\", X_fit.shape)\n",
    "\n",
    "# ---- 3) RFF: elegir dimensionalidad D dinámica según RAM y n_fit ----\n",
    "# Objetivo: ~2 GB para la matriz expandida (float32)\n",
    "# D ≈ floor(2e9 bytes / (n_fit * 4 bytes))\n",
    "D_target = max(256, min(1024, int(2_000_000_000 // (max(1, X_fit.shape[0]) * 4))))\n",
    "D = int(D_target)\n",
    "# gamma RBF: si no tenemos best_cfg, usa 0.1 como heurística segura\n",
    "gamma_cfg = 0.1\n",
    "if \"best_cfg\" in globals() and best_cfg:\n",
    "    g = best_cfg.get(\"gamma\", \"auto\")\n",
    "    gamma_cfg = 0.1 if (g in [\"auto\",\"scale\"]) else float(g)\n",
    "print(f\"[FIT-RFF] D={D}  |  gamma≈{gamma_cfg}\")\n",
    "\n",
    "rff = RBFSampler(gamma=gamma_cfg, n_components=D, random_state=42)\n",
    "t0 = time.time()\n",
    "X_rff = rff.fit_transform(X_fit)  # ocupa ~ n_fit x D x 4 bytes\n",
    "print(f\"[FIT-RFF] RFF listo en {time.time()-t0:.1f}s  |  RAM libre: {psutil.virtual_memory().available/1e9:.1f} GB\")\n",
    "\n",
    "# ---- 4) Entrenar SGDOneClassSVM (lineal en espacio RFF) ----\n",
    "nu_val = 0.05 if \"best_cfg\" not in globals() or not best_cfg else float(best_cfg.get(\"nu\", 0.05))\n",
    "\n",
    "# Constructor compatible y estable\n",
    "sgd_oc = SGDOneClassSVM(\n",
    "    nu=nu_val,\n",
    "    fit_intercept=True,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    tol=1e-3,\n",
    "    max_iter=2000,      # puedes subir a 5000 si lo ves converger rápido\n",
    "    # nota: NO usar early_stopping / learning_rate aquí\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "sgd_oc.fit(X_rff)\n",
    "print(f\"[FIT-RFF] SGDOneClassSVM entrenado en {(time.time()-t1)/60:.1f} min  |  soporte: {np.count_nonzero(sgd_oc.coef_)} de {D}\")\n",
    "\n",
    "# ---- 5) Guardar artefactos (pipeline) ----\n",
    "prefix = CFG.get(\"artifact_prefix\",\"ocsvm_rbf\")\n",
    "joblib.dump(imputer, OUT_DIR / f\"{prefix}_imputer.pkl\")\n",
    "joblib.dump(scaler,  OUT_DIR / f\"{prefix}_robust_scaler.pkl\")\n",
    "if USE_PCA: joblib.dump(pca, OUT_DIR / f\"{prefix}_pca.pkl\")\n",
    "joblib.dump(rff,     OUT_DIR / f\"{prefix}_rff.pkl\")\n",
    "joblib.dump(sgd_oc,  OUT_DIR / f\"{prefix}_model_sgd_rff.pkl\")\n",
    "\n",
    "print(\"✅ Guardados:\",\n",
    "      (OUT_DIR / f\"{prefix}_imputer.pkl\").name,\n",
    "      (OUT_DIR / f\"{prefix}_robust_scaler.pkl\").name,\n",
    "      (OUT_DIR / f\"{prefix}_rff.pkl\").name,\n",
    "      (OUT_DIR / f\"{prefix}_model_sgd_rff.pkl\").name)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL parquet: windows_with_labels_aligned.parquet\n",
      "X_eval: (27789660, 19) | feats: 19 | groups: mmsi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1871"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Reconstrucción X_eval (desde parquets aligned) ---\n",
    "import pandas as pd, numpy as np, pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "DATA_DIR = Path(CFG[\"external_data_dir\"])\n",
    "assert DATA_DIR.exists()\n",
    "\n",
    "# Elegimos el mayor *windows*aligned*.parquet para garantizar orden/longitud\n",
    "eval_path = max(DATA_DIR.glob(\"*windows*aligned*.parquet\"), key=lambda p: p.stat().st_size)\n",
    "print(\"EVAL parquet:\", eval_path.name)\n",
    "\n",
    "# Lectura\n",
    "df_ev = pq.read_table(eval_path).to_pandas()\n",
    "for c in df_ev.columns:\n",
    "    if pd.api.types.is_float_dtype(df_ev[c]): df_ev[c] = df_ev[c].astype(np.float32)\n",
    "\n",
    "# Detectar columnas a excluir\n",
    "drop_common = {\"lat\",\"lon\",\"idx\",\"idx_end\",\"window_id\"}\n",
    "gcol_ev = next((c for c in [\"mmsi\",\"group\",\"ship_id\"] if c in df_ev.columns), None)\n",
    "ycol_ev = next((c for c in [\"is_suspicious\",\"label\",\"y\",\"target\"] if c in df_ev.columns), None)\n",
    "\n",
    "feature_cols_ev = [c for c in df_ev.columns if c not in (drop_common | {gcol_ev, ycol_ev} if gcol_ev else drop_common)]\n",
    "X_eval = df_ev[feature_cols_ev].to_numpy(dtype=np.float32)\n",
    "groups_eval = df_ev[gcol_ev].to_numpy() if gcol_ev else None\n",
    "\n",
    "print(\"X_eval:\", X_eval.shape, \"| feats:\", len(feature_cols_ev), \"| groups:\", gcol_ev)\n",
    "del df_ev; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 14.39% | 4,000,000/27,789,660 rows\n",
      "Progress: 28.79% | 8,000,000/27,789,660 rows\n",
      "Progress: 43.18% | 12,000,000/27,789,660 rows\n",
      "Progress: 57.58% | 16,000,000/27,789,660 rows\n",
      "Progress: 71.97% | 20,000,000/27,789,660 rows\n",
      "Progress: 86.36% | 24,000,000/27,789,660 rows\n",
      "Progress: 100.00% | 27,789,660/27,789,660 rows\n",
      "Scores memmap: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_eval_scores_mm.dat | shape: (27789660,)\n"
     ]
    }
   ],
   "source": [
    "# --- Scoring por lotes (RFF+SGD) ---\n",
    "import numpy as np, os, time, joblib\n",
    "from pathlib import Path\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"]); OUT.mkdir(parents=True, exist_ok=True)\n",
    "prefix = CFG.get(\"artifact_prefix\",\"ocsvm_rbf\")\n",
    "scores_path = OUT / f\"{prefix}_eval_scores_mm.dat\"\n",
    "\n",
    "# Cargar artefactos\n",
    "imputer = joblib.load(OUT / f\"{prefix}_imputer.pkl\")\n",
    "scaler  = joblib.load(OUT / f\"{prefix}_robust_scaler.pkl\")\n",
    "rff     = joblib.load(OUT / f\"{prefix}_rff.pkl\")\n",
    "sgd_oc  = joblib.load(OUT / f\"{prefix}_model_sgd_rff.pkl\")\n",
    "\n",
    "# Memmap\n",
    "n_eval = X_eval.shape[0]\n",
    "if scores_path.exists(): os.remove(scores_path)\n",
    "scores_mm = np.memmap(scores_path, dtype=np.float32, mode=\"w+\", shape=(n_eval,))\n",
    "\n",
    "def naninf_to_nan(arr):\n",
    "    if not np.issubdtype(arr.dtype, np.floating):\n",
    "        arr = arr.astype(np.float32, copy=False)\n",
    "    bad = ~np.isfinite(arr)\n",
    "    if bad.any(): arr[bad] = np.nan\n",
    "    return arr\n",
    "\n",
    "B = int(CFG.get(\"eval_batch_size\", 2_000_000))\n",
    "t0=time.time(); last=t0\n",
    "for s in range(0, n_eval, B):\n",
    "    e = min(s+B, n_eval)\n",
    "    Xe = X_eval[s:e].astype(np.float32, copy=False)\n",
    "    Xe = naninf_to_nan(Xe)\n",
    "    Xe = imputer.transform(Xe)\n",
    "    Xe = scaler.transform(Xe)\n",
    "    Xe = rff.transform(Xe)\n",
    "    scores_mm[s:e] = -sgd_oc.decision_function(Xe)\n",
    "    scores_mm.flush()\n",
    "    now=time.time()\n",
    "    if now-last>10:\n",
    "        print(f\"Progress: {100*e/n_eval:5.2f}% | {e:,}/{n_eval:,} rows\")\n",
    "        last=now\n",
    "\n",
    "scores = scores_mm  # vista\n",
    "print(\"Scores memmap:\", scores_path, \"| shape:\", scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positivos: 1003200 / 27789660\n",
      "ROC-AUC:0.4848 | PR-AUC:0.0390 | AP:0.0390\n",
      "@1% -> P:0.0671 | R:0.0186 | F1:0.0291  (k=277896)\n",
      "✅ Config actualizado: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_config.json\n"
     ]
    }
   ],
   "source": [
    "# --- Métricas: JOIN por 'window_id' + guardado ---\n",
    "import json, numpy as np, pandas as pd, pyarrow.parquet as pq\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"]); prefix = CFG.get(\"artifact_prefix\",\"ocsvm_rbf\")\n",
    "\n",
    "# Claves de eval (orden exacto)\n",
    "eval_pf = pq.ParquetFile(max(Path(CFG[\"external_data_dir\"]).glob(\"*windows*aligned*.parquet\"),\n",
    "                             key=lambda p: p.stat().st_size))\n",
    "key_eval = next(k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in eval_pf.schema_arrow.names)\n",
    "eval_key = eval_pf.read(columns=[key_eval]).to_pandas()[key_eval].astype(np.int64).reset_index(drop=True)\n",
    "\n",
    "# Conjunto de anómalos\n",
    "lab_pf = pq.ParquetFile(next(Path(CFG[\"external_data_dir\"]).glob(\"*labels_anom*.parquet\")))\n",
    "key_lab = key_eval if key_eval in lab_pf.schema_arrow.names else \\\n",
    "          next(k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in lab_pf.schema_arrow.names)\n",
    "anom_set = set(lab_pf.read(columns=[key_lab]).to_pandas()[key_lab].astype(np.int64).tolist())\n",
    "\n",
    "y_eval = eval_key.isin(anom_set).astype(int).to_numpy()\n",
    "print(\"Positivos:\", int(y_eval.sum()), \"/\", len(y_eval))\n",
    "\n",
    "# Métricas globales\n",
    "if len(np.unique(y_eval)) > 1:\n",
    "    roc = roc_auc_score(y_eval, scores)\n",
    "    prc, rec, _ = precision_recall_curve(y_eval, scores); pr_auc = auc(rec, prc)\n",
    "    ap = average_precision_score(y_eval, scores)\n",
    "else:\n",
    "    roc = pr_auc = ap = float('nan')\n",
    "print(f\"ROC-AUC:{roc:.4f} | PR-AUC:{pr_auc:.4f} | AP:{ap:.4f}\")\n",
    "\n",
    "# @k=1%\n",
    "k_rate=0.01; k=max(1,int(len(scores)*k_rate))\n",
    "thr=np.partition(scores,-k)[-k]; pred=(scores>=thr).astype(np.int8)\n",
    "tp=int(((pred==1)&(y_eval==1)).sum()); fp=int(((pred==1)&(y_eval==0)).sum()); fn=int(((pred==0)&(y_eval==1)).sum())\n",
    "P=tp/(tp+fp) if (tp+fp)>0 else 0.0; R=tp/(tp+fn) if (tp+fn)>0 else 0.0; F1=2*P*R/(P+R) if (P+R)>0 else 0.0\n",
    "print(f\"@1% -> P:{P:.4f} | R:{R:.4f} | F1:{F1:.4f}  (k={k})\")\n",
    "\n",
    "# Guardar/actualizar config\n",
    "cfg_path = OUT / f\"{prefix}_config.json\"\n",
    "cfg = json.loads(cfg_path.read_text()) if cfg_path.exists() else {}\n",
    "cfg.update({\n",
    "    \"artifact_prefix\": prefix,\n",
    "    \"external_data_dir\": CFG[\"external_data_dir\"],\n",
    "    \"model_type\": \"SGDOneClassSVM_RFF\",\n",
    "    \"rff_components\": int(rff.n_components),\n",
    "    \"best_params\": {\"nu\": float(getattr(sgd_oc, \"nu\", 0.05)), \"gamma\": \"approx(%.4f)\" % getattr(rff, \"gamma\", 0.1)},\n",
    "    \"metrics\": {\"roc_auc\": float(roc), \"pr_auc\": float(pr_auc), \"ap\": float(ap)},\n",
    "    \"metrics_at_k\": {\"0.01\": {\"k\": int(k), \"precision\": float(P), \"recall\": float(R), \"f1\": float(F1)}},\n",
    "    \"label_file\": lab_pf.metadata.metadata.get(b\"ARROW:schema\", b\"labels_anom.parquet\").decode(errors=\"ignore\") if hasattr(lab_pf,'metadata') else \"labels_anom.parquet\",\n",
    "    \"label_col\": key_lab,\n",
    "    \"label_mapping\": f\"JOIN on {key_eval}\"\n",
    "})\n",
    "cfg_path.write_text(json.dumps(cfg, indent=2))\n",
    "print(\"✅ Config actualizado:\", cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TOP-1%: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_top1pct_detailed.parquet y /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_top1pct_detailed.csv | rows: 277897\n",
      "Saved MMSI agg: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_mmsi_agg.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmsi</th>\n",
       "      <th>n_win</th>\n",
       "      <th>anom_win</th>\n",
       "      <th>anom_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>33266086194351</td>\n",
       "      <td>428760</td>\n",
       "      <td>26474</td>\n",
       "      <td>0.061745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>277458320934593</td>\n",
       "      <td>22520</td>\n",
       "      <td>1153</td>\n",
       "      <td>0.051199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>168015789077263</td>\n",
       "      <td>26420</td>\n",
       "      <td>1344</td>\n",
       "      <td>0.050871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>196010605910823</td>\n",
       "      <td>306700</td>\n",
       "      <td>11723</td>\n",
       "      <td>0.038223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>100771710683634</td>\n",
       "      <td>32440</td>\n",
       "      <td>1133</td>\n",
       "      <td>0.034926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12639560807591</td>\n",
       "      <td>23520</td>\n",
       "      <td>820</td>\n",
       "      <td>0.034864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>77182424306278</td>\n",
       "      <td>169660</td>\n",
       "      <td>5090</td>\n",
       "      <td>0.030001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>208886908053041</td>\n",
       "      <td>19200</td>\n",
       "      <td>507</td>\n",
       "      <td>0.026406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>87919276942456</td>\n",
       "      <td>567520</td>\n",
       "      <td>14732</td>\n",
       "      <td>0.025959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>103576446797335</td>\n",
       "      <td>148920</td>\n",
       "      <td>3844</td>\n",
       "      <td>0.025813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                mmsi   n_win  anom_win  anom_rate\n",
       "12    33266086194351  428760     26474   0.061745\n",
       "106  277458320934593   22520      1153   0.051199\n",
       "61   168015789077263   26420      1344   0.050871\n",
       "72   196010605910823  306700     11723   0.038223\n",
       "40   100771710683634   32440      1133   0.034926\n",
       "1     12639560807591   23520       820   0.034864\n",
       "27    77182424306278  169660      5090   0.030001\n",
       "83   208886908053041   19200       507   0.026406\n",
       "33    87919276942456  567520     14732   0.025959\n",
       "42   103576446797335  148920      3844   0.025813"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Export Top-1% y agregados por MMSI ---\n",
    "import numpy as np, pandas as pd, os\n",
    "\n",
    "prefix = CFG.get(\"artifact_prefix\",\"ocsvm_rbf\")\n",
    "topk_rate = 0.01\n",
    "k = max(1, int(len(scores) * topk_rate))\n",
    "thr_k = np.partition(scores, -k)[-k]\n",
    "pred_topk = (scores >= thr_k)\n",
    "\n",
    "rows = np.where(pred_topk)[0]\n",
    "top = pd.DataFrame({\"idx\": rows, \"anomaly_score\": scores[rows].astype(np.float32)})\n",
    "if 'groups_eval' in globals() and groups_eval is not None:\n",
    "    top[\"mmsi\"] = groups_eval[rows].astype(np.int64)\n",
    "\n",
    "top = top.sort_values(\"anomaly_score\", ascending=False)\n",
    "out_pq = OUT / f\"{prefix}_top1pct_detailed.parquet\"\n",
    "out_csv = OUT / f\"{prefix}_top1pct_detailed.csv\"\n",
    "top.to_parquet(out_pq, index=False); top.to_csv(out_csv, index=False)\n",
    "print(\"Saved TOP-1%:\", out_pq, \"y\", out_csv, \"| rows:\", len(top))\n",
    "\n",
    "# Agregado por MMSI (si hay grupos)\n",
    "if 'groups_eval' in globals() and groups_eval is not None:\n",
    "    mmsi_all, n_by_mmsi = np.unique(groups_eval, return_counts=True)\n",
    "    mmsi_top, n_top_by_mmsi = np.unique(groups_eval[pred_topk==1], return_counts=True)\n",
    "    top_map = dict(zip(mmsi_top.tolist(), n_top_by_mmsi.tolist()))\n",
    "    anom_win = np.array([top_map.get(m, 0) for m in mmsi_all], dtype=np.int32)\n",
    "    anom_rate = anom_win / n_by_mmsi\n",
    "    agg_df = pd.DataFrame({\"mmsi\": mmsi_all, \"n_win\": n_by_mmsi, \"anom_win\": anom_win, \"anom_rate\": anom_rate})\n",
    "    agg_path = OUT / f\"{prefix}_mmsi_agg.parquet\"\n",
    "    agg_df.to_parquet(agg_path, index=False)\n",
    "    print(\"Saved MMSI agg:\", agg_path)\n",
    "    display(agg_df.sort_values(\"anom_rate\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D=512 g=0.03 nu=0.01 | ROC=0.4859 PR=0.0392 AP=0.0392 | 7.9s\n",
      "D=512 g=0.03 nu=0.03 | ROC=0.4804 PR=0.0387 AP=0.0387 | 8.0s\n",
      "D=512 g=0.03 nu=0.05 | ROC=0.4861 PR=0.0393 AP=0.0393 | 8.2s\n",
      "D=512 g=0.03 nu=0.08 | ROC=0.4934 PR=0.0401 AP=0.0401 | 8.3s\n",
      "D=512 g=0.1 nu=0.01 | ROC=0.5207 PR=0.0443 AP=0.0444 | 7.9s\n",
      "D=512 g=0.1 nu=0.03 | ROC=0.5207 PR=0.0445 AP=0.0445 | 8.1s\n",
      "D=512 g=0.1 nu=0.05 | ROC=0.5160 PR=0.0439 AP=0.0439 | 8.2s\n",
      "D=512 g=0.1 nu=0.08 | ROC=0.5105 PR=0.0431 AP=0.0431 | 8.3s\n",
      "D=512 g=0.3 nu=0.01 | ROC=0.5177 PR=0.0412 AP=0.0412 | 8.0s\n",
      "D=512 g=0.3 nu=0.03 | ROC=0.5306 PR=0.0434 AP=0.0434 | 8.1s\n",
      "D=512 g=0.3 nu=0.05 | ROC=0.5291 PR=0.0445 AP=0.0445 | 8.3s\n",
      "D=512 g=0.3 nu=0.08 | ROC=0.5335 PR=0.0450 AP=0.0450 | 8.4s\n",
      "D=1024 g=0.03 nu=0.01 | ROC=0.4942 PR=0.0389 AP=0.0389 | 11.9s\n",
      "D=1024 g=0.03 nu=0.03 | ROC=0.5008 PR=0.0404 AP=0.0404 | 12.0s\n",
      "D=1024 g=0.03 nu=0.05 | ROC=0.5061 PR=0.0411 AP=0.0411 | 12.2s\n",
      "D=1024 g=0.03 nu=0.08 | ROC=0.5081 PR=0.0413 AP=0.0413 | 12.5s\n",
      "D=1024 g=0.1 nu=0.01 | ROC=0.5194 PR=0.0424 AP=0.0424 | 12.0s\n",
      "D=1024 g=0.1 nu=0.03 | ROC=0.5258 PR=0.0436 AP=0.0436 | 12.1s\n",
      "D=1024 g=0.1 nu=0.05 | ROC=0.5235 PR=0.0435 AP=0.0435 | 12.2s\n",
      "D=1024 g=0.1 nu=0.08 | ROC=0.5200 PR=0.0432 AP=0.0432 | 12.4s\n",
      "D=1024 g=0.3 nu=0.01 | ROC=0.5387 PR=0.0442 AP=0.0442 | 12.0s\n",
      "D=1024 g=0.3 nu=0.03 | ROC=0.5387 PR=0.0444 AP=0.0444 | 12.2s\n",
      "D=1024 g=0.3 nu=0.05 | ROC=0.5407 PR=0.0446 AP=0.0446 | 12.3s\n",
      "D=1024 g=0.3 nu=0.08 | ROC=0.5397 PR=0.0448 AP=0.0448 | 12.4s\n",
      "D=2048 g=0.03 nu=0.01 | ROC=0.4944 PR=0.0402 AP=0.0402 | 20.0s\n",
      "D=2048 g=0.03 nu=0.03 | ROC=0.5019 PR=0.0413 AP=0.0413 | 20.5s\n",
      "D=2048 g=0.03 nu=0.05 | ROC=0.5052 PR=0.0416 AP=0.0416 | 20.6s\n",
      "D=2048 g=0.03 nu=0.08 | ROC=0.5087 PR=0.0418 AP=0.0418 | 21.1s\n",
      "D=2048 g=0.1 nu=0.01 | ROC=0.5122 PR=0.0424 AP=0.0424 | 20.3s\n",
      "D=2048 g=0.1 nu=0.03 | ROC=0.5147 PR=0.0433 AP=0.0433 | 20.3s\n",
      "D=2048 g=0.1 nu=0.05 | ROC=0.5185 PR=0.0437 AP=0.0437 | 20.7s\n",
      "D=2048 g=0.1 nu=0.08 | ROC=0.5203 PR=0.0437 AP=0.0437 | 21.2s\n",
      "D=2048 g=0.3 nu=0.01 | ROC=0.5325 PR=0.0451 AP=0.0451 | 20.4s\n",
      "D=2048 g=0.3 nu=0.03 | ROC=0.5266 PR=0.0448 AP=0.0448 | 20.7s\n",
      "D=2048 g=0.3 nu=0.05 | ROC=0.5343 PR=0.0460 AP=0.0460 | 20.9s\n",
      "D=2048 g=0.3 nu=0.08 | ROC=0.5366 PR=0.0464 AP=0.0464 | 21.5s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D</th>\n",
       "      <th>gamma</th>\n",
       "      <th>nu</th>\n",
       "      <th>ROC</th>\n",
       "      <th>PR</th>\n",
       "      <th>AP</th>\n",
       "      <th>secs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2048</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.536598</td>\n",
       "      <td>0.046422</td>\n",
       "      <td>0.046426</td>\n",
       "      <td>21.463260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2048</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.534300</td>\n",
       "      <td>0.046043</td>\n",
       "      <td>0.046048</td>\n",
       "      <td>20.889090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2048</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.532473</td>\n",
       "      <td>0.045127</td>\n",
       "      <td>0.045130</td>\n",
       "      <td>20.420836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>512</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.533468</td>\n",
       "      <td>0.045016</td>\n",
       "      <td>0.045023</td>\n",
       "      <td>8.401397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1024</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.539700</td>\n",
       "      <td>0.044824</td>\n",
       "      <td>0.044828</td>\n",
       "      <td>12.443507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2048</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.526614</td>\n",
       "      <td>0.044763</td>\n",
       "      <td>0.044767</td>\n",
       "      <td>20.741860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1024</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.540656</td>\n",
       "      <td>0.044617</td>\n",
       "      <td>0.044622</td>\n",
       "      <td>12.303638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>512</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.520742</td>\n",
       "      <td>0.044505</td>\n",
       "      <td>0.044511</td>\n",
       "      <td>8.063840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>512</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.529117</td>\n",
       "      <td>0.044465</td>\n",
       "      <td>0.044473</td>\n",
       "      <td>8.275153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1024</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.538653</td>\n",
       "      <td>0.044354</td>\n",
       "      <td>0.044359</td>\n",
       "      <td>12.157986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       D  gamma    nu       ROC        PR        AP       secs\n",
       "35  2048    0.3  0.08  0.536598  0.046422  0.046426  21.463260\n",
       "34  2048    0.3  0.05  0.534300  0.046043  0.046048  20.889090\n",
       "32  2048    0.3  0.01  0.532473  0.045127  0.045130  20.420836\n",
       "11   512    0.3  0.08  0.533468  0.045016  0.045023   8.401397\n",
       "23  1024    0.3  0.08  0.539700  0.044824  0.044828  12.443507\n",
       "33  2048    0.3  0.03  0.526614  0.044763  0.044767  20.741860\n",
       "22  1024    0.3  0.05  0.540656  0.044617  0.044622  12.303638\n",
       "5    512    0.1  0.03  0.520742  0.044505  0.044511   8.063840\n",
       "10   512    0.3  0.05  0.529117  0.044465  0.044473   8.275153\n",
       "21  1024    0.3  0.03  0.538653  0.044354  0.044359  12.157986"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best (mini-HPO): {'D': 2048.0, 'gamma': 0.3, 'nu': 0.08, 'ROC': 0.536597509400389, 'PR': 0.04642206818669361, 'AP': 0.04642636837618991, 'secs': 21.463260173797607}\n",
      "✅ Guardado pipeline ganador (rff + sgd_oc). Ahora re-ejecuta la Celda B para recalcular scores completos.\n"
     ]
    }
   ],
   "source": [
    "# === Mini-HPO RFF+SGD (rápido) ===\n",
    "import numpy as np, time, joblib, gc, pyarrow.parquet as pq, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import SGDOneClassSVM\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, average_precision_score\n",
    "\n",
    "# 0) helpers\n",
    "def sample_by_idx(n_max, n_total, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    if n_total <= n_max: return np.arange(n_total)\n",
    "    return np.sort(rng.choice(n_total, n_max, replace=False))\n",
    "\n",
    "def build_eval_subset(n_eval_sub=3_000_000):\n",
    "    DATA_DIR = Path(CFG[\"external_data_dir\"])\n",
    "    eval_path = max(DATA_DIR.glob(\"*windows*aligned*.parquet\"), key=lambda p: p.stat().st_size)\n",
    "    df = pq.read_table(eval_path).to_pandas()\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_float_dtype(df[c]): df[c] = df[c].astype(np.float32)\n",
    "    drop = {\"lat\",\"lon\",\"idx\",\"idx_end\",\"window_id\"}\n",
    "    gcol = next((c for c in [\"mmsi\",\"group\",\"ship_id\"] if c in df.columns), None)\n",
    "    ycol = next((c for c in [\"is_suspicious\",\"label\",\"y\",\"target\"] if c in df.columns), None)\n",
    "    feats = [c for c in df.columns if c not in (drop | {gcol,ycol} if gcol else drop)]\n",
    "    Xev = df[feats].to_numpy(np.float32)\n",
    "\n",
    "    # y por JOIN window_id\n",
    "    key_eval = next(k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in pq.ParquetFile(eval_path).schema_arrow.names)\n",
    "    eval_key = df[key_eval].astype(np.int64).to_numpy()\n",
    "    lab_pf = pq.ParquetFile(next(DATA_DIR.glob(\"*labels_anom*.parquet\")))\n",
    "    key_lab = key_eval if key_eval in lab_pf.schema_arrow.names else next(k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in lab_pf.schema_arrow.names)\n",
    "    anom_set = set(lab_pf.read(columns=[key_lab]).to_pandas()[key_lab].astype(np.int64).tolist())\n",
    "    yb = np.isin(eval_key, list(anom_set)).astype(int)\n",
    "\n",
    "    # subset indices\n",
    "    idx = sample_by_idx(n_eval_sub, Xev.shape[0])\n",
    "    return Xev[idx], yb[idx]\n",
    "\n",
    "def prepro(X):\n",
    "    X = X.astype(np.float32, copy=False)\n",
    "    X = np.where(np.isfinite(X), X, np.nan)\n",
    "    # imputar (imputer ya cargado)\n",
    "    X = imputer.transform(X)\n",
    "    X = scaler.transform(X)\n",
    "    return X\n",
    "\n",
    "# 1) subset train para fit\n",
    "n_fit = int(CFG.get(\"max_train_samples\", 800_000))\n",
    "rng_idx = sample_by_idx(n_fit, X_train.shape[0])\n",
    "Xf = prepro(X_train[rng_idx])\n",
    "\n",
    "# 2) subset eval para validar\n",
    "Xe_sub, y_sub = build_eval_subset(n_eval_sub=3_000_000)\n",
    "Xe_sub = prepro(Xe_sub)\n",
    "\n",
    "# 3) grillas pequeñas\n",
    "nu_grid    = [0.01, 0.03, 0.05, 0.08]\n",
    "gamma_grid = [0.03, 0.10, 0.30]\n",
    "D_grid     = [512, 1024, 2048]  # si te sobra RAM, añade 2048\n",
    "\n",
    "results = []\n",
    "t_all = time.time()\n",
    "for D in D_grid:\n",
    "    for g in gamma_grid:\n",
    "        # transformar train una sola vez por (D,g)\n",
    "        rff = RBFSampler(gamma=g, n_components=D, random_state=42)\n",
    "        Xf_rff = rff.fit_transform(Xf)\n",
    "        Xe_rff = rff.transform(Xe_sub)\n",
    "        for nu in nu_grid:\n",
    "            model = SGDOneClassSVM(nu=nu, fit_intercept=True, shuffle=True,\n",
    "                                   random_state=42, tol=1e-3, max_iter=2000)\n",
    "            t0 = time.time()\n",
    "            model.fit(Xf_rff)\n",
    "            # scores (neg decision_function mayor => más anómalo)\n",
    "            scores = -model.decision_function(Xe_rff)\n",
    "            if len(np.unique(y_sub))>1:\n",
    "                roc = roc_auc_score(y_sub, scores)\n",
    "                prec, rec, _ = precision_recall_curve(y_sub, scores); pr_auc = auc(rec, prec)\n",
    "                ap  = average_precision_score(y_sub, scores)\n",
    "            else:\n",
    "                roc = pr_auc = ap = float(\"nan\")\n",
    "            results.append({\"D\":D,\"gamma\":g,\"nu\":nu,\"ROC\":roc,\"PR\":pr_auc,\"AP\":ap,\"secs\":time.time()-t0})\n",
    "            print(f\"D={D} g={g} nu={nu} | ROC={roc:.4f} PR={pr_auc:.4f} AP={ap:.4f} | {results[-1]['secs']:.1f}s\")\n",
    "\n",
    "# 4) elegir mejor por PR-AUC\n",
    "res = pd.DataFrame(results).sort_values([\"PR\",\"AP\",\"ROC\"], ascending=False)\n",
    "display(res.head(10))\n",
    "best = res.iloc[0].to_dict()\n",
    "print(\"Best (mini-HPO):\", best)\n",
    "\n",
    "# 5) OPCIONAL: guardar pipeline ganador para scoring completo\n",
    "SAVE = True\n",
    "if SAVE:\n",
    "    rff_best = RBFSampler(gamma=float(best[\"gamma\"]), n_components=int(best[\"D\"]), random_state=42)\n",
    "    Xf_rff = rff_best.fit_transform(Xf)\n",
    "    model_best = SGDOneClassSVM(nu=float(best[\"nu\"]), fit_intercept=True, shuffle=True,\n",
    "                                random_state=42, tol=1e-3, max_iter=2000)\n",
    "    model_best.fit(Xf_rff)\n",
    "    prefix = CFG.get(\"artifact_prefix\",\"ocsvm_rbf\")\n",
    "    joblib.dump(rff_best, Path(CFG[\"out_dir\"]) / f\"{prefix}_rff.pkl\")\n",
    "    joblib.dump(model_best, Path(CFG[\"out_dir\"]) / f\"{prefix}_model_sgd_rff.pkl\")\n",
    "    print(\"✅ Guardado pipeline ganador (rff + sgd_oc). Ahora re-ejecuta la Celda B para recalcular scores completos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  7.20% | 2,000,000/27,789,660 rows\n",
      "Progress: 14.39% | 4,000,000/27,789,660 rows\n",
      "Progress: 21.59% | 6,000,000/27,789,660 rows\n",
      "Progress: 28.79% | 8,000,000/27,789,660 rows\n",
      "Progress: 35.98% | 10,000,000/27,789,660 rows\n",
      "Progress: 43.18% | 12,000,000/27,789,660 rows\n",
      "Progress: 50.38% | 14,000,000/27,789,660 rows\n",
      "Progress: 57.58% | 16,000,000/27,789,660 rows\n",
      "Progress: 64.77% | 18,000,000/27,789,660 rows\n",
      "Progress: 71.97% | 20,000,000/27,789,660 rows\n",
      "Progress: 79.17% | 22,000,000/27,789,660 rows\n",
      "Progress: 86.36% | 24,000,000/27,789,660 rows\n",
      "Progress: 93.56% | 26,000,000/27,789,660 rows\n",
      "Progress: 100.00% | 27,789,660/27,789,660 rows\n",
      "Scores memmap: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_eval_scores_mm.dat | shape: (27789660,)\n"
     ]
    }
   ],
   "source": [
    "# --- Scoring por lotes (RFF+SGD) ---\n",
    "import numpy as np, os, time, joblib\n",
    "from pathlib import Path\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"]); OUT.mkdir(parents=True, exist_ok=True)\n",
    "prefix = CFG.get(\"artifact_prefix\",\"ocsvm_rbf\")\n",
    "scores_path = OUT / f\"{prefix}_eval_scores_mm.dat\"\n",
    "\n",
    "# Cargar artefactos\n",
    "imputer = joblib.load(OUT / f\"{prefix}_imputer.pkl\")\n",
    "scaler  = joblib.load(OUT / f\"{prefix}_robust_scaler.pkl\")\n",
    "rff     = joblib.load(OUT / f\"{prefix}_rff.pkl\")\n",
    "sgd_oc  = joblib.load(OUT / f\"{prefix}_model_sgd_rff.pkl\")\n",
    "\n",
    "# Memmap\n",
    "n_eval = X_eval.shape[0]\n",
    "if scores_path.exists(): os.remove(scores_path)\n",
    "scores_mm = np.memmap(scores_path, dtype=np.float32, mode=\"w+\", shape=(n_eval,))\n",
    "\n",
    "def naninf_to_nan(arr):\n",
    "    if not np.issubdtype(arr.dtype, np.floating):\n",
    "        arr = arr.astype(np.float32, copy=False)\n",
    "    bad = ~np.isfinite(arr)\n",
    "    if bad.any(): arr[bad] = np.nan\n",
    "    return arr\n",
    "\n",
    "B = int(CFG.get(\"eval_batch_size\", 2_000_000))\n",
    "t0=time.time(); last=t0\n",
    "for s in range(0, n_eval, B):\n",
    "    e = min(s+B, n_eval)\n",
    "    Xe = X_eval[s:e].astype(np.float32, copy=False)\n",
    "    Xe = naninf_to_nan(Xe)\n",
    "    Xe = imputer.transform(Xe)\n",
    "    Xe = scaler.transform(Xe)\n",
    "    Xe = rff.transform(Xe)\n",
    "    scores_mm[s:e] = -sgd_oc.decision_function(Xe)\n",
    "    scores_mm.flush()\n",
    "    now=time.time()\n",
    "    if now-last>10:\n",
    "        print(f\"Progress: {100*e/n_eval:5.2f}% | {e:,}/{n_eval:,} rows\")\n",
    "        last=now\n",
    "\n",
    "scores = scores_mm  # vista\n",
    "print(\"Scores memmap:\", scores_path, \"| shape:\", scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positivos: 1003200 / 27789660\n",
      "ROC-AUC:0.5362 | PR-AUC:0.0462 | AP:0.0462\n",
      "@1% -> P:0.0799 | R:0.0221 | F1:0.0347  (k=277896)\n",
      "✅ Config actualizado: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_config.json\n"
     ]
    }
   ],
   "source": [
    "# --- Métricas: JOIN por 'window_id' + guardado ---\n",
    "import json, numpy as np, pandas as pd, pyarrow.parquet as pq\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"]); prefix = CFG.get(\"artifact_prefix\",\"ocsvm_rbf\")\n",
    "\n",
    "# Claves de eval (orden exacto)\n",
    "eval_pf = pq.ParquetFile(max(Path(CFG[\"external_data_dir\"]).glob(\"*windows*aligned*.parquet\"),\n",
    "                             key=lambda p: p.stat().st_size))\n",
    "key_eval = next(k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in eval_pf.schema_arrow.names)\n",
    "eval_key = eval_pf.read(columns=[key_eval]).to_pandas()[key_eval].astype(np.int64).reset_index(drop=True)\n",
    "\n",
    "# Conjunto de anómalos\n",
    "lab_pf = pq.ParquetFile(next(Path(CFG[\"external_data_dir\"]).glob(\"*labels_anom*.parquet\")))\n",
    "key_lab = key_eval if key_eval in lab_pf.schema_arrow.names else \\\n",
    "          next(k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in lab_pf.schema_arrow.names)\n",
    "anom_set = set(lab_pf.read(columns=[key_lab]).to_pandas()[key_lab].astype(np.int64).tolist())\n",
    "\n",
    "y_eval = eval_key.isin(anom_set).astype(int).to_numpy()\n",
    "print(\"Positivos:\", int(y_eval.sum()), \"/\", len(y_eval))\n",
    "\n",
    "# Métricas globales\n",
    "if len(np.unique(y_eval)) > 1:\n",
    "    roc = roc_auc_score(y_eval, scores)\n",
    "    prc, rec, _ = precision_recall_curve(y_eval, scores); pr_auc = auc(rec, prc)\n",
    "    ap = average_precision_score(y_eval, scores)\n",
    "else:\n",
    "    roc = pr_auc = ap = float('nan')\n",
    "print(f\"ROC-AUC:{roc:.4f} | PR-AUC:{pr_auc:.4f} | AP:{ap:.4f}\")\n",
    "\n",
    "# @k=1%\n",
    "k_rate=0.01; k=max(1,int(len(scores)*k_rate))\n",
    "thr=np.partition(scores,-k)[-k]; pred=(scores>=thr).astype(np.int8)\n",
    "tp=int(((pred==1)&(y_eval==1)).sum()); fp=int(((pred==1)&(y_eval==0)).sum()); fn=int(((pred==0)&(y_eval==1)).sum())\n",
    "P=tp/(tp+fp) if (tp+fp)>0 else 0.0; R=tp/(tp+fn) if (tp+fn)>0 else 0.0; F1=2*P*R/(P+R) if (P+R)>0 else 0.0\n",
    "print(f\"@1% -> P:{P:.4f} | R:{R:.4f} | F1:{F1:.4f}  (k={k})\")\n",
    "\n",
    "# Guardar/actualizar config\n",
    "cfg_path = OUT / f\"{prefix}_config.json\"\n",
    "cfg = json.loads(cfg_path.read_text()) if cfg_path.exists() else {}\n",
    "cfg.update({\n",
    "    \"artifact_prefix\": prefix,\n",
    "    \"external_data_dir\": CFG[\"external_data_dir\"],\n",
    "    \"model_type\": \"SGDOneClassSVM_RFF\",\n",
    "    \"rff_components\": int(rff.n_components),\n",
    "    \"best_params\": {\"nu\": float(getattr(sgd_oc, \"nu\", 0.05)), \"gamma\": \"approx(%.4f)\" % getattr(rff, \"gamma\", 0.1)},\n",
    "    \"metrics\": {\"roc_auc\": float(roc), \"pr_auc\": float(pr_auc), \"ap\": float(ap)},\n",
    "    \"metrics_at_k\": {\"0.01\": {\"k\": int(k), \"precision\": float(P), \"recall\": float(R), \"f1\": float(F1)}},\n",
    "    \"label_file\": lab_pf.metadata.metadata.get(b\"ARROW:schema\", b\"labels_anom.parquet\").decode(errors=\"ignore\") if hasattr(lab_pf,'metadata') else \"labels_anom.parquet\",\n",
    "    \"label_col\": key_lab,\n",
    "    \"label_mapping\": f\"JOIN on {key_eval}\"\n",
    "})\n",
    "cfg_path.write_text(json.dumps(cfg, indent=2))\n",
    "print(\"✅ Config actualizado:\", cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: erickdsuarez10 | Host: computeinstance-e00vebkmv7kfvb90pk | RAM total: 135.1 GB | Libre: 128.7 GB\n",
      "CFG: {\n",
      "  \"artifact_prefix\": \"ocsvm_rbf\",\n",
      "  \"external_data_dir\": \"/teamspace/studios/this_studio/data\",\n",
      "  \"max_train_samples\": 800000,\n",
      "  \"eval_batch_size\": 2000000\n",
      "}\n",
      "Imputer: ocsvm_rbf_imputer.pkl\n",
      "Scaler: ocsvm_rbf_robust_scaler.pkl\n",
      "TRAIN loaded: (27789660, 19) | feats: 19\n",
      "PCA ajustado: 16 comps -> 16 feats\n",
      "RFF fit+transform (train subset): (800000, 4096) | 12.7s\n",
      "SGDOneClassSVM fit: 0.5 min | soporte:4096 / 4096\n",
      "Artefactos guardados en /teamspace/studios/this_studio/data/ocsvm_runs\n",
      "EVAL: (27789660, 19) | groups: mmsi\n",
      "Scoring -> /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_eval_scores_mm.dat\n",
      "Batch size: 1,220,703 filas\n",
      "Progress:  4.39% | 1,220,703/27,789,660 rows\n",
      "Progress:  8.79% | 2,441,406/27,789,660 rows\n",
      "Progress: 13.18% | 3,662,109/27,789,660 rows\n",
      "Progress: 17.57% | 4,882,812/27,789,660 rows\n",
      "Progress: 21.96% | 6,103,515/27,789,660 rows\n",
      "Progress: 26.36% | 7,324,218/27,789,660 rows\n",
      "Progress: 30.75% | 8,544,921/27,789,660 rows\n",
      "Progress: 35.14% | 9,765,624/27,789,660 rows\n",
      "Progress: 39.53% | 10,986,327/27,789,660 rows\n",
      "Progress: 43.93% | 12,207,030/27,789,660 rows\n",
      "Progress: 48.32% | 13,427,733/27,789,660 rows\n",
      "Progress: 52.71% | 14,648,436/27,789,660 rows\n",
      "Progress: 57.10% | 15,869,139/27,789,660 rows\n",
      "Progress: 61.50% | 17,089,842/27,789,660 rows\n",
      "Progress: 65.89% | 18,310,545/27,789,660 rows\n",
      "Progress: 70.28% | 19,531,248/27,789,660 rows\n",
      "Progress: 74.68% | 20,751,951/27,789,660 rows\n",
      "Progress: 79.07% | 21,972,654/27,789,660 rows\n",
      "Progress: 83.46% | 23,193,357/27,789,660 rows\n",
      "Progress: 87.85% | 24,414,060/27,789,660 rows\n",
      "Progress: 92.25% | 25,634,763/27,789,660 rows\n",
      "Progress: 96.64% | 26,855,466/27,789,660 rows\n",
      "Progress: 100.00% | 27,789,660/27,789,660 rows\n",
      "Scoring listo en 8.5 min\n",
      "Positivos: 1003200 / 27789660\n",
      "ROC-AUC:0.5300 | PR-AUC:0.0464 | AP:0.0464\n",
      "@1% -> P:0.0865 | R:0.0240 | F1:0.0375  (k=277896)\n",
      "Config actualizado: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_config.json\n",
      "TOP-1% guardado: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_top1pct_detailed.parquet y /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_top1pct_detailed.csv | rows: 277897\n",
      "MMSI agg guardado: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_mmsi_agg.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mmsi</th>\n",
       "      <th>n_win</th>\n",
       "      <th>anom_win</th>\n",
       "      <th>anom_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>168015789077263</td>\n",
       "      <td>26420</td>\n",
       "      <td>3135</td>\n",
       "      <td>0.118660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12639560807591</td>\n",
       "      <td>23520</td>\n",
       "      <td>1526</td>\n",
       "      <td>0.064881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>281060477973977</td>\n",
       "      <td>36160</td>\n",
       "      <td>2276</td>\n",
       "      <td>0.062942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>103576446797335</td>\n",
       "      <td>148920</td>\n",
       "      <td>7814</td>\n",
       "      <td>0.052471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>277458320934593</td>\n",
       "      <td>22520</td>\n",
       "      <td>1109</td>\n",
       "      <td>0.049245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>33266086194351</td>\n",
       "      <td>428760</td>\n",
       "      <td>19839</td>\n",
       "      <td>0.046271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>77182424306278</td>\n",
       "      <td>169660</td>\n",
       "      <td>7718</td>\n",
       "      <td>0.045491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>100771710683634</td>\n",
       "      <td>32440</td>\n",
       "      <td>1473</td>\n",
       "      <td>0.045407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>208886908053041</td>\n",
       "      <td>19200</td>\n",
       "      <td>736</td>\n",
       "      <td>0.038333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>149117408188942</td>\n",
       "      <td>36040</td>\n",
       "      <td>1147</td>\n",
       "      <td>0.031826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                mmsi   n_win  anom_win  anom_rate\n",
       "61   168015789077263   26420      3135   0.118660\n",
       "1     12639560807591   23520      1526   0.064881\n",
       "108  281060477973977   36160      2276   0.062942\n",
       "42   103576446797335  148920      7814   0.052471\n",
       "106  277458320934593   22520      1109   0.049245\n",
       "12    33266086194351  428760     19839   0.046271\n",
       "27    77182424306278  169660      7718   0.045491\n",
       "40   100771710683634   32440      1473   0.045407\n",
       "83   208886908053041   19200       736   0.038333\n",
       "56   149117408188942   36040      1147   0.031826"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === ALL-IN-ONE: PCA(16) + RFF(D=4096, gamma=0.3) + SGDOneClassSVM(nu=0.08) + Scoring + Métricas + TopK ===\n",
    "import os, json, time, gc, getpass, socket\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import joblib\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import SGDOneClassSVM\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, average_precision_score\n",
    "\n",
    "# -------- Utilidades --------\n",
    "def log_sys():\n",
    "    try:\n",
    "        import psutil\n",
    "        ram = psutil.virtual_memory()\n",
    "        print(f\"User: {getpass.getuser()} | Host: {socket.gethostname()} | RAM total: {ram.total/1e9:.1f} GB | Libre: {ram.available/1e9:.1f} GB\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def naninf_to_nan(X):\n",
    "    X = X.astype(np.float32, copy=False)\n",
    "    bad = ~np.isfinite(X)\n",
    "    if bad.any(): X[bad] = np.nan\n",
    "    return X\n",
    "\n",
    "def sample_idx(n_max, n_total, seed=42):\n",
    "    if n_total <= n_max: return np.arange(n_total)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return np.sort(rng.choice(n_total, n_max, replace=False))\n",
    "\n",
    "def metrics_at_k(scores, yb, rate):\n",
    "    n=len(scores); k=max(1,int(n*rate))\n",
    "    thr=np.partition(scores,-k)[-k]; pred=(scores>=thr).astype(np.int8)\n",
    "    tp=int(((pred==1)&(yb==1)).sum()); fp=int(((pred==1)&(yb==0)).sum()); fn=int(((pred==0)&(yb==1)).sum())\n",
    "    P=tp/(tp+fp) if (tp+fp)>0 else 0.0; R=tp/(tp+fn) if (tp+fn)>0 else 0.0; F1=2*P*R/(P+R) if (P+R)>0 else 0.0\n",
    "    return k,P,R,F1\n",
    "\n",
    "log_sys()\n",
    "\n",
    "# -------- Rutas / CFG --------\n",
    "OUT_DIR = Path(\"data/ocsvm_runs\").resolve(); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATA_DIR = Path(\"/teamspace/studios/this_studio/data\").resolve()\n",
    "cfg_candidates = [\"ocsvm_v2_config.json\", \"ocsvm_rbf_config.json\"]\n",
    "cfg_path = next((OUT_DIR / f for f in cfg_candidates if (OUT_DIR / f).exists()), OUT_DIR / \"ocsvm_rbf_config.json\")\n",
    "CFG = json.loads(cfg_path.read_text()) if cfg_path.exists() else {\n",
    "    \"artifact_prefix\": \"ocsvm_rbf\",\n",
    "    \"external_data_dir\": str(DATA_DIR),\n",
    "    \"out_dir\": str(OUT_DIR),\n",
    "    \"max_train_samples\": 800_000,\n",
    "    \"eval_batch_size\": 2_000_000\n",
    "}\n",
    "CFG.setdefault(\"artifact_prefix\", \"ocsvm_rbf\")\n",
    "CFG.setdefault(\"external_data_dir\", str(DATA_DIR))\n",
    "CFG.setdefault(\"out_dir\", str(OUT_DIR))\n",
    "print(\"CFG:\", json.dumps({k: CFG.get(k) for k in [\"artifact_prefix\",\"external_data_dir\",\"max_train_samples\",\"eval_batch_size\"]}, indent=2))\n",
    "\n",
    "prefix = CFG[\"artifact_prefix\"]\n",
    "\n",
    "# -------- Cargar / reconstruir preprocesadores --------\n",
    "# Imputer\n",
    "imp_pkl = OUT_DIR / f\"{prefix}_imputer.pkl\"\n",
    "imp_meds = OUT_DIR / f\"{prefix}_imputer_medians.npy\"\n",
    "if imp_pkl.exists():\n",
    "    imputer = joblib.load(imp_pkl)\n",
    "    print(\"Imputer:\", imp_pkl.name)\n",
    "elif imp_meds.exists():\n",
    "    meds = np.load(imp_meds).astype(np.float32)\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    imputer.fit(np.zeros((1, meds.size), dtype=np.float32))\n",
    "    imputer.statistics_ = meds\n",
    "    print(\"Imputer reconstruido desde:\", imp_meds.name)\n",
    "else:\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    print(\"Imputer NUEVO (se ajustará)\")\n",
    "\n",
    "# Scaler\n",
    "scaler_pkl = OUT_DIR / f\"{prefix}_robust_scaler.pkl\"\n",
    "if scaler_pkl.exists():\n",
    "    scaler = joblib.load(scaler_pkl)\n",
    "    print(\"Scaler:\", scaler_pkl.name)\n",
    "else:\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0))\n",
    "    print(\"Scaler NUEVO (se ajustará)\")\n",
    "\n",
    "# -------- Cargar TRAIN mínimo --------\n",
    "train_path = DATA_DIR / \"windows_aligned_normal.parquet\"\n",
    "assert train_path.exists(), f\"No se encontró {train_path}\"\n",
    "df_tr = pq.read_table(train_path).to_pandas()\n",
    "for c in df_tr.columns:\n",
    "    if pd.api.types.is_float_dtype(df_tr[c]): df_tr[c] = df_tr[c].astype(np.float32)\n",
    "\n",
    "mmsi_col = \"mmsi\" if \"mmsi\" in df_tr.columns else None\n",
    "drop_cols = {\"lat\",\"lon\",\"idx\",\"idx_end\",\"window_id\"}\n",
    "feature_cols = [c for c in df_tr.columns if c not in drop_cols and c != mmsi_col]\n",
    "X_train_full = df_tr[feature_cols].to_numpy(np.float32)\n",
    "del df_tr; gc.collect()\n",
    "print(\"TRAIN loaded:\", X_train_full.shape, \"| feats:\", len(feature_cols))\n",
    "\n",
    "# Subset para ajustar PCA/RFF/SGD\n",
    "n_fit = int(CFG.get(\"max_train_samples\", 800_000))\n",
    "idx = sample_idx(n_fit, X_train_full.shape[0])\n",
    "Xf_raw = X_train_full[idx]\n",
    "Xf_raw = naninf_to_nan(Xf_raw)\n",
    "\n",
    "# Ajustar Imputer/Scaler si son nuevos\n",
    "if not hasattr(imputer, \"statistics_\"):\n",
    "    X_tmp = imputer.fit_transform(Xf_raw)\n",
    "    print(\"Imputer ajustado.\")\n",
    "else:\n",
    "    X_tmp = imputer.transform(Xf_raw)\n",
    "\n",
    "if not hasattr(scaler, \"scale_\"):\n",
    "    X_tmp = scaler.fit_transform(X_tmp)\n",
    "    print(\"Scaler ajustado.\")\n",
    "else:\n",
    "    X_tmp = scaler.transform(X_tmp)\n",
    "\n",
    "# -------- PCA(16) --------\n",
    "NCOMP = 16\n",
    "pca = PCA(n_components=NCOMP, random_state=42)\n",
    "Xf_pca = pca.fit_transform(X_tmp)\n",
    "print(f\"PCA ajustado: {NCOMP} comps -> {Xf_pca.shape[1]} feats\")\n",
    "\n",
    "# -------- RFF(D=4096, gamma=0.3) + SGDOneClassSVM(nu=0.08) --------\n",
    "D = 4096\n",
    "gamma_rbf = 0.3\n",
    "nu_val = 0.08\n",
    "\n",
    "rff = RBFSampler(gamma=gamma_rbf, n_components=D, random_state=42)\n",
    "t0 = time.time()\n",
    "Xf_rff = rff.fit_transform(Xf_pca)\n",
    "print(f\"RFF fit+transform (train subset): {Xf_rff.shape} | {time.time()-t0:.1f}s\")\n",
    "\n",
    "sgd_oc = SGDOneClassSVM(nu=nu_val, fit_intercept=True, shuffle=True,\n",
    "                        random_state=42, tol=1e-3, max_iter=2000)\n",
    "t1 = time.time()\n",
    "sgd_oc.fit(Xf_rff)\n",
    "print(f\"SGDOneClassSVM fit: {(time.time()-t1)/60:.1f} min | soporte:{np.count_nonzero(sgd_oc.coef_)} / {D}\")\n",
    "\n",
    "# Guardar artefactos\n",
    "joblib.dump(imputer, OUT_DIR / f\"{prefix}_imputer.pkl\")\n",
    "joblib.dump(scaler,  OUT_DIR / f\"{prefix}_robust_scaler.pkl\")\n",
    "joblib.dump(pca,     OUT_DIR / f\"{prefix}_pca.pkl\")\n",
    "joblib.dump(rff,     OUT_DIR / f\"{prefix}_rff.pkl\")\n",
    "joblib.dump(sgd_oc,  OUT_DIR / f\"{prefix}_model_sgd_rff.pkl\")\n",
    "print(\"Artefactos guardados en\", OUT_DIR)\n",
    "\n",
    "# -------- Cargar EVAL y scorear por lotes --------\n",
    "eval_path = max(DATA_DIR.glob(\"*windows*aligned*.parquet\"), key=lambda p: p.stat().st_size)\n",
    "df_ev = pq.read_table(eval_path).to_pandas()\n",
    "for c in df_ev.columns:\n",
    "    if pd.api.types.is_float_dtype(df_ev[c]): df_ev[c] = df_ev[c].astype(np.float32)\n",
    "\n",
    "drop_common = {\"lat\",\"lon\",\"idx\",\"idx_end\",\"window_id\"}\n",
    "gcol_ev = next((c for c in [\"mmsi\",\"group\",\"ship_id\"] if c in df_ev.columns), None)\n",
    "ycol_ev = next((c for c in [\"is_suspicious\",\"label\",\"y\",\"target\"] if c in df_ev.columns), None)\n",
    "feature_cols_ev = [c for c in df_ev.columns if c not in (drop_common | {gcol_ev, ycol_ev} if gcol_ev else drop_common)]\n",
    "X_eval = df_ev[feature_cols_ev].to_numpy(np.float32)\n",
    "groups_eval = df_ev[gcol_ev].to_numpy() if gcol_ev else None\n",
    "print(\"EVAL:\", X_eval.shape, \"| groups:\", gcol_ev)\n",
    "# clave para JOIN\n",
    "key_eval = next(k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in df_ev.columns)\n",
    "eval_key = df_ev[key_eval].astype(np.int64).to_numpy()\n",
    "del df_ev; gc.collect()\n",
    "\n",
    "# Memmap de scores\n",
    "scores_path = OUT_DIR / f\"{prefix}_eval_scores_mm.dat\"\n",
    "if scores_path.exists(): os.remove(scores_path)\n",
    "scores_mm = np.memmap(scores_path, dtype=np.float32, mode=\"w+\", shape=(X_eval.shape[0],))\n",
    "print(\"Scoring ->\", scores_path)\n",
    "\n",
    "# Batch size dinámico (~ objetivo 20GB para matriz RFF): 20e9 / (4 bytes * D)\n",
    "target_bytes = 20_000_000_000\n",
    "B_max = int(target_bytes // (4 * D))\n",
    "B_cfg = int(CFG.get(\"eval_batch_size\", 2_000_000))\n",
    "B = max(200_000, min(B_cfg, B_max))  # límite seguro\n",
    "print(f\"Batch size: {B:,} filas\")\n",
    "\n",
    "t0 = time.time(); last=t0; n = X_eval.shape[0]\n",
    "for s in range(0, n, B):\n",
    "    e = min(s+B, n)\n",
    "    Xe = X_eval[s:e]\n",
    "    Xe = naninf_to_nan(Xe)\n",
    "    Xe = imputer.transform(Xe)\n",
    "    Xe = scaler.transform(Xe)\n",
    "    Xe = pca.transform(Xe)\n",
    "    Xe = rff.transform(Xe)\n",
    "    scores_mm[s:e] = -sgd_oc.decision_function(Xe)\n",
    "    scores_mm.flush()\n",
    "    now = time.time()\n",
    "    if now - last > 10:\n",
    "        print(f\"Progress: {100*e/n:5.2f}% | {e:,}/{n:,} rows\")\n",
    "        last = now\n",
    "print(f\"Scoring listo en {(time.time()-t0)/60:.1f} min\")\n",
    "\n",
    "scores = scores_mm  # vista\n",
    "\n",
    "# -------- Construir y_eval por JOIN window_id --------\n",
    "lab_pf = pq.ParquetFile(next(DATA_DIR.glob(\"*labels_anom*.parquet\")))\n",
    "key_lab = key_eval if key_eval in lab_pf.schema_arrow.names else \\\n",
    "          next(k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in lab_pf.schema_arrow.names)\n",
    "anom_set = set(lab_pf.read(columns=[key_lab]).to_pandas()[key_lab].astype(np.int64).tolist())\n",
    "y_eval = np.isin(eval_key, list(anom_set)).astype(int)\n",
    "print(\"Positivos:\", int(y_eval.sum()), \"/\", len(y_eval))\n",
    "\n",
    "# -------- Métricas globales + @1% --------\n",
    "if len(np.unique(y_eval))>1:\n",
    "    roc = roc_auc_score(y_eval, scores)\n",
    "    prc, rec, _ = precision_recall_curve(y_eval, scores); pr_auc = auc(rec, prc)\n",
    "    ap = average_precision_score(y_eval, scores)\n",
    "else:\n",
    "    roc = pr_auc = ap = float('nan')\n",
    "k,P,R,F1 = metrics_at_k(scores, y_eval, 0.01)\n",
    "print(f\"ROC-AUC:{roc:.4f} | PR-AUC:{pr_auc:.4f} | AP:{ap:.4f}\")\n",
    "print(f\"@1% -> P:{P:.4f} | R:{R:.4f} | F1:{F1:.4f}  (k={k})\")\n",
    "\n",
    "# -------- Guardar métricas en config --------\n",
    "cfg = json.loads(cfg_path.read_text()) if cfg_path.exists() else {}\n",
    "cfg.update({\n",
    "    \"artifact_prefix\": prefix,\n",
    "    \"external_data_dir\": CFG[\"external_data_dir\"],\n",
    "    \"model_type\": \"PCA16 + RFF4096 + SGDOneClassSVM\",\n",
    "    \"best_params\": {\"nu\": float(nu_val), \"gamma\": float(gamma_rbf), \"D\": int(D), \"pca_components\": int(NCOMP)},\n",
    "    \"metrics\": {\"roc_auc\": float(roc), \"pr_auc\": float(pr_auc), \"ap\": float(ap)},\n",
    "    \"metrics_at_k\": {\"0.01\": {\"k\": int(k), \"precision\": float(P), \"recall\": float(R), \"f1\": float(F1)}},\n",
    "    \"label_file\": \"labels_anom.parquet\",\n",
    "    \"label_col\": key_lab,\n",
    "    \"label_mapping\": f\"JOIN on {key_eval}\"\n",
    "})\n",
    "cfg_path.write_text(json.dumps(cfg, indent=2))\n",
    "print(\"Config actualizado:\", cfg_path)\n",
    "\n",
    "# -------- Guardar Top-1% y agregados por MMSI --------\n",
    "top_rate = 0.01\n",
    "k = max(1, int(len(scores)*top_rate))\n",
    "thr = np.partition(scores,-k)[-k]\n",
    "pred = (scores >= thr)\n",
    "rows = np.where(pred)[0]\n",
    "\n",
    "top = pd.DataFrame({\"idx\": rows, \"anomaly_score\": scores[rows].astype(np.float32)})\n",
    "if groups_eval is not None:\n",
    "    top[\"mmsi\"] = groups_eval[rows].astype(np.int64)\n",
    "top = top.sort_values(\"anomaly_score\", ascending=False)\n",
    "\n",
    "top_pq = OUT_DIR / f\"{prefix}_top1pct_detailed.parquet\"\n",
    "top_csv = OUT_DIR / f\"{prefix}_top1pct_detailed.csv\"\n",
    "top.to_parquet(top_pq, index=False); top.to_csv(top_csv, index=False)\n",
    "print(\"TOP-1% guardado:\", top_pq, \"y\", top_csv, \"| rows:\", len(top))\n",
    "\n",
    "if groups_eval is not None:\n",
    "    mmsi_all, n_by_mmsi = np.unique(groups_eval, return_counts=True)\n",
    "    mmsi_top, n_top_by_mmsi = np.unique(groups_eval[pred==1], return_counts=True)\n",
    "    top_map = dict(zip(mmsi_top.tolist(), n_top_by_mmsi.tolist()))\n",
    "    anom_win = np.array([top_map.get(m, 0) for m in mmsi_all], dtype=np.int32)\n",
    "    anom_rate = anom_win / n_by_mmsi\n",
    "    agg_df = pd.DataFrame({\"mmsi\": mmsi_all, \"n_win\": n_by_mmsi, \"anom_win\": anom_win, \"anom_rate\": anom_rate})\n",
    "    agg_path = OUT_DIR / f\"{prefix}_mmsi_agg.parquet\"\n",
    "    agg_df.to_parquet(agg_path, index=False)\n",
    "    print(\"MMSI agg guardado:\", agg_path)\n",
    "    display(agg_df.sort_values(\"anom_rate\", ascending=False).head(10))\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_summary_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_af3da\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_af3da_level0_col0\" class=\"col_heading level0 col0\" >ROC-AUC</th>\n",
       "      <th id=\"T_af3da_level0_col1\" class=\"col_heading level0 col1\" >PR-AUC</th>\n",
       "      <th id=\"T_af3da_level0_col2\" class=\"col_heading level0 col2\" >AP</th>\n",
       "      <th id=\"T_af3da_level0_col3\" class=\"col_heading level0 col3\" >k_rate</th>\n",
       "      <th id=\"T_af3da_level0_col4\" class=\"col_heading level0 col4\" >k</th>\n",
       "      <th id=\"T_af3da_level0_col5\" class=\"col_heading level0 col5\" >precision</th>\n",
       "      <th id=\"T_af3da_level0_col6\" class=\"col_heading level0 col6\" >recall</th>\n",
       "      <th id=\"T_af3da_level0_col7\" class=\"col_heading level0 col7\" >f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_af3da_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_af3da_row0_col0\" class=\"data row0 col0\" >0.5300</td>\n",
       "      <td id=\"T_af3da_row0_col1\" class=\"data row0 col1\" >0.0464</td>\n",
       "      <td id=\"T_af3da_row0_col2\" class=\"data row0 col2\" >0.0464</td>\n",
       "      <td id=\"T_af3da_row0_col3\" class=\"data row0 col3\" >0.005000</td>\n",
       "      <td id=\"T_af3da_row0_col4\" class=\"data row0 col4\" >138948</td>\n",
       "      <td id=\"T_af3da_row0_col5\" class=\"data row0 col5\" >0.0865</td>\n",
       "      <td id=\"T_af3da_row0_col6\" class=\"data row0 col6\" >0.0120</td>\n",
       "      <td id=\"T_af3da_row0_col7\" class=\"data row0 col7\" >0.0210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af3da_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_af3da_row1_col0\" class=\"data row1 col0\" >0.5300</td>\n",
       "      <td id=\"T_af3da_row1_col1\" class=\"data row1 col1\" >0.0464</td>\n",
       "      <td id=\"T_af3da_row1_col2\" class=\"data row1 col2\" >0.0464</td>\n",
       "      <td id=\"T_af3da_row1_col3\" class=\"data row1 col3\" >0.010000</td>\n",
       "      <td id=\"T_af3da_row1_col4\" class=\"data row1 col4\" >277896</td>\n",
       "      <td id=\"T_af3da_row1_col5\" class=\"data row1 col5\" >0.0865</td>\n",
       "      <td id=\"T_af3da_row1_col6\" class=\"data row1 col6\" >0.0240</td>\n",
       "      <td id=\"T_af3da_row1_col7\" class=\"data row1 col7\" >0.0375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af3da_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_af3da_row2_col0\" class=\"data row2 col0\" >0.5300</td>\n",
       "      <td id=\"T_af3da_row2_col1\" class=\"data row2 col1\" >0.0464</td>\n",
       "      <td id=\"T_af3da_row2_col2\" class=\"data row2 col2\" >0.0464</td>\n",
       "      <td id=\"T_af3da_row2_col3\" class=\"data row2 col3\" >0.020000</td>\n",
       "      <td id=\"T_af3da_row2_col4\" class=\"data row2 col4\" >555793</td>\n",
       "      <td id=\"T_af3da_row2_col5\" class=\"data row2 col5\" >0.0834</td>\n",
       "      <td id=\"T_af3da_row2_col6\" class=\"data row2 col6\" >0.0462</td>\n",
       "      <td id=\"T_af3da_row2_col7\" class=\"data row2 col7\" >0.0595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af3da_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_af3da_row3_col0\" class=\"data row3 col0\" >0.5300</td>\n",
       "      <td id=\"T_af3da_row3_col1\" class=\"data row3 col1\" >0.0464</td>\n",
       "      <td id=\"T_af3da_row3_col2\" class=\"data row3 col2\" >0.0464</td>\n",
       "      <td id=\"T_af3da_row3_col3\" class=\"data row3 col3\" >0.050000</td>\n",
       "      <td id=\"T_af3da_row3_col4\" class=\"data row3 col4\" >1389483</td>\n",
       "      <td id=\"T_af3da_row3_col5\" class=\"data row3 col5\" >0.0704</td>\n",
       "      <td id=\"T_af3da_row3_col6\" class=\"data row3 col6\" >0.0975</td>\n",
       "      <td id=\"T_af3da_row3_col7\" class=\"data row3 col7\" >0.0817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f5059b1b2c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Multi-k (0.5/1/2/5%) + CSV ===\n",
    "import numpy as np, pandas as pd, json\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import precision_recall_curve, auc, average_precision_score, roc_auc_score\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"]); prefix = CFG.get(\"artifact_prefix\",\"ocsvm_rbf\")\n",
    "scores = np.memmap(OUT / f\"{prefix}_eval_scores_mm.dat\", dtype=np.float32, mode=\"r\")\n",
    "\n",
    "# Reconstruir y_eval si no está en memoria (reusa el JOIN de tu celda C si hace falta)\n",
    "try:\n",
    "    yb = y_eval\n",
    "except NameError:\n",
    "    import pyarrow.parquet as pq\n",
    "    DATA_DIR = Path(CFG[\"external_data_dir\"])\n",
    "    eval_path = max(DATA_DIR.glob(\"*windows*aligned*.parquet\"), key=lambda p: p.stat().st_size)\n",
    "    pf_eval = pq.ParquetFile(eval_path)\n",
    "    key_eval = next(k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in pf_eval.schema_arrow.names)\n",
    "    eval_key = pf_eval.read(columns=[key_eval]).to_pandas()[key_eval].astype(np.int64).to_numpy()\n",
    "    lab_pf = pq.ParquetFile(next(DATA_DIR.glob(\"*labels_anom*.parquet\")))\n",
    "    key_lab = key_eval if key_eval in lab_pf.schema_arrow.names else \\\n",
    "              next(k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in lab_pf.schema_arrow.names)\n",
    "    anom = set(lab_pf.read(columns=[key_lab]).to_pandas()[key_lab].astype(np.int64).tolist())\n",
    "    yb = np.isin(eval_key, list(anom)).astype(int)\n",
    "\n",
    "def metrics_at_k(scores, yb, rate):\n",
    "    n=len(scores); k=max(1,int(n*rate))\n",
    "    thr=np.partition(scores,-k)[-k]; pred=(scores>=thr).astype(np.int8)\n",
    "    tp=int(((pred==1)&(yb==1)).sum()); fp=int(((pred==1)&(yb==0)).sum()); fn=int(((pred==0)&(yb==1)).sum())\n",
    "    P=tp/(tp+fp) if (tp+fp)>0 else 0.0; R=tp/(tp+fn) if (tp+fn)>0 else 0.0; F1=2*P*R/(P+R) if (P+R)>0 else 0.0\n",
    "    return {\"k\":k,\"precision\":P,\"recall\":R,\"f1\":F1}\n",
    "\n",
    "roc = roc_auc_score(yb, scores)\n",
    "prec, rec, _ = precision_recall_curve(yb, scores); pr_auc = auc(rec, prec); ap = average_precision_score(yb, scores)\n",
    "\n",
    "rows=[]\n",
    "for r in [0.005, 0.01, 0.02, 0.05]:\n",
    "    m=metrics_at_k(scores, yb, r); m[\"k_rate\"]=r; rows.append(m)\n",
    "\n",
    "summary = pd.DataFrame(rows)[[\"k_rate\",\"k\",\"precision\",\"recall\",\"f1\"]]\n",
    "summary.insert(0,\"AP\",ap); summary.insert(0,\"PR-AUC\",pr_auc); summary.insert(0,\"ROC-AUC\",roc)\n",
    "csv_path = OUT / f\"{prefix}_summary_metrics.csv\"\n",
    "summary.to_csv(csv_path, index=False)\n",
    "print(\"Guardado:\", csv_path)\n",
    "display(summary.style.format({\"ROC-AUC\":\"{:.4f}\",\"PR-AUC\":\"{:.4f}\",\"AP\":\"{:.4f}\",\"precision\":\"{:.4f}\",\"recall\":\"{:.4f}\",\"f1\":\"{:.4f}\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[scores_smooth] @1% -> P:0.1060 | R:0.0294 | F1:0.0460\n"
     ]
    }
   ],
   "source": [
    "# === Suavizado por MMSI (rolling 3) y nueva @1% ===\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"]); prefix = CFG.get(\"artifact_prefix\",\"ocsvm_rbf\")\n",
    "scores = np.memmap(OUT / f\"{prefix}_eval_scores_mm.dat\", dtype=np.float32, mode=\"r\")\n",
    "assert 'groups_eval' in globals() and groups_eval is not None, \"Necesito groups_eval (mmsi).\"\n",
    "\n",
    "df = pd.DataFrame({\"mmsi\": groups_eval, \"score\": np.asarray(scores, dtype=np.float32)})\n",
    "df[\"score_smooth\"] = df.groupby(\"mmsi\")[\"score\"].transform(lambda s: s.rolling(3, min_periods=1, center=True).mean())\n",
    "scores_s = df[\"score_smooth\"].to_numpy(np.float32)\n",
    "\n",
    "# @1%\n",
    "n=len(scores_s); k=max(1,int(n*0.01))\n",
    "thr=np.partition(scores_s,-k)[-k]; pred=(scores_s>=thr).astype(np.int8)\n",
    "tp=int(((pred==1)&(y_eval==1)).sum()); fp=int(((pred==1)&(y_eval==0)).sum()); fn=int(((pred==0)&(y_eval==1)).sum())\n",
    "P=tp/(tp+fp) if (tp+fp)>0 else 0.0; R=tp/(tp+fn) if (tp+fn)>0 else 0.0; F1=2*P*R/(P+R) if (P+R)>0 else 0.0\n",
    "print(f\"[scores_smooth] @1% -> P:{P:.4f} | R:{R:.4f} | F1:{F1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# subset de train para refit\u001b[39;00m\n\u001b[1;32m     13\u001b[0m n_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(CFG\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_train_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m800_000\u001b[39m))\n\u001b[0;32m---> 14\u001b[0m rng \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(\u001b[38;5;241m42\u001b[39m); idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msort(rng\u001b[38;5;241m.\u001b[39mchoice(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], n_fit, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m     15\u001b[0m Xf \u001b[38;5;241m=\u001b[39m imputer\u001b[38;5;241m.\u001b[39mtransform(X_train[idx]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)); Xf \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(Xf); Xf \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mtransform(Xf)\n\u001b[1;32m     17\u001b[0m D\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m; gamma_rbf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m; nu_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.08\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# === RFF(D=8192) + SGD re-fit (usando PCA ya guardado) y re-scoring ===\n",
    "import joblib, time, numpy as np, os\n",
    "from pathlib import Path\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import SGDOneClassSVM\n",
    "\n",
    "OUT=Path(CFG[\"out_dir\"]); prefix=CFG.get(\"artifact_prefix\",\"ocsvm_rbf\")\n",
    "imputer = joblib.load(OUT / f\"{prefix}_imputer.pkl\")\n",
    "scaler  = joblib.load(OUT / f\"{prefix}_robust_scaler.pkl\")\n",
    "pca     = joblib.load(OUT / f\"{prefix}_pca.pkl\")\n",
    "\n",
    "# subset de train para refit\n",
    "n_fit = int(CFG.get(\"max_train_samples\", 800_000))\n",
    "rng = np.random.default_rng(42); idx = np.sort(rng.choice(X_train.shape[0], n_fit, replace=False))\n",
    "Xf = imputer.transform(X_train[idx].astype(np.float32)); Xf = scaler.transform(Xf); Xf = pca.transform(Xf)\n",
    "\n",
    "D=8192; gamma_rbf=0.3; nu_val=0.08\n",
    "rff = RBFSampler(gamma=gamma_rbf, n_components=D, random_state=42)\n",
    "t0=time.time(); Xf_rff = rff.fit_transform(Xf); print(\"RFF(8192) listo en\", f\"{time.time()-t0:.1f}s\")\n",
    "sgd_oc = SGDOneClassSVM(nu=nu_val, fit_intercept=True, shuffle=True, random_state=42, tol=1e-3, max_iter=2000)\n",
    "t1=time.time(); sgd_oc.fit(Xf_rff); print(\"SGD fit en\", f\"{(time.time()-t1)/60:.1f} min\")\n",
    "\n",
    "# guardar\n",
    "joblib.dump(rff, OUT / f\"{prefix}_rff.pkl\")\n",
    "joblib.dump(sgd_oc, OUT / f\"{prefix}_model_sgd_rff.pkl\")\n",
    "print(\"✅ Reemplazado RFF+SGD. Re-ejecuta la celda de scoring por lotes (B) y métricas (C).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  7.20% | 2,000,000/27,789,660\n",
      "Progress: 14.39% | 4,000,000/27,789,660\n",
      "Progress: 21.59% | 6,000,000/27,789,660\n",
      "Progress: 28.79% | 8,000,000/27,789,660\n",
      "Progress: 35.98% | 10,000,000/27,789,660\n",
      "Progress: 43.18% | 12,000,000/27,789,660\n",
      "Progress: 50.38% | 14,000,000/27,789,660\n",
      "Progress: 57.58% | 16,000,000/27,789,660\n",
      "Progress: 64.77% | 18,000,000/27,789,660\n",
      "Progress: 71.97% | 20,000,000/27,789,660\n",
      "Progress: 79.17% | 22,000,000/27,789,660\n",
      "Progress: 86.36% | 24,000,000/27,789,660\n",
      "Progress: 93.56% | 26,000,000/27,789,660\n",
      "Progress: 100.00% | 27,789,660/27,789,660\n",
      "Scores memmap: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_eval_scores_mm.dat | shape: (27789660,)\n"
     ]
    }
   ],
   "source": [
    "# === Re-scoring completo con el NUEVO RFF=8192 + SGD guardados ===\n",
    "import numpy as np, os, time, joblib, pyarrow.parquet as pq, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"]); OUT.mkdir(parents=True, exist_ok=True)\n",
    "prefix = CFG.get(\"artifact_prefix\",\"ocsvm_rbf\")\n",
    "scores_path = OUT / f\"{prefix}_eval_scores_mm.dat\"\n",
    "\n",
    "# Cargar artefactos actualizados\n",
    "imputer = joblib.load(OUT / f\"{prefix}_imputer.pkl\")\n",
    "scaler  = joblib.load(OUT / f\"{prefix}_robust_scaler.pkl\")\n",
    "pca     = joblib.load(OUT / f\"{prefix}_pca.pkl\")\n",
    "rff     = joblib.load(OUT / f\"{prefix}_rff.pkl\")              # << ahora D=8192\n",
    "sgd_oc  = joblib.load(OUT / f\"{prefix}_model_sgd_rff.pkl\")    # << reentrenado con D=8192\n",
    "\n",
    "# X_eval ya en memoria; si no, reconstruir como antes:\n",
    "try:\n",
    "    X_eval\n",
    "except NameError:\n",
    "    DATA_DIR = Path(CFG[\"external_data_dir\"])\n",
    "    eval_path = max(DATA_DIR.glob(\"*windows*aligned*.parquet\"), key=lambda p: p.stat().st_size)\n",
    "    df = pq.read_table(eval_path).to_pandas()\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_float_dtype(df[c]): df[c] = df[c].astype(np.float32)\n",
    "    drop={\"lat\",\"lon\",\"idx\",\"idx_end\",\"window_id\"}\n",
    "    gcol = next((c for c in [\"mmsi\",\"group\",\"ship_id\"] if c in df.columns), None)\n",
    "    ycol = next((c for c in [\"is_suspicious\",\"label\",\"y\",\"target\"] if c in df.columns), None)\n",
    "    feats=[c for c in df.columns if c not in (drop | {gcol,ycol} if gcol else drop)]\n",
    "    X_eval = df[feats].to_numpy(np.float32)\n",
    "    groups_eval = df[gcol].to_numpy() if gcol else None\n",
    "    del df\n",
    "\n",
    "# Memmap\n",
    "n_eval = X_eval.shape[0]\n",
    "if scores_path.exists(): os.remove(scores_path)\n",
    "scores_mm = np.memmap(scores_path, dtype=np.float32, mode=\"w+\", shape=(n_eval,))\n",
    "\n",
    "def naninf_to_nan(arr):\n",
    "    arr = arr.astype(np.float32, copy=False)\n",
    "    bad = ~np.isfinite(arr)\n",
    "    if bad.any(): arr[bad] = np.nan\n",
    "    return arr\n",
    "\n",
    "B = int(CFG.get(\"eval_batch_size\", 2_000_000))\n",
    "t0 = time.time(); last=t0\n",
    "for s in range(0, n_eval, B):\n",
    "    e = min(s+B, n_eval)\n",
    "    Xe = naninf_to_nan(X_eval[s:e])\n",
    "    Xe = imputer.transform(Xe)\n",
    "    Xe = scaler.transform(Xe)\n",
    "    Xe = pca.transform(Xe)\n",
    "    Xe = rff.transform(Xe)\n",
    "    scores_mm[s:e] = -sgd_oc.decision_function(Xe)\n",
    "    scores_mm.flush()\n",
    "    now=time.time()\n",
    "    if now-last>10:\n",
    "        print(f\"Progress: {100*e/n_eval:5.2f}% | {e:,}/{n_eval:,}\")\n",
    "        last=now\n",
    "\n",
    "scores = scores_mm\n",
    "print(\"Scores memmap:\", scores_path, \"| shape:\", scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positivos: 1003200 / 27789660\n",
      "[RAW] ROC:0.5300 PR-AUC:0.0464 AP:0.0464 | @1% P:0.0865 R:0.0240 F1:0.0375\n",
      "[SMOOTH] ROC:0.5463 PR-AUC:0.0497 AP:0.0497 | @1% P:0.1060 R:0.0294 F1:0.0460\n",
      "✅ Config actualizado con RAW y SMOOTH: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_config.json\n"
     ]
    }
   ],
   "source": [
    "# === Métricas RAW + SMOOTH y persistencia en config ===\n",
    "import json, numpy as np, pandas as pd, pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, average_precision_score\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"]); prefix = CFG.get(\"artifact_prefix\",\"ocsvm_rbf\")\n",
    "scores = np.memmap(OUT / f\"{prefix}_eval_scores_mm.dat\", dtype=np.float32, mode=\"r\")\n",
    "\n",
    "# JOIN para y_eval\n",
    "DATA_DIR = Path(CFG[\"external_data_dir\"])\n",
    "eval_path = max(DATA_DIR.glob(\"*windows*aligned*.parquet\"), key=lambda p: p.stat().st_size)\n",
    "pf_eval = pq.ParquetFile(eval_path)\n",
    "key_eval = next(k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in pf_eval.schema_arrow.names)\n",
    "eval_key = pf_eval.read(columns=[key_eval]).to_pandas()[key_eval].astype(np.int64).to_numpy()\n",
    "\n",
    "lab_pf = pq.ParquetFile(next(DATA_DIR.glob(\"*labels_anom*.parquet\")))\n",
    "key_lab = key_eval if key_eval in lab_pf.schema_arrow.names else \\\n",
    "          next(k for k in [\"window_id\",\"idx\",\"idx_end\",\"row\",\"row_id\"] if k in lab_pf.schema_arrow.names)\n",
    "anom = set(lab_pf.read(columns=[key_lab]).to_pandas()[key_lab].astype(np.int64).tolist())\n",
    "y_eval = np.isin(eval_key, list(anom)).astype(int)\n",
    "print(\"Positivos:\", int(y_eval.sum()), \"/\", len(y_eval))\n",
    "\n",
    "def global_metrics(s, y):\n",
    "    roc = roc_auc_score(y, s)\n",
    "    prc, rec, _ = precision_recall_curve(y, s); pr_auc = auc(rec, prc); ap = average_precision_score(y, s)\n",
    "    return roc, pr_auc, ap\n",
    "\n",
    "def at_k(s, y, rate):\n",
    "    n=len(s); k=max(1,int(n*rate))\n",
    "    thr=np.partition(s, -k)[-k]; pred=(s>=thr).astype(np.int8)\n",
    "    tp=int(((pred==1)&(y==1)).sum()); fp=int(((pred==1)&(y==0)).sum()); fn=int(((pred==0)&(y==1)).sum())\n",
    "    P=tp/(tp+fp) if (tp+fp)>0 else 0.0; R=tp/(tp+fn) if (tp+fn)>0 else 0.0; F1=2*P*R/(P+R) if (P+R)>0 else 0.0\n",
    "    return k,P,R,F1\n",
    "\n",
    "# RAW\n",
    "roc_raw, pr_raw, ap_raw = global_metrics(scores, y_eval)\n",
    "k1, P1, R1, F1 = at_k(scores, y_eval, 0.01)\n",
    "print(f\"[RAW] ROC:{roc_raw:.4f} PR-AUC:{pr_raw:.4f} AP:{ap_raw:.4f} | @1% P:{P1:.4f} R:{R1:.4f} F1:{F1:.4f}\")\n",
    "\n",
    "# SMOOTH por MMSI (rolling=3)\n",
    "if 'groups_eval' not in globals() or groups_eval is None:\n",
    "    # reconstruir groups_eval si no está\n",
    "    df_tmp = pf_eval.read(columns=[key_eval,\"mmsi\"]).to_pandas() if \"mmsi\" in pf_eval.schema_arrow.names else None\n",
    "    assert df_tmp is not None, \"No hallé mmsi en eval para suavizado.\"\n",
    "    groups_eval = df_tmp[\"mmsi\"].to_numpy()\n",
    "\n",
    "df = pd.DataFrame({\"mmsi\": groups_eval, \"score\": np.asarray(scores, dtype=np.float32)})\n",
    "df[\"score_smooth\"] = df.groupby(\"mmsi\")[\"score\"].transform(lambda s: s.rolling(3, min_periods=1, center=True).mean())\n",
    "scores_s = df[\"score_smooth\"].to_numpy(np.float32)\n",
    "\n",
    "roc_s, pr_s, ap_s = global_metrics(scores_s, y_eval)\n",
    "k1s, P1s, R1s, F1s = at_k(scores_s, y_eval, 0.01)\n",
    "print(f\"[SMOOTH] ROC:{roc_s:.4f} PR-AUC:{pr_s:.4f} AP:{ap_s:.4f} | @1% P:{P1s:.4f} R:{R1s:.4f} F1:{F1s:.4f}\")\n",
    "\n",
    "# Persistir en config\n",
    "cfg_path = OUT / f\"{prefix}_config.json\"\n",
    "cfg = json.loads(cfg_path.read_text()) if cfg_path.exists() else {}\n",
    "cfg.setdefault(\"metrics\", {})\n",
    "cfg.setdefault(\"metrics_at_k\", {})\n",
    "cfg[\"metrics\"][\"raw\"]   = {\"roc_auc\": float(roc_raw), \"pr_auc\": float(pr_raw), \"ap\": float(ap_raw)}\n",
    "cfg[\"metrics\"][\"smooth\"]= {\"roc_auc\": float(roc_s),   \"pr_auc\": float(pr_s),   \"ap\": float(ap_s)}\n",
    "cfg[\"metrics_at_k\"][\"raw@0.01\"]    = {\"k\": int(k1),  \"precision\": float(P1),  \"recall\": float(R1),  \"f1\": float(F1)}\n",
    "cfg[\"metrics_at_k\"][\"smooth@0.01\"] = {\"k\": int(k1s), \"precision\": float(P1s), \"recall\": float(R1s), \"f1\": float(F1s)}\n",
    "cfg[\"best_params\"] = {\"nu\": 0.08, \"gamma\": 0.3, \"D\": int(getattr(rff, \"n_components\", 0)), \"pca_components\": 16}\n",
    "cfg[\"model_type\"]  = \"PCA16 + RFF8192 + SGDOneClassSVM\"\n",
    "cfg[\"label_file\"]  = \"labels_anom.parquet\"\n",
    "cfg[\"label_col\"]   = key_lab\n",
    "cfg[\"label_mapping\"] = f\"JOIN on {key_eval}\"\n",
    "cfg_path.write_text(json.dumps(cfg, indent=2))\n",
    "print(\"✅ Config actualizado con RAW y SMOOTH:\", cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TOP-1% SMOOTH guardado: /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_top1pct_smooth.parquet y /teamspace/studios/this_studio/data/ocsvm_runs/ocsvm_rbf_top1pct_smooth.csv | rows: 277897\n"
     ]
    }
   ],
   "source": [
    "# === Top-1% suavizado por MMSI (rolling=3) ===\n",
    "import numpy as np, pandas as pd, os\n",
    "from pathlib import Path\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"]); prefix = CFG.get(\"artifact_prefix\",\"ocsvm_rbf\")\n",
    "scores = np.memmap(OUT / f\"{prefix}_eval_scores_mm.dat\", dtype=np.float32, mode=\"r\")\n",
    "df = pd.DataFrame({\"mmsi\": groups_eval, \"score\": np.asarray(scores, dtype=np.float32)})\n",
    "df[\"score_smooth\"] = df.groupby(\"mmsi\")[\"score\"].transform(lambda s: s.rolling(3, min_periods=1, center=True).mean())\n",
    "scores_s = df[\"score_smooth\"].to_numpy(np.float32)\n",
    "\n",
    "n=len(scores_s); k=max(1, int(n*0.01))\n",
    "thr=np.partition(scores_s, -k)[-k]; pred=(scores_s>=thr)\n",
    "rows = np.where(pred)[0]\n",
    "top = pd.DataFrame({\"idx\": rows, \"anomaly_score_smooth\": scores_s[rows].astype(np.float32)})\n",
    "top[\"mmsi\"] = groups_eval[rows].astype(np.int64)\n",
    "top = top.sort_values(\"anomaly_score_smooth\", ascending=False)\n",
    "\n",
    "out_pq = OUT / f\"{prefix}_top1pct_smooth.parquet\"\n",
    "out_csv = OUT / f\"{prefix}_top1pct_smooth.csv\"\n",
    "top.to_parquet(out_pq, index=False); top.to_csv(out_csv, index=False)\n",
    "print(\"✅ TOP-1% SMOOTH guardado:\", out_pq, \"y\", out_csv, \"| rows:\", len(top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Snapshot final en: /teamspace/studios/this_studio/data/ocsvm_runs/final_run_20251027_0238\n"
     ]
    }
   ],
   "source": [
    "# === Snapshot final de artefactos y reportes ===\n",
    "import shutil, datetime, json\n",
    "from pathlib import Path\n",
    "\n",
    "OUT = Path(CFG[\"out_dir\"]); prefix = CFG.get(\"artifact_prefix\",\"ocsvm_rbf\")\n",
    "stamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "dest = OUT / f\"final_run_{stamp}\"\n",
    "dest.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Archivos clave\n",
    "to_copy = [\n",
    "    f\"{prefix}_config.json\",\n",
    "    f\"{prefix}_imputer.pkl\",\n",
    "    f\"{prefix}_robust_scaler.pkl\",\n",
    "    f\"{prefix}_pca.pkl\",\n",
    "    f\"{prefix}_rff.pkl\",\n",
    "    f\"{prefix}_model_sgd_rff.pkl\",\n",
    "    f\"{prefix}_eval_scores_mm.dat\",\n",
    "    f\"{prefix}_summary_metrics.csv\",\n",
    "    f\"{prefix}_top1pct_detailed.parquet\",\n",
    "    f\"{prefix}_top1pct_detailed.csv\",\n",
    "    f\"{prefix}_top1pct_smooth.parquet\",\n",
    "    f\"{prefix}_top1pct_smooth.csv\",\n",
    "    f\"{prefix}_mmsi_agg.parquet\",\n",
    "]\n",
    "for name in to_copy:\n",
    "    p = OUT / name\n",
    "    if p.exists(): shutil.copy2(p, dest / name)\n",
    "\n",
    "# Model card mínimo\n",
    "cfg = json.loads((OUT / f\"{prefix}_config.json\").read_text())\n",
    "model_card = {\n",
    "    \"model\": cfg.get(\"model_type\", \"PCA16 + RFF8192 + SGDOneClassSVM\"),\n",
    "    \"hyperparams\": cfg.get(\"best_params\", {}),\n",
    "    \"data\": {\"eval_parquet\": \"windows_with_labels_aligned.parquet\", \"labels\": \"labels_anom.parquet\"},\n",
    "    \"metrics\": cfg.get(\"metrics\", {}),\n",
    "    \"metrics_at_k\": cfg.get(\"metrics_at_k\", {}),\n",
    "    \"notes\": [\n",
    "        \"scores_smooth = rolling mean (w=3) por MMSI\",\n",
    "        \"usar top1pct_smooth para priorización operativa\"\n",
    "    ]\n",
    "}\n",
    "( dest / \"MODEL_CARD.json\" ).write_text(json.dumps(model_card, indent=2))\n",
    "print(\"📦 Snapshot final en:\", dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
